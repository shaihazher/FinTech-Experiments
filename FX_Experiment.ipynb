{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import time\n",
    "import datetime\n",
    "import ta\n",
    "import pickle\n",
    "def upper_shadow(opener, close, high):\n",
    "    if opener >= close:\n",
    "        return abs(opener-high)\n",
    "    else:\n",
    "        return abs(close-high)\n",
    "\n",
    "def lower_shadow(opener, close, low):\n",
    "    if opener <= close:\n",
    "        return abs(opener-low)\n",
    "    else:\n",
    "        return abs(close-low)\n",
    "\n",
    "def real_body(opener, close):\n",
    "    return abs(opener-close)\n",
    "\n",
    "def total_dollar(av,bv,a,b):\n",
    "    p = (a+b)/2.0\n",
    "    v = av+bv\n",
    "    return p*v\n",
    "\n",
    "def log_dollar(a,b):\n",
    "    p = (a+b)/2.0\n",
    "    return np.log(p)\n",
    "\n",
    "def volumen(av,bv):\n",
    "    return av+bv\n",
    "\n",
    "def price(a,b):\n",
    "    g = (a+b)/2.0\n",
    "    return b\n",
    "\n",
    "prev = 1\n",
    "def signer(x):\n",
    "    global prev\n",
    "    if x.values[-1] > x.values[0]:\n",
    "        prev = 1\n",
    "        return 1\n",
    "    elif x.values[-1] < x.values[0]:\n",
    "        prev = -1\n",
    "        return -1\n",
    "    else:\n",
    "        return prev\n",
    "#df = pd.read_csv('Dec.csv')\n",
    "#df[\"Dollar\"] = df.apply(lambda x: total_dollar(x.AskVolume,x.BidVolume,x.Ask,x.Bid), axis = 1)\n",
    "def tick_preprocess(df):\n",
    "    df[\"Price\"] = df.apply(lambda x: price(x.Ask,x.Bid), axis = 1)\n",
    "    return df\n",
    "#df.to_csv(\"test_v6.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tick_imbalance_candle(df):\n",
    "    opener = []\n",
    "    close = []\n",
    "    high = []\n",
    "    low = []\n",
    "    tick_count = []\n",
    "    tema = []\n",
    "\n",
    "    start = 0\n",
    "    cum_tsign = []\n",
    "    csize = 1500\n",
    "    for i, t in df.iterrows():\n",
    "        cum_tsign += [t.tsign]\n",
    "        jjj = t.tsign_ema\n",
    "        if abs(sum(cum_tsign)) > abs(csize*jjj):\n",
    "            buff_df = df.iloc[start:i+1]\n",
    "            #print(str(start) + '----' + str(i+1))\n",
    "            start = i + 1\n",
    "            opener += [buff_df.Price.tolist()[0]]\n",
    "            close += [buff_df.Price.tolist()[-1]]\n",
    "            high += [max(buff_df.Price.tolist())]\n",
    "            low += [min(buff_df.Price.tolist())]\n",
    "            tema += [t.tsign_ema]\n",
    "            tick_count += [len(buff_df.Ask.tolist())]\n",
    "            cum_tsign = []\n",
    "    start = 0\n",
    "    imb = pd.DataFrame({'Open':opener,'High':high,'Low':low,'Close':close, 'tsign_ema':tema,'tick_count':tick_count})\n",
    "    imb['us'] = imb.apply(lambda x: upper_shadow(x.Open, x.Close, x.High), axis = 1)\n",
    "    imb['ls'] = imb.apply(lambda x: lower_shadow(x.Open, x.Close, x.Low), axis = 1)\n",
    "    imb['rb'] = imb.apply(lambda x: real_body(x.Open, x.Close), axis = 1)\n",
    "    return imb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev = 1\n",
    "signs_cum = []\n",
    "signs = []\n",
    "q = 1\n",
    "exp_size = 1500\n",
    "boundary_labs = []\n",
    "def signer(x):\n",
    "    global prev\n",
    "    if x.values[-1] > x.values[0]:\n",
    "        prev = 1\n",
    "        return 1\n",
    "    elif x.values[-1] < x.values[0]:\n",
    "        prev = -1\n",
    "        return -1\n",
    "    else:\n",
    "        return prev\n",
    "\n",
    "def bnd(x):\n",
    "    global signs_cum\n",
    "    global signs\n",
    "    global q\n",
    "    global exp_size\n",
    "    global boundary_labs\n",
    "    exp_size = 1500\n",
    "    if abs(signs_cum[x.name]) > abs(exp_size * x.tsign_ema):\n",
    "        if x.name+1 < len(signs_cum):\n",
    "            q = q+1\n",
    "            signs_cum[x.name+1:] = list(np.cumsum(signs[x.name+1:]))\n",
    "            boundary_labs[x.name+1:] = [q] * len(boundary_labs[x.name+1:])\n",
    "        return 'break' + '--' + str(abs(signs_cum[x.name])) + '--' + str(abs(exp_size * x.tsign_ema))\n",
    "    else:\n",
    "        return 'pass'\n",
    "    \n",
    "def tick_imbalance_candle_v2(df):\n",
    "    global signs_cum\n",
    "    global signs\n",
    "    global q\n",
    "    global exp_size\n",
    "    global boundary_labs\n",
    "    df['tsign_cum'] = df.tsign.cumsum()\n",
    "    signs_cum = df.tsign_cum.tolist()\n",
    "    signs = df.tsign.tolist()\n",
    "    boundary_labs = [q] * len(df)\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    df['boundary'] = df.apply(lambda x: bnd(x), axis = 1)\n",
    "    df['tsign_cum_v2'] = signs_cum\n",
    "    df['boundary_labs'] = boundary_labs\n",
    "    signs_cum = []\n",
    "    sizes = []\n",
    "    df['shifted_boundary'] = df.boundary_labs.shift(1)\n",
    "    df['grouper_check'] = df.shifted_boundary != df.boundary_labs\n",
    "    df['grouper'] = df.grouper_check.cumsum()\n",
    "    fin_df = df.groupby('grouper')['Ask'].agg(['min', 'max', 'first', 'last', 'size'])\n",
    "    fin_df.columns = ['Low', 'High', 'Open', 'Close', 'Size']\n",
    "    fin_df['us'] = fin_df.apply(lambda x: upper_shadow(x.Open, x.Close, x.High), axis = 1)\n",
    "    fin_df['ls'] = fin_df.apply(lambda x: lower_shadow(x.Open, x.Close, x.Low), axis = 1)\n",
    "    fin_df['rb'] = fin_df.apply(lambda x: real_body(x.Open, x.Close), axis = 1)\n",
    "    df = pd.DataFrame({'a':[]})\n",
    "    return fin_df\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candle_form(df):\n",
    "    opener = []\n",
    "    high = []\n",
    "    low = []\n",
    "    close = []\n",
    "    rb = []\n",
    "    us = []\n",
    "    ls = []\n",
    "    dur = []\n",
    "    counter = 0\n",
    "    price = df.Price.tolist()\n",
    "\n",
    "    buff = 0\n",
    "    start = 0\n",
    "    min_max = df.groupby(np.arange(len(df))//300)['Price'].agg(['min','max'])\n",
    "    op = df.groupby(np.arange(len(df))//300)['Price'].first()\n",
    "    cl = df.groupby(np.arange(len(df))//300)['Price'].last()\n",
    "    df = pd.concat([min_max, op,cl], axis = 1)\n",
    "    df.columns = ['Low','High','Open','Close']\n",
    "    df['us'] = df.apply(lambda x: upper_shadow(x.Open, x.Close, x.High), axis = 1)\n",
    "    df['ls'] = df.apply(lambda x: lower_shadow(x.Open, x.Close, x.Low), axis = 1)\n",
    "    df['rb'] = df.apply(lambda x: real_body(x.Open, x.Close), axis = 1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indicator_fill(df):\n",
    "    a = ta.trend.ema_indicator(close=df.Close,window=5)\n",
    "    f = ta.trend.sma_indicator(close=df.Close,window=50)\n",
    "    i = ta.trend.sma_indicator(close=df.Close,window=20)\n",
    "    c = ta.volatility.average_true_range(high=df.High,low=df.Low,close=df.Close)\n",
    "    df[\"5_EMA\"] = a\n",
    "    df[\"50_SMA\"] = f\n",
    "    df[\"20_SMA\"] = i\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "side_le = LabelEncoder()\n",
    "\n",
    "\n",
    "\n",
    "def label_calc_v2(df):\n",
    "    df['last_close'] = df.Close.rolling(window = 10).apply(lambda x: x.values[-1])\n",
    "    df.last_close = df.last_close.shift(-10)\n",
    "    df = df[:-10]\n",
    "    df['Labels'] = df.apply(lambda x: 'Buy' if x.last_close > x.Close else 'Sell', axis = 1)\n",
    "    df['Labels'] = side_le.fit_transform(df['Labels'].tolist())\n",
    "    with open(\"side_label_le_tick_spx.pkl\",\"wb\") as f:\n",
    "        pickle.dump(side_le,f)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing function that prepares the training data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = []\n",
    "labs = []\n",
    "from numpy_ext import rolling_apply\n",
    "def veccr(c,h,l,o,rb,ls,us,ma):\n",
    "    global vec\n",
    "    vec += [np.hstack((c,h,l,o,rb,ls,us,ma))]\n",
    "    return 0\n",
    "\n",
    "def labr(x):\n",
    "    global labs\n",
    "    labs += [x.values[-1]]\n",
    "    return 0\n",
    "\n",
    "def df_vectorize_v2(df):\n",
    "    side = []\n",
    "\n",
    "    \n",
    "    global vec\n",
    "    global labs\n",
    "    \n",
    "    rolling_apply(veccr, 10, df.Close.values, df.High.values, df.Low.values, df.Open.values, df.rb.values, df.ls.values\n",
    "                 , df.us.values, df['20_SMA'].values)\n",
    "    \n",
    "    \n",
    "    df.Labels.rolling(10).apply(labr)\n",
    "    side = labs\n",
    "    labs = []\n",
    "    \n",
    "    fin_vec = np.array(vec)\n",
    "    vec = []\n",
    "    \n",
    "    return fin_vec, side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling the tick data and converting them in to training data using the above helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "df_collect = []\n",
    "vectors = []\n",
    "sides = []\n",
    "metas = []\n",
    "bmv = []\n",
    "smv = []\n",
    "bml = []\n",
    "sml = []\n",
    "from fracdiff import FracdiffStat\n",
    "cf = FracdiffStat()\n",
    "hf = FracdiffStat()\n",
    "of = FracdiffStat()\n",
    "lf = FracdiffStat()\n",
    "mf = FracdiffStat()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "for root, dirs, files in os.walk(r'C:\\Users\\91984\\Desktop\\Bot ML\\Training Data\\Ticks Data S&P500 - Copy'):\n",
    "    if not files:\n",
    "        continue\n",
    "    else:\n",
    "        for f in files:\n",
    "            df_collect += [pd.read_csv(root+'\\\\'+f)]\n",
    "        df = pd.concat(df_collect)\n",
    "        df.reset_index(drop = True, inplace = True)\n",
    "        df = tick_preprocess(df)\n",
    "        df = candle_form(df)\n",
    "        if(len(df)<50):\n",
    "            continue\n",
    "        df = indicator_fill(df)\n",
    "        df = df[20:]\n",
    "        df = label_calc_v2(df)\n",
    "        df.Close = list(np.squeeze(cf.fit_transform(np.array(df.Close).reshape(-1,1)), axis = 1))\n",
    "        df.High = list(np.squeeze(hf.fit_transform(np.array(df.High).reshape(-1,1)), axis = 1))\n",
    "        df.Open = list(np.squeeze(of.fit_transform(np.array(df.Open).reshape(-1,1)), axis = 1))\n",
    "        df.Low = list(np.squeeze(lf.fit_transform(np.array(df.Low).reshape(-1,1)), axis = 1))\n",
    "        df['20_SMA'] = list(np.squeeze(mf.fit_transform(np.array(df['20_SMA']).reshape(-1,1)), axis = 1))\n",
    "        df = df[1:]\n",
    "        #df = frac_diff_calc(df)\n",
    "        v, s = df_vectorize_v2(df)\n",
    "        vectors += [v]\n",
    "        sides += s\n",
    "        df_collect = []\n",
    "        print(f + ' ' + str(vectors[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalizing and storing the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec = np.vstack((vectors))\n",
    "side_lab = sides\n",
    "print(train_vec.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "side_lab = list(map(lambda x: int(x), side_lab))\n",
    "\n",
    "ctr2 = Counter(side_lab)\n",
    "\n",
    "print(ctr2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE Class Imbalance Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.under_sampling import NearMiss\n",
    "ov1 = SMOTE()\n",
    "undersample = NearMiss(version=3, n_neighbors_ver3=3)\n",
    "#train_vec = np.hstack((train_vec,side_lab.reshape(-1,1)))\n",
    "train_vec = np.nan_to_num(train_vec)\n",
    "side_vec, side_lab = undersample.fit_resample(train_vec,side_lab)\n",
    "#mv, ml = ov2.fit_resample(train_vec,meta_lab)\n",
    "#smv, sml = ov3.fit_resample(smv,sml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr2 = Counter(side_lab)\n",
    "\n",
    "print(ctr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and processing the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "#fin_vec, labels = sklearn.datasets.load_svmlight_file(\"fracdiff_side_prediction.txt.train\")\n",
    "\n",
    "side_lab = np.array(side_lab, dtype='int32')\n",
    "print(side_lab.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_vec, side_lab, test_size=0.1, random_state=42, stratify=side_lab)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST SKLEARN API training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from joblib import dump\n",
    "from joblib import load\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators = 1000, max_depth = 100, max_features = 0.1, bootstrap = True, \n",
    "                            n_jobs = -1, verbose = 2, max_samples = 0.05, random_state = 42)\n",
    "xgbclasser = XGBClassifier(max_depth=100, objective='multi:softprob', use_label_encoder=False,num_class=2,n_estimators=100)\n",
    "#xgbclasser.fit(X_train,y_train,eval_set=[(X_test,y_test)],early_stopping_rounds=30)\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "rf.fit(X_train,y_train)\n",
    "rf.score(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "dump(rf, 'RF_SPX_tick_classifier.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "X_test = np.nan_to_num(X_test)\n",
    "rf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = BaggingClassifier(n_estimators = 200, max_features = 0.25, bootstrap = True, \n",
    "                            n_jobs = -1, verbose = 2, max_samples = 0.2, random_state = 42, bootstrap_features= True)\n",
    "bc.fit(X_train,y_train)\n",
    "bc.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump(xgbclasser, \"fracdiff_side_prediction_xgbclasser.model\")\n",
    "dump(xgbclasser, \"10_lookback_1500_tick_imbalance_spx.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50 lookback predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "df_collect = []\n",
    "vectors = []\n",
    "sides = []\n",
    "metas = []\n",
    "bmv = []\n",
    "smv = []\n",
    "bml = []\n",
    "sml = []\n",
    "for root, dirs, files in os.walk(r'C:\\Users\\91984\\Desktop\\Bot ML\\FX_Advanced_ML\\Jan 2021 S_P 500'):\n",
    "    if not files:\n",
    "        continue\n",
    "    else:\n",
    "        for f in files:\n",
    "            df_collect += [pd.read_csv(root+'\\\\'+f)]\n",
    "        df = pd.concat(df_collect)\n",
    "        df.reset_index(drop = True, inplace = True)\n",
    "        df = tick_preprocess(df)\n",
    "        df = candle_form(df)\n",
    "        if(len(df)<50):\n",
    "            continue\n",
    "        df = indicator_fill(df)\n",
    "        df = df[20:]\n",
    "        df = label_calc_v2(df)\n",
    "        df.Close = list(np.squeeze(cf.fit_transform(np.array(df.Close).reshape(-1,1)), axis = 1))\n",
    "        df.High = list(np.squeeze(hf.fit_transform(np.array(df.High).reshape(-1,1)), axis = 1))\n",
    "        df.Open = list(np.squeeze(of.fit_transform(np.array(df.Open).reshape(-1,1)), axis = 1))\n",
    "        df.Low = list(np.squeeze(lf.fit_transform(np.array(df.Low).reshape(-1,1)), axis = 1))\n",
    "        df['20_SMA'] = list(np.squeeze(mf.fit_transform(np.array(df['20_SMA']).reshape(-1,1)), axis = 1))\n",
    "        df = df[1:]\n",
    "        #df = frac_diff_calc(df)\n",
    "        v, s = df_vectorize_v2(df)\n",
    "        vectors += [v]\n",
    "        sides += s\n",
    "        df_collect = []\n",
    "        print(f + ' ' + str(vectors[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "import pickle\n",
    "model_1 = load(\"10_lookback_1500_tick_imbalance_spx.model\")\n",
    "\n",
    "\n",
    "with open(\"side_label_le_tick_spx.pkl\",\"rb\") as f:\n",
    "    side_le = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "test_s = np.array(sides)\n",
    "test_v = np.vstack((vectors))\n",
    "test_v = np.nan_to_num(test_v)\n",
    "preds = rf.predict(test_v)\n",
    "probs = np.max(rf.predict_proba(test_v),axis=1)\n",
    "\n",
    "probs = list(probs)\n",
    "from sklearn.metrics import precision_score, \\\n",
    "    recall_score, confusion_matrix, classification_report, \\\n",
    "    accuracy_score, f1_score\n",
    "\n",
    "print(accuracy_score(test_s, preds))\n",
    "print(classification_report(test_s, preds))\n",
    "\n",
    "predictions = preds.astype(int)\n",
    "predictions = side_le.inverse_transform(list(predictions))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ab_df = pd.DataFrame({\"Orig\":sides,\"Pred\":predictions,\"Pred_Probs\":probs})\n",
    "ab_df.to_csv(\"rf_tick_spx_output.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bst = xgb.Booster({'nthread':4})\n",
    "#bst.load_model('side_prediction_xgb.model')\n",
    "#side_predict_sk = load('fracdiff_side_prediction_xgbclasser.model')\n",
    "side_predict_sk = load('fracdiff_side_prediction_xgbclasser_interaction_features.model')\n",
    "#bm_predict_sk = load('buy_meta_prediction_xgbclasser.model')\n",
    "#sm_predict_sk = load('sell_meta_prediction_xgbclasser.model')\n",
    "\n",
    "\n",
    "pred_side_sk = side_predict_sk.predict(tv)\n",
    "prob_side_sk = np.max(side_predict_sk.predict_proba(tv), axis = 1)\n",
    "\n",
    "#buy_pred_meta_sk = bm_predict_sk.predict(bmvp)\n",
    "#buy_prob_meta_sk = np.max(bm_predict_sk.predict_proba(bmvp), axis = 1)\n",
    "\n",
    "#sell_pred_meta_sk = sm_predict_sk.predict(smvp)\n",
    "#sell_prob_meta_sk = np.max(sm_predict_sk.predict_proba(smvp), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_predict_sk = load('fracdiff_side_prediction_xgbclasser_interaction_features.model')\n",
    "df = pd.read_csv('Jan_Test.csv')\n",
    "df = tick_preprocess(df)\n",
    "df = candle_bar(df)\n",
    "df = indicator_fill(df)\n",
    "df, _, _ = label_calc(df)\n",
    "df = frac_diff_calc(df)\n",
    "\n",
    "test_v, test_s, test_m = df_vectorize(df)\n",
    "test_v = np.array(test_v)\n",
    "preds = side_predict_sk.predict(test_v)\n",
    "probs = np.max(side_predict_sk.predict_proba(test_v), axis = 1)\n",
    "\n",
    "preds = preds.astype(int)\n",
    "preds = side_le.inverse_transform(list(preds))\n",
    "probs = list(probs)\n",
    "df1 = df[20:]\n",
    "df1[\"SK_Side_Prediction\"] = preds\n",
    "df1[\"SK_Side_Probability\"] = probs\n",
    "df1.to_csv('Jan_Test_preds.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, \\\n",
    "    recall_score, confusion_matrix, classification_report, \\\n",
    "    accuracy_score, f1_score\n",
    "\n",
    "print(accuracy_score(ts, pred_side_sk))\n",
    "print(classification_report(ts, pred_side_sk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from joblib import dump\n",
    "from joblib import load\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "xgclass = XGBClassifier(max_depth=100, objective='multi:softprob', use_label_encoder=False,num_class=4, n_estimators = 10)\n",
    "rf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "lr = LogisticRegression()\n",
    "lvc = LinearSVC(random_state=42)\n",
    "bc = BaggingClassifier(random_state=42)\n",
    "#estimators = [('rf', rf),('xgb', xgclass),('lr', lr)]\n",
    "estimators = [('rf', rf),('lvc', lvc),('bc', bc),('lr', lr)]\n",
    "clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
    "\n",
    "clf.fit(side_vec,side_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(clf,'StackingClassifier.DAT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, \\\n",
    "    recall_score, confusion_matrix, classification_report, \\\n",
    "    accuracy_score, f1_score\n",
    "\n",
    "print(accuracy_score(ts, pred_side_sk))\n",
    "print(classification_report(ts, pred_side_sk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(df))\n",
    "df1 = df[50:]\n",
    "print(len(df1))\n",
    "print(len(df))\n",
    "#bdf = buy_df[50:]\n",
    "#sdf = sell_df[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pred_side_sk = pred_side_sk.astype(int)\n",
    "pred_side_sk = side_le.inverse_transform(list(pred_side_sk))\n",
    "prob_side_sk = list(prob_side_sk)\n",
    "\n",
    "#buy_pred_meta_sk = buy_pred_meta_sk.astype(int)\n",
    "#buy_pred_meta_sk = meta_le.inverse_transform(list(buy_pred_meta_sk))\n",
    "#buy_prob_meta_sk = list(buy_prob_meta_sk)\n",
    "\n",
    "#sell_pred_meta_sk = sell_pred_meta_sk.astype(int)\n",
    "#sell_pred_meta_sk = meta_le.inverse_transform(list(sell_pred_meta_sk))\n",
    "#sell_prob_meta_sk = list(sell_prob_meta_sk)\n",
    "\n",
    "\n",
    "#df1[\"BST_Side_Prediction\"] = pred_bst\n",
    "df1[\"SK_Side_Prediction\"] = pred_side_sk\n",
    "df1[\"SK_Side_Probability\"] = prob_side_sk\n",
    "#bdf[\"SK_Meta_Prediction\"] = buy_pred_meta_sk\n",
    "#bdf[\"SK_Meta_Probability\"] = buy_prob_meta_sk\n",
    "#sdf[\"SK_Meta_Prediction\"] = sell_pred_meta_sk\n",
    "#sdf[\"SK_Meta_Probability\"] = sell_prob_meta_sk\n",
    "df1.to_csv('Jan_Test_preds.csv', index = False)\n",
    "#bdf.to_csv('Jan_Test_buy_meta.csv', index = False)\n",
    "#sdf.to_csv('Jan_Test_sell_meta.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## positional encoding for transformer encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = []\n",
    "inner = []\n",
    "import math\n",
    "import numpy as np\n",
    "for p in range(550):\n",
    "    for i in range(0,8,2):\n",
    "        inner += [math.sin(p/(10000**(i/32)))]\n",
    "        inner += [math.cos(p/(10000**(i/32)))]\n",
    "    \n",
    "    pe += [inner]\n",
    "    inner = []\n",
    "pe = np.array(pe)\n",
    "pe = np.expand_dims(pe,axis=0)\n",
    "pe = np.repeat(pe,fin_vec.shape[0],axis = 0)\n",
    "print(pe.shape)\n",
    "#xt1 = np.expand_dims(X_train,axis = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import datasets\n",
    "fin_vec, labels = sklearn.datasets.load_svmlight_file(\"side_prediction.txt.train\")\n",
    "print(fin_vec.shape)\n",
    "#X_train_positional = np.add(pe,X_train)\n",
    "#print(X_train_positional.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_vec = fin_vec.toarray()\n",
    "print(fin_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_vec = np.repeat(fin_vec,8, axis = 2)\n",
    "print(fin_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_vec = fin_vec + pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=8, nhead=8)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "src = torch.rand(550, 32, 8)\n",
    "out = transformer_encoder(src)\n",
    "print(out.shape)\n",
    "fwd = nn.Linear(550*8,128)\n",
    "out = fwd(out.view(out.shape[1],-1))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optimizer\n",
    "import numpy as np\n",
    "from torchvision import transforms,models,datasets\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "from torchvision import transforms,models,datasets\n",
    "class Tick_encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Tick_encoder, self).__init__()\n",
    "\n",
    "        \n",
    "        self.encoder = nn.TransformerEncoderLayer(d_model=8, nhead=8)\n",
    "        self.trans_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.fc1 = nn.Linear(550 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 2)\n",
    "\n",
    "\n",
    "    def forward(self, state):\n",
    "        \n",
    "        out = self.trans_encoder(state)\n",
    "        out = F.relu(self.fc1(out.view(out.shape[1],-1)))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Tick_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_t = T.FloatTensor(fin_vec)\n",
    "labs = T.LongTensor(labels)\n",
    "print(X_t.shape)\n",
    "train_data = TensorDataset(X_t,labs)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "save_location = 'C:\\\\Users\\\\91984\\\\Desktop\\\\Bot ML\\\\FX_Advanced_ML\\\\Torch_Encoder_Model'\n",
    "\n",
    "#model = T.load(save_location+'\\\\'+'GAF_CNN_trading_model_target.pth')\n",
    "trainer = optimizer.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(20):\n",
    "    running_loss = 0\n",
    "    acc = 0\n",
    "    for i, (batch,labs) in enumerate(train_loader):\n",
    "        \n",
    "        trainer.zero_grad()\n",
    "        out1 = model(batch.reshape(550,-1,8))\n",
    "        loss = criterion(out1,labs)\n",
    "        loss.backward()\n",
    "        trainer.step()\n",
    "        running_loss += loss.item()\n",
    "        out1 = T.nn.functional.softmax(out1,dim=1)\n",
    "        Conf_score,prediction = T.max(out1,1)\n",
    "        tot_score = sum(x.item()==y.item() for x,y in zip(prediction,labs))\n",
    "        acc += tot_score/len(labs)\n",
    "        if i%1==0:\n",
    "            print('Processing batch '+str(i+1)+' of '+str(98273//32)+\n",
    "                  ' batches in Epoch '+str(e+1)+' with Loss: '+\n",
    "                  str(running_loss/200)+' with accuracy: '+str(acc/200))\n",
    "            running_loss = 0\n",
    "            acc = 0\n",
    "print('Saving The Model')\n",
    "T.save(model,save_location+'\\\\'+'Tick_transformer_Encoder.pth')\n",
    "print('Training Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df1.pos_run_agg_EMA_5.tolist()[200:240], label=\"line1\")\n",
    "plt.plot(df1.pos_run_agg_SMA_100.tolist()[200:240], label=\"line2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "buy_go = pd.read_csv(\"buy_go.csv\")\n",
    "buy_no_go = pd.read_csv(\"buy_no_go.csv\")\n",
    "sell_go = pd.read_csv(\"Sell_Go.csv\")\n",
    "sell_no_go = pd.read_csv(\"Sell_No_Go.csv\")\n",
    "buy_near = pd.read_csv(\"Buy_near_sell.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Candlestick(open=buy_go['Open'], high=buy_go['High'],\n",
    "                low=buy_go['Low'], close=buy_go['Close'])\n",
    "                     ])\n",
    "\n",
    "fig.update_layout(xaxis_rangeslider_visible=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
