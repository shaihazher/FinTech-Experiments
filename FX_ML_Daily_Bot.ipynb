{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import time\n",
    "import datetime\n",
    "import ta\n",
    "def upper_shadow(opener, close, high):\n",
    "    if opener >= close:\n",
    "        return abs(opener-high)\n",
    "    else:\n",
    "        return abs(close-high)\n",
    "\n",
    "def lower_shadow(opener, close, low):\n",
    "    if opener <= close:\n",
    "        return abs(opener-low)\n",
    "    else:\n",
    "        return abs(close-low)\n",
    "\n",
    "def real_body(opener, close):\n",
    "    return abs(opener-close)\n",
    "\n",
    "def total_dollar(av,bv,a,b):\n",
    "    p = (a+b)/2.0\n",
    "    v = av+bv\n",
    "    return p*v\n",
    "\n",
    "def log_dollar(a,b):\n",
    "    p = (a+b)/2.0\n",
    "    return np.log(p)\n",
    "\n",
    "def volumen(av,bv):\n",
    "    return av+bv\n",
    "\n",
    "def price(a,b):\n",
    "    g = (a+b)/2.0\n",
    "    return b\n",
    "\n",
    "#df = pd.read_csv('Dec.csv')\n",
    "#df[\"Dollar\"] = df.apply(lambda x: total_dollar(x.AskVolume,x.BidVolume,x.Ask,x.Bid), axis = 1)\n",
    "def tick_preprocess(df):\n",
    "    df[\"Price\"] = df.apply(lambda x: price(x.Ask,x.Bid), axis = 1)\n",
    "    return df\n",
    "#df.to_csv(\"test_v6.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indicator_fill(df):\n",
    "    a = ta.trend.ema_indicator(close=df.Close,window=5)\n",
    "    f = ta.trend.sma_indicator(close=df.Close,window=50)\n",
    "    i = ta.trend.sma_indicator(close=df.Close,window=20)\n",
    "    c = ta.volatility.average_true_range(high=df.High,low=df.Low,close=df.Close)\n",
    "    df[\"5_EMA\"] = a\n",
    "    df[\"50_SMA\"] = f\n",
    "    df[\"20_SMA\"] = i\n",
    "    df['ATR'] = c\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buy_calc_side_v(hi,lo,clo,start):\n",
    "    if statistics.median(clo[1:]) > start:\n",
    "        return \"buy\"\n",
    "    else:\n",
    "        return \"wrong buy\"\n",
    "\n",
    "def sell_calc_side_v(hi,lo,clo,start):\n",
    "    if statistics.median(clo[1:]) < start:\n",
    "        return \"sell\"\n",
    "    else:\n",
    "        return \"wrong sell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import statistics\n",
    "def buy_quality(hi,lo,clo,start,bl):\n",
    "    if bl =='buy' and abs(max(hi[1:]) - start) > abs(min(lo[1:]) - start):\n",
    "        return \"go buy\"\n",
    "    else:\n",
    "        return \"wrong go buy\"\n",
    "\n",
    "def sell_quality(hi,lo,clo,start,sl):\n",
    "    if sl == 'sell' and abs(min(lo[1:]) - start) > abs(max(hi[1:]) - start):\n",
    "        return \"go sell\"\n",
    "    else:\n",
    "        return \"wrong go sell\"\n",
    "\n",
    "def mover(hi,lo,clo,start):\n",
    "    up_move = max(hi[1:]) - start\n",
    "    low_move = start - min(lo[1:])\n",
    "    return up_move, low_move\n",
    "\n",
    "def buy_calc_side(hi,lo,clo,start):\n",
    "    if statistics.median(clo[1:]) > start:\n",
    "        return \"buy\"\n",
    "    else:\n",
    "        return \"wrong buy\"\n",
    "\n",
    "def sell_calc_side(hi,lo,clo,start):\n",
    "    if statistics.median(clo[1:]) < start:\n",
    "        return \"sell\"\n",
    "    else:\n",
    "        return \"wrong sell\"\n",
    "    \n",
    "def label_calc(df):\n",
    "    buy_lab = []\n",
    "    sell_lab = []\n",
    "    bq = []\n",
    "    sq = []\n",
    "    go = []\n",
    "    profit = []\n",
    "    loss = []\n",
    "    k = 10\n",
    "    for t in range(0,len(df)):\n",
    "        if t >= len(df) - k:\n",
    "            break\n",
    "        a1 = df[t:t+k]\n",
    "        a0 = df[t:t+k]\n",
    "        st = a0.Close.tolist()[0]\n",
    "        hi = a1.High.tolist()\n",
    "        lo = a1.Low.tolist()\n",
    "        clo = a1.Close.tolist()\n",
    "        buy_label = buy_calc_side(hi,lo,clo,st)\n",
    "        sell_label = sell_calc_side(hi,lo,clo,st)\n",
    "        b_quality = buy_quality(hi,lo,clo,st,buy_label)\n",
    "        s_quality = sell_quality(hi,lo,clo,st,sell_label)\n",
    "        up_move, low_move = mover(hi,lo,clo,st)\n",
    "        \n",
    "        \n",
    "        buy_lab += [buy_label]\n",
    "        sell_lab += [sell_label]\n",
    "        bq += [b_quality]\n",
    "        sq += [s_quality]\n",
    "        \n",
    "        \n",
    "        profit += [up_move]\n",
    "        loss += [low_move]\n",
    "\n",
    "    if len(buy_lab) < len(df):\n",
    "        buy_lab += [\"Pass\"] * (len(df)-len(buy_lab))\n",
    "    if len(sell_lab) < len(df):\n",
    "        sell_lab += [\"Pass\"] * (len(df)-len(sell_lab))\n",
    "    if len(sq) < len(df):\n",
    "        sq += [\"Pass\"] * (len(df)-len(sq))\n",
    "    if len(bq) < len(df):\n",
    "        bq += [\"Pass\"] * (len(df)-len(bq))\n",
    "    if len(profit) < len(df):\n",
    "        profit += [0] * (len(df)-len(profit))\n",
    "    if len(loss) < len(df):\n",
    "        loss += [0] * (len(df)-len(loss))\n",
    "\n",
    "    df[\"buy_label\"] = buy_lab\n",
    "    df[\"sell_label\"] = sell_lab\n",
    "    df[\"bu_quality\"] = bq\n",
    "    df[\"se_quality\"] = sq\n",
    "    df['Profit'] = profit\n",
    "    df['Loss'] = loss\n",
    "    \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_vectorize(df):\n",
    "    vec_load = []\n",
    "    m5_vec = []\n",
    "    buy_side = []\n",
    "    sell_side = []\n",
    "    b_quality = []\n",
    "    s_quality = []\n",
    "    take_profit = []\n",
    "    stop_loss = []\n",
    "    print(len(df))\n",
    "    for h in range(52,len(df)):\n",
    "        buff_df = df[h-52:h]\n",
    "        if buff_df.buy_label.tolist()[-1] != \"Skip\":\n",
    "            \n",
    "            orig_close = buff_df.cl.tolist()\n",
    "            orig_hi = buff_df.hi.tolist()\n",
    "            orig_lo = buff_df.lo.tolist()\n",
    "            orig_op = buff_df.op.tolist()\n",
    "            close_log = buff_df.Close.tolist()\n",
    "            hi = buff_df.High.tolist()\n",
    "            lo = buff_df.Low.tolist()\n",
    "            op = buff_df.Open.tolist()\n",
    "            rb_vec = buff_df.rb.tolist()\n",
    "            us_vec = buff_df.us.tolist()\n",
    "            ls_vec = buff_df.ls.tolist()\n",
    "            pct = buff_df.pct.tolist()\n",
    "            t_ma = buff_df['20_SMA'].tolist()\n",
    "            f_ma = buff_df['5_EMA'].tolist()\n",
    "            sma = buff_df.sma.tolist()\n",
    "            fema = buff_df.fema.tolist()\n",
    "            atr = buff_df.ATR.tolist()\n",
    "            ma_diff = buff_df.ma_diff.tolist()\n",
    "            tclose_diff = buff_df.tclose_diff.tolist()\n",
    "            fclose_diff = buff_df.fclose_diff.tolist()\n",
    "            med = statistics.median(buff_df.cl.tolist())\n",
    "            maximum = max(buff_df.cl.tolist())\n",
    "            minimum = min(buff_df.cl.tolist())\n",
    "            stand_dev = statistics.stdev(buff_df.cl.tolist())\n",
    "            \n",
    "            buy_side += [buff_df.buy_label.tolist()[-1]]\n",
    "            sell_side += [buff_df.sell_label.tolist()[-1]]\n",
    "            b_quality += [buff_df.bu_quality.tolist()[-1]]\n",
    "            s_quality += [buff_df.se_quality.tolist()[-1]]\n",
    "            take_profit += [buff_df.Profit.tolist()[-1]]\n",
    "            stop_loss += [buff_df.Loss.tolist()[-1]]\n",
    "\n",
    "            #vec_load += [close_log + rb_vec + us_vec + ls_vec + ma_diff + tclose_diff + fclose_diff]\n",
    "            m5_vec += [rb_vec + us_vec + ls_vec + ma_diff + atr + pct]\n",
    "\n",
    "        else:\n",
    "            print('Skip')\n",
    "    print('Vector_Done')\n",
    "    vec_load = m5_vec\n",
    "    return vec_load, buy_side, sell_side, b_quality, s_quality, take_profit, stop_loss\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy_ext import rolling_apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fracdiff import FracdiffStat\n",
    "cf = FracdiffStat()\n",
    "hf = FracdiffStat()\n",
    "of = FracdiffStat()\n",
    "lf = FracdiffStat()\n",
    "mf = FracdiffStat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label done\n",
      "closefracdiff\n",
      "hifracdiff\n",
      "openfracdiff\n",
      "lowfracdiff\n",
      "tema\n",
      "fema\n",
      "2347\n",
      "Vector_Done\n"
     ]
    }
   ],
   "source": [
    "#df = pd.read_csv('SPX_5min_last_3.csv')\n",
    "#df = df[~((df.Close == df.High) & (df.Close == df.Low) & (df.Close == df.Open))]\n",
    "df = pd.read_csv('last_10_train.csv')\n",
    "df.Close = list(reversed(df.Close.tolist()))\n",
    "df.Open = list(reversed(df.Open.tolist()))\n",
    "df.High = list(reversed(df.High.tolist()))\n",
    "df.Low = list(reversed(df.Low.tolist()))\n",
    "df = label_calc(df)\n",
    "df = indicator_fill(df)\n",
    "df = df[20:]\n",
    "df = df[df.buy_label!='Pass']\n",
    "print('label done')\n",
    "df['cl'] = df.Close\n",
    "df['pct'] = df.Close.pct_change()\n",
    "df['hi'] = df.High\n",
    "df['lo'] = df.Low\n",
    "df['op'] = df.Open\n",
    "df['sma'] = df['20_SMA']\n",
    "df['fema'] = df['5_EMA']\n",
    "\n",
    "df['us'] = df.apply(lambda x: upper_shadow(x.Open, x.Close, x.High), axis = 1)\n",
    "df['ls'] = df.apply(lambda x: lower_shadow(x.Open, x.Close, x.Low), axis = 1)\n",
    "df['rb'] = df.apply(lambda x: real_body(x.Open, x.Close), axis = 1)\n",
    "df['ma_diff'] = df['5_EMA'] - df['20_SMA']\n",
    "df['tclose_diff'] = df['Close'] - df['20_SMA']\n",
    "df['fclose_diff'] = df['Close'] - df['5_EMA']\n",
    "print('closefracdiff')\n",
    "df.Close = list(np.squeeze(cf.fit_transform(np.array(df.Close).reshape(-1,1)), axis = 1))\n",
    "print('hifracdiff')\n",
    "df.High = list(np.squeeze(cf.transform(np.array(df.High).reshape(-1,1)), axis = 1))\n",
    "print('openfracdiff')\n",
    "df.Open = list(np.squeeze(cf.transform(np.array(df.Open).reshape(-1,1)), axis = 1))\n",
    "print('lowfracdiff')\n",
    "df.Low = list(np.squeeze(cf.transform(np.array(df.Low).reshape(-1,1)), axis = 1))\n",
    "print('tema')\n",
    "df['20_SMA'] = list(np.squeeze(cf.transform(np.array(df['20_SMA']).reshape(-1,1)), axis = 1))\n",
    "print('fema')\n",
    "df['5_EMA'] = list(np.squeeze(cf.transform(np.array(df['5_EMA']).reshape(-1,1)), axis = 1))\n",
    "df = df[1:]\n",
    "v,bs,ss,bq_vec,sq_vec,tp,sl = df_vectorize(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "160\n",
      "149\n",
      "149\n",
      "Vector_Done\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('last_10_test.csv')\n",
    "df.Close = list(reversed(df.Close.tolist()))\n",
    "df.Open = list(reversed(df.Open.tolist()))\n",
    "df.High = list(reversed(df.High.tolist()))\n",
    "df.Low = list(reversed(df.Low.tolist()))\n",
    "df = label_calc(df)\n",
    "df = indicator_fill(df)\n",
    "print(len(df))\n",
    "df = df[20:]\n",
    "print(len(df))\n",
    "df = df[df.buy_label!='Pass']\n",
    "\n",
    "df['cl'] = df.Close\n",
    "df['pct'] = df.Close.pct_change()\n",
    "df['hi'] = df.High\n",
    "df['lo'] = df.Low\n",
    "df['op'] = df.Open\n",
    "df['sma'] = df['20_SMA']\n",
    "df['fema'] = df['5_EMA']\n",
    "\n",
    "\n",
    "df['ma_diff'] = df['5_EMA'] - df['20_SMA']\n",
    "df['tclose_diff'] = df['Close'] - df['20_SMA']\n",
    "df['fclose_diff'] = df['Close'] - df['5_EMA']\n",
    "df.Close = list(np.squeeze(cf.fit_transform(np.array(df.Close).reshape(-1,1)), axis = 1))\n",
    "df.High = list(np.squeeze(cf.transform(np.array(df.High).reshape(-1,1)), axis = 1))\n",
    "df.Open = list(np.squeeze(cf.transform(np.array(df.Open).reshape(-1,1)), axis = 1))\n",
    "df.Low = list(np.squeeze(cf.transform(np.array(df.Low).reshape(-1,1)), axis = 1))\n",
    "df['20_SMA'] = list(np.squeeze(cf.transform(np.array(df['20_SMA']).reshape(-1,1)), axis = 1))\n",
    "df['5_EMA'] = list(np.squeeze(cf.transform(np.array(df['5_EMA']).reshape(-1,1)), axis = 1))\n",
    "\n",
    "#df['20_SMA'] = list(np.squeeze(cf.transform(np.array(df['20_SMA']).reshape(-1,1)), axis = 1))\n",
    "df = df[1:]\n",
    "print(len(df))\n",
    "\n",
    "df['us'] = df.apply(lambda x: upper_shadow(x.Open, x.Close, x.High), axis = 1)\n",
    "df['ls'] = df.apply(lambda x: lower_shadow(x.Open, x.Close, x.Low), axis = 1)\n",
    "df['rb'] = df.apply(lambda x: real_body(x.Open, x.Close), axis = 1)\n",
    "vt,bst,sst,bqt,sqt,tpt,slt = df_vectorize(df)\n",
    "df = df[10:]\n",
    "vt = np.array(vt)\n",
    "\n",
    "bside_lab = buy_le.transform(bst)\n",
    "sside_lab = sell_le.transform(sst)\n",
    "bqside_lab = bq_le.transform(bqt)\n",
    "sqside_lab = sq_le.transform(sqt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.819999999999936\n",
      "31.570000000000164\n",
      "403.9899999999998\n",
      "2130.82\n"
     ]
    }
   ],
   "source": [
    "print(statistics.median(tp))\n",
    "print(statistics.median(sl))\n",
    "print(max(tp))\n",
    "print(max(sl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sq_le_SPX.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from joblib import dump,load\n",
    "\n",
    "v = np.array(v)\n",
    "v = np.nan_to_num(v)\n",
    "# D candle best feature selection based on ANOVA and fregression\n",
    "\n",
    "tp = np.array(tp).reshape(-1,1)\n",
    "sl = np.array(sl).reshape(-1,1)\n",
    "#v_h = np.hstack((v,tp,sl))\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "buy_le = LabelEncoder()\n",
    "buy_lab = buy_le.fit_transform(bs)\n",
    "sell_le = LabelEncoder()\n",
    "sell_lab = sell_le.fit_transform(ss)\n",
    "\n",
    "sq_le = LabelEncoder()\n",
    "sq_lab = sq_le.fit_transform(sq_vec)\n",
    "bq_le = LabelEncoder()\n",
    "bq_lab = bq_le.fit_transform(bq_vec)\n",
    "\n",
    "\n",
    "dump(buy_le,'buy_le_SPX.joblib')\n",
    "dump(sell_le,'sell_le_SPX.joblib')\n",
    "dump(bq_le,'bq_le_SPX.joblib')\n",
    "dump(sq_le,'sq_le_SPX.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 1462, 1: 833})\n",
      "Counter({1: 1464, 0: 831})\n",
      "Counter({1: 1214, 0: 1081})\n",
      "Counter({1: 1494, 0: 801})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(Counter(buy_lab))\n",
    "print(Counter(sell_lab))\n",
    "print(Counter(bq_lab))\n",
    "print(Counter(sq_lab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 833, 1: 833})\n"
     ]
    }
   ],
   "source": [
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.under_sampling import NearMiss\n",
    "ov1 = SMOTE()\n",
    "undersample = NearMiss(version=3, n_neighbors_ver3=3)\n",
    "v_v,buy_lab= undersample.fit_resample(v,buy_lab)\n",
    "print(Counter(buy_lab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 831, 1: 831})\n",
      "Counter({1: 1214, 0: 1081})\n",
      "Counter({0: 801, 1: 801})\n"
     ]
    }
   ],
   "source": [
    "v_s,sell_lab= undersample.fit_resample(v,sell_lab)\n",
    "#v_bq,bq_lab= undersample.fit_resample(v,bq_lab)\n",
    "v_sq,sq_lab= undersample.fit_resample(v,sq_lab)\n",
    "print(Counter(sell_lab))\n",
    "print(Counter(bq_lab))\n",
    "print(Counter(sq_lab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1535, 70)\n",
      "(1535,)\n",
      "(2103, 70)\n",
      "(2103, 1)\n",
      "(2103, 70)\n",
      "(2103, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_trainb, X_testb, y_trainb, y_testb = train_test_split(v_v, buy_lab, test_size=0.1, random_state=42, stratify=buy_lab)\n",
    "X_trains, X_tests, y_trains, y_tests = train_test_split(v_s, sell_lab, test_size=0.1, random_state=42, stratify=sell_lab)\n",
    "X_trainbq, X_testbq, y_trainbq, y_testbq = train_test_split(v, bq_lab, test_size=0.1, random_state=42, stratify=bq_lab)\n",
    "X_trainsq, X_testsq, y_trainsq, y_testsq = train_test_split(v_sq, sq_lab, test_size=0.1, random_state=42, stratify=sq_lab)\n",
    "X_train_tp, X_test_tp, y_train_tp, y_test_tp = train_test_split(v, tp, test_size=0.1, random_state=42)\n",
    "X_train_sl, X_test_sl, y_train_sl, y_test_sl = train_test_split(v, sl, test_size=0.1, random_state=42)\n",
    "print(X_trainb.shape)\n",
    "print(y_trainb.shape)\n",
    "print(X_train_tp.shape)\n",
    "print(y_train_tp.shape)\n",
    "print(X_train_sl.shape)\n",
    "print(y_train_sl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras GRU Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buy_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "18/18 [==============================] - 19s 508ms/step - loss: 1.0104 - accuracy: 0.5150 - val_loss: 0.6830 - val_accuracy: 0.5979\n",
      "Epoch 2/250\n",
      "18/18 [==============================] - 8s 421ms/step - loss: 0.9316 - accuracy: 0.5242 - val_loss: 0.7197 - val_accuracy: 0.5155\n",
      "Epoch 3/250\n",
      "18/18 [==============================] - 8s 449ms/step - loss: 0.8838 - accuracy: 0.5420 - val_loss: 0.7132 - val_accuracy: 0.4330\n",
      "Epoch 4/250\n",
      "18/18 [==============================] - 8s 441ms/step - loss: 0.8533 - accuracy: 0.5564 - val_loss: 0.7039 - val_accuracy: 0.4536\n",
      "Epoch 5/250\n",
      "18/18 [==============================] - 8s 447ms/step - loss: 0.8311 - accuracy: 0.5538 - val_loss: 0.7209 - val_accuracy: 0.4330\n",
      "Epoch 6/250\n",
      "18/18 [==============================] - 8s 447ms/step - loss: 0.7950 - accuracy: 0.5630 - val_loss: 0.6890 - val_accuracy: 0.5155\n",
      "Epoch 7/250\n",
      "18/18 [==============================] - 8s 462ms/step - loss: 0.7744 - accuracy: 0.5669 - val_loss: 0.6578 - val_accuracy: 0.6598\n",
      "Epoch 8/250\n",
      "18/18 [==============================] - 9s 477ms/step - loss: 0.7801 - accuracy: 0.5521 - val_loss: 0.6427 - val_accuracy: 0.7010\n",
      "Epoch 9/250\n",
      "18/18 [==============================] - 8s 466ms/step - loss: 0.7768 - accuracy: 0.5495 - val_loss: 0.6306 - val_accuracy: 0.6907\n",
      "Epoch 10/250\n",
      "18/18 [==============================] - 8s 448ms/step - loss: 0.7346 - accuracy: 0.5830 - val_loss: 0.6231 - val_accuracy: 0.6907\n",
      "Epoch 11/250\n",
      "18/18 [==============================] - 8s 450ms/step - loss: 0.7405 - accuracy: 0.5830 - val_loss: 0.6238 - val_accuracy: 0.6907\n",
      "Epoch 12/250\n",
      "18/18 [==============================] - 8s 453ms/step - loss: 0.7067 - accuracy: 0.5978 - val_loss: 0.6290 - val_accuracy: 0.6907\n",
      "Epoch 13/250\n",
      "18/18 [==============================] - 8s 447ms/step - loss: 0.7185 - accuracy: 0.5826 - val_loss: 0.6414 - val_accuracy: 0.6907\n",
      "Epoch 14/250\n",
      "18/18 [==============================] - 8s 452ms/step - loss: 0.7150 - accuracy: 0.5791 - val_loss: 0.6375 - val_accuracy: 0.6907\n",
      "Epoch 15/250\n",
      "18/18 [==============================] - 8s 451ms/step - loss: 0.7022 - accuracy: 0.5969 - val_loss: 0.6353 - val_accuracy: 0.6907\n",
      "Epoch 16/250\n",
      "18/18 [==============================] - 8s 474ms/step - loss: 0.7018 - accuracy: 0.5856 - val_loss: 0.6347 - val_accuracy: 0.6907\n",
      "Epoch 17/250\n",
      "18/18 [==============================] - 8s 471ms/step - loss: 0.6850 - accuracy: 0.6083 - val_loss: 0.6361 - val_accuracy: 0.6907\n",
      "Epoch 18/250\n",
      "18/18 [==============================] - 8s 470ms/step - loss: 0.6858 - accuracy: 0.6035 - val_loss: 0.6332 - val_accuracy: 0.6907\n",
      "Epoch 19/250\n",
      "18/18 [==============================] - 9s 474ms/step - loss: 0.6879 - accuracy: 0.5978 - val_loss: 0.6289 - val_accuracy: 0.6907\n",
      "Epoch 20/250\n",
      "18/18 [==============================] - 9s 479ms/step - loss: 0.6774 - accuracy: 0.6044 - val_loss: 0.6269 - val_accuracy: 0.6907\n",
      "Epoch 21/250\n",
      "18/18 [==============================] - 9s 478ms/step - loss: 0.6763 - accuracy: 0.6100 - val_loss: 0.6232 - val_accuracy: 0.6907\n",
      "Epoch 22/250\n",
      "18/18 [==============================] - 8s 472ms/step - loss: 0.6787 - accuracy: 0.6057 - val_loss: 0.6236 - val_accuracy: 0.6907\n",
      "Epoch 23/250\n",
      "18/18 [==============================] - 9s 476ms/step - loss: 0.6671 - accuracy: 0.6205 - val_loss: 0.6239 - val_accuracy: 0.6907\n",
      "Epoch 24/250\n",
      "18/18 [==============================] - 9s 483ms/step - loss: 0.6737 - accuracy: 0.6183 - val_loss: 0.6238 - val_accuracy: 0.6907\n",
      "Epoch 25/250\n",
      "18/18 [==============================] - 9s 479ms/step - loss: 0.6639 - accuracy: 0.6266 - val_loss: 0.6229 - val_accuracy: 0.6907\n",
      "Epoch 26/250\n",
      "18/18 [==============================] - 9s 475ms/step - loss: 0.6638 - accuracy: 0.6231 - val_loss: 0.6210 - val_accuracy: 0.6907\n",
      "Epoch 27/250\n",
      "18/18 [==============================] - 9s 478ms/step - loss: 0.6657 - accuracy: 0.6270 - val_loss: 0.6217 - val_accuracy: 0.6907\n",
      "Epoch 28/250\n",
      "18/18 [==============================] - 9s 474ms/step - loss: 0.6645 - accuracy: 0.6174 - val_loss: 0.6228 - val_accuracy: 0.6907\n",
      "Epoch 29/250\n",
      "18/18 [==============================] - 9s 474ms/step - loss: 0.6610 - accuracy: 0.6275 - val_loss: 0.6235 - val_accuracy: 0.6907\n",
      "Epoch 30/250\n",
      "18/18 [==============================] - 9s 474ms/step - loss: 0.6598 - accuracy: 0.6318 - val_loss: 0.6245 - val_accuracy: 0.6907\n",
      "Epoch 31/250\n",
      "18/18 [==============================] - 9s 474ms/step - loss: 0.6596 - accuracy: 0.6270 - val_loss: 0.6239 - val_accuracy: 0.6907\n",
      "Epoch 32/250\n",
      "18/18 [==============================] - 9s 475ms/step - loss: 0.6631 - accuracy: 0.6266 - val_loss: 0.6245 - val_accuracy: 0.6907\n",
      "Epoch 33/250\n",
      "18/18 [==============================] - 9s 475ms/step - loss: 0.6594 - accuracy: 0.6322 - val_loss: 0.6255 - val_accuracy: 0.6907\n",
      "Epoch 34/250\n",
      "18/18 [==============================] - 9s 478ms/step - loss: 0.6595 - accuracy: 0.6375 - val_loss: 0.6259 - val_accuracy: 0.6907\n",
      "Epoch 35/250\n",
      "18/18 [==============================] - 9s 479ms/step - loss: 0.6567 - accuracy: 0.6309 - val_loss: 0.6255 - val_accuracy: 0.6907\n",
      "Epoch 36/250\n",
      "18/18 [==============================] - 9s 485ms/step - loss: 0.6609 - accuracy: 0.6327 - val_loss: 0.6249 - val_accuracy: 0.6907\n",
      "Epoch 37/250\n",
      "18/18 [==============================] - 9s 497ms/step - loss: 0.6574 - accuracy: 0.6309 - val_loss: 0.6233 - val_accuracy: 0.6907\n",
      "Epoch 38/250\n",
      "18/18 [==============================] - 9s 485ms/step - loss: 0.6570 - accuracy: 0.6353 - val_loss: 0.6221 - val_accuracy: 0.6907\n",
      "Epoch 39/250\n",
      "18/18 [==============================] - 9s 480ms/step - loss: 0.6530 - accuracy: 0.6340 - val_loss: 0.6220 - val_accuracy: 0.6907\n",
      "Epoch 40/250\n",
      "18/18 [==============================] - 9s 480ms/step - loss: 0.6528 - accuracy: 0.6383 - val_loss: 0.6227 - val_accuracy: 0.6907\n",
      "Epoch 41/250\n",
      "18/18 [==============================] - 9s 481ms/step - loss: 0.6514 - accuracy: 0.6357 - val_loss: 0.6224 - val_accuracy: 0.6907\n",
      "Epoch 42/250\n",
      "18/18 [==============================] - 9s 478ms/step - loss: 0.6546 - accuracy: 0.6327 - val_loss: 0.6218 - val_accuracy: 0.6907\n",
      "Epoch 43/250\n",
      "18/18 [==============================] - 9s 477ms/step - loss: 0.6568 - accuracy: 0.6353 - val_loss: 0.6208 - val_accuracy: 0.6907\n",
      "Epoch 44/250\n",
      "18/18 [==============================] - 9s 484ms/step - loss: 0.6569 - accuracy: 0.6340 - val_loss: 0.6212 - val_accuracy: 0.6907\n",
      "Epoch 45/250\n",
      "18/18 [==============================] - 9s 500ms/step - loss: 0.6539 - accuracy: 0.6379 - val_loss: 0.6200 - val_accuracy: 0.6907\n",
      "Epoch 46/250\n",
      "18/18 [==============================] - 9s 490ms/step - loss: 0.6505 - accuracy: 0.6344 - val_loss: 0.6203 - val_accuracy: 0.6907\n",
      "Epoch 47/250\n",
      "18/18 [==============================] - 9s 487ms/step - loss: 0.6547 - accuracy: 0.6379 - val_loss: 0.6214 - val_accuracy: 0.6907\n",
      "Epoch 48/250\n",
      "18/18 [==============================] - 9s 488ms/step - loss: 0.6507 - accuracy: 0.6375 - val_loss: 0.6189 - val_accuracy: 0.6907\n",
      "Epoch 49/250\n",
      "18/18 [==============================] - 9s 485ms/step - loss: 0.6506 - accuracy: 0.6362 - val_loss: 0.6173 - val_accuracy: 0.6907\n",
      "Epoch 50/250\n",
      "18/18 [==============================] - 9s 490ms/step - loss: 0.6522 - accuracy: 0.6340 - val_loss: 0.6172 - val_accuracy: 0.6907\n",
      "Epoch 51/250\n",
      "18/18 [==============================] - 9s 489ms/step - loss: 0.6485 - accuracy: 0.6392 - val_loss: 0.6152 - val_accuracy: 0.6907\n",
      "Epoch 52/250\n",
      "18/18 [==============================] - 9s 487ms/step - loss: 0.6494 - accuracy: 0.6379 - val_loss: 0.6198 - val_accuracy: 0.6907\n",
      "Epoch 53/250\n",
      "18/18 [==============================] - 9s 485ms/step - loss: 0.6514 - accuracy: 0.6370 - val_loss: 0.6220 - val_accuracy: 0.6907\n",
      "Epoch 54/250\n",
      "18/18 [==============================] - 9s 486ms/step - loss: 0.6487 - accuracy: 0.6370 - val_loss: 0.6223 - val_accuracy: 0.6907\n",
      "Epoch 55/250\n",
      "18/18 [==============================] - 9s 481ms/step - loss: 0.6468 - accuracy: 0.6401 - val_loss: 0.6281 - val_accuracy: 0.6907\n",
      "Epoch 56/250\n",
      "18/18 [==============================] - 9s 481ms/step - loss: 0.6482 - accuracy: 0.6379 - val_loss: 0.6274 - val_accuracy: 0.6907\n",
      "Epoch 57/250\n",
      "18/18 [==============================] - 9s 480ms/step - loss: 0.6500 - accuracy: 0.6349 - val_loss: 0.6257 - val_accuracy: 0.6907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/250\n",
      "18/18 [==============================] - 9s 485ms/step - loss: 0.6464 - accuracy: 0.6366 - val_loss: 0.6226 - val_accuracy: 0.6907\n",
      "Epoch 59/250\n",
      "18/18 [==============================] - 9s 483ms/step - loss: 0.6464 - accuracy: 0.6401 - val_loss: 0.6375 - val_accuracy: 0.6907\n",
      "Epoch 60/250\n",
      "18/18 [==============================] - 9s 481ms/step - loss: 0.6459 - accuracy: 0.6397 - val_loss: 0.6349 - val_accuracy: 0.6907\n",
      "Epoch 61/250\n",
      "18/18 [==============================] - 9s 484ms/step - loss: 0.6478 - accuracy: 0.6331 - val_loss: 0.6429 - val_accuracy: 0.6804\n",
      "Epoch 62/250\n",
      "18/18 [==============================] - 9s 482ms/step - loss: 0.6394 - accuracy: 0.6357 - val_loss: 0.6456 - val_accuracy: 0.6907\n",
      "Epoch 63/250\n",
      "18/18 [==============================] - 9s 482ms/step - loss: 0.6429 - accuracy: 0.6383 - val_loss: 0.6450 - val_accuracy: 0.6907\n",
      "Epoch 64/250\n",
      "18/18 [==============================] - 9s 480ms/step - loss: 0.6497 - accuracy: 0.6340 - val_loss: 0.6197 - val_accuracy: 0.6701\n",
      "Epoch 65/250\n",
      "18/18 [==============================] - 9s 481ms/step - loss: 0.6368 - accuracy: 0.6453 - val_loss: 0.6114 - val_accuracy: 0.6907\n",
      "Epoch 66/250\n",
      "18/18 [==============================] - 9s 483ms/step - loss: 0.6410 - accuracy: 0.6375 - val_loss: 0.6200 - val_accuracy: 0.6804\n",
      "Epoch 67/250\n",
      "18/18 [==============================] - 9s 481ms/step - loss: 0.6388 - accuracy: 0.6383 - val_loss: 0.6892 - val_accuracy: 0.4948\n",
      "Epoch 68/250\n",
      "18/18 [==============================] - 9s 485ms/step - loss: 0.6329 - accuracy: 0.6379 - val_loss: 0.6901 - val_accuracy: 0.4845\n",
      "Epoch 69/250\n",
      "18/18 [==============================] - 9s 478ms/step - loss: 0.6336 - accuracy: 0.6423 - val_loss: 0.6946 - val_accuracy: 0.4021\n",
      "Epoch 70/250\n",
      "18/18 [==============================] - 9s 481ms/step - loss: 0.6315 - accuracy: 0.6444 - val_loss: 0.6666 - val_accuracy: 0.5670\n",
      "Epoch 71/250\n",
      "18/18 [==============================] - 9s 483ms/step - loss: 0.6342 - accuracy: 0.6431 - val_loss: 0.6557 - val_accuracy: 0.5979\n",
      "Epoch 72/250\n",
      "18/18 [==============================] - 9s 485ms/step - loss: 0.6333 - accuracy: 0.6353 - val_loss: 0.6490 - val_accuracy: 0.6804\n",
      "Epoch 73/250\n",
      "18/18 [==============================] - 9s 481ms/step - loss: 0.6303 - accuracy: 0.6427 - val_loss: 0.6242 - val_accuracy: 0.6598\n",
      "Epoch 74/250\n",
      "18/18 [==============================] - 9s 484ms/step - loss: 0.6286 - accuracy: 0.6505 - val_loss: 0.6672 - val_accuracy: 0.5258\n",
      "Epoch 75/250\n",
      "18/18 [==============================] - 9s 475ms/step - loss: 0.6246 - accuracy: 0.6484 - val_loss: 0.6795 - val_accuracy: 0.5773\n",
      "Epoch 76/250\n",
      "18/18 [==============================] - 9s 477ms/step - loss: 0.6313 - accuracy: 0.6370 - val_loss: 0.7319 - val_accuracy: 0.3402\n",
      "Epoch 77/250\n",
      "18/18 [==============================] - 9s 476ms/step - loss: 0.6289 - accuracy: 0.6331 - val_loss: 0.6902 - val_accuracy: 0.4639\n",
      "Epoch 78/250\n",
      "18/18 [==============================] - 8s 473ms/step - loss: 0.6220 - accuracy: 0.6440 - val_loss: 0.7056 - val_accuracy: 0.4742\n",
      "Epoch 79/250\n",
      "18/18 [==============================] - 8s 473ms/step - loss: 0.6293 - accuracy: 0.6392 - val_loss: 0.7250 - val_accuracy: 0.4227\n",
      "Epoch 80/250\n",
      "18/18 [==============================] - 9s 480ms/step - loss: 0.6180 - accuracy: 0.6488 - val_loss: 0.6986 - val_accuracy: 0.4845\n",
      "Epoch 81/250\n",
      "18/18 [==============================] - 9s 486ms/step - loss: 0.6151 - accuracy: 0.6458 - val_loss: 0.7200 - val_accuracy: 0.4433\n",
      "Epoch 82/250\n",
      "18/18 [==============================] - 9s 481ms/step - loss: 0.6146 - accuracy: 0.6549 - val_loss: 0.7434 - val_accuracy: 0.3608\n",
      "Epoch 83/250\n",
      "18/18 [==============================] - 9s 483ms/step - loss: 0.6125 - accuracy: 0.6545 - val_loss: 0.7418 - val_accuracy: 0.3814\n",
      "Epoch 84/250\n",
      "18/18 [==============================] - 9s 481ms/step - loss: 0.6105 - accuracy: 0.6549 - val_loss: 0.6510 - val_accuracy: 0.6289\n",
      "Epoch 85/250\n",
      "18/18 [==============================] - 9s 516ms/step - loss: 0.6223 - accuracy: 0.6440 - val_loss: 0.7642 - val_accuracy: 0.3402\n",
      "Epoch 86/250\n",
      "18/18 [==============================] - 9s 493ms/step - loss: 0.6095 - accuracy: 0.6540 - val_loss: 0.6925 - val_accuracy: 0.5052\n",
      "Epoch 87/250\n",
      "18/18 [==============================] - 9s 480ms/step - loss: 0.6074 - accuracy: 0.6606 - val_loss: 0.7227 - val_accuracy: 0.4227\n",
      "Epoch 88/250\n",
      "18/18 [==============================] - 9s 475ms/step - loss: 0.6116 - accuracy: 0.6571 - val_loss: 0.7639 - val_accuracy: 0.3299\n",
      "Epoch 89/250\n",
      "18/18 [==============================] - 8s 471ms/step - loss: 0.6074 - accuracy: 0.6527 - val_loss: 0.6452 - val_accuracy: 0.5773\n",
      "Epoch 90/250\n",
      "18/18 [==============================] - 8s 472ms/step - loss: 0.6059 - accuracy: 0.6767 - val_loss: 0.7677 - val_accuracy: 0.3299\n",
      "Epoch 91/250\n",
      "18/18 [==============================] - 8s 472ms/step - loss: 0.6124 - accuracy: 0.6475 - val_loss: 0.7164 - val_accuracy: 0.4330\n",
      "Epoch 92/250\n",
      "18/18 [==============================] - 8s 472ms/step - loss: 0.6032 - accuracy: 0.6606 - val_loss: 0.6821 - val_accuracy: 0.4639\n",
      "Epoch 93/250\n",
      "18/18 [==============================] - 9s 505ms/step - loss: 0.5971 - accuracy: 0.6715 - val_loss: 0.6833 - val_accuracy: 0.6082\n",
      "Epoch 94/250\n",
      "18/18 [==============================] - 9s 492ms/step - loss: 0.5986 - accuracy: 0.6601 - val_loss: 0.6597 - val_accuracy: 0.6082\n",
      "Epoch 95/250\n",
      "18/18 [==============================] - 9s 479ms/step - loss: 0.5893 - accuracy: 0.6610 - val_loss: 0.6689 - val_accuracy: 0.5567\n",
      "Epoch 96/250\n",
      "18/18 [==============================] - 8s 472ms/step - loss: 0.5866 - accuracy: 0.6728 - val_loss: 0.6896 - val_accuracy: 0.6598\n",
      "Epoch 97/250\n",
      "18/18 [==============================] - 8s 470ms/step - loss: 0.5876 - accuracy: 0.6719 - val_loss: 0.7337 - val_accuracy: 0.4227\n",
      "Epoch 98/250\n",
      "18/18 [==============================] - 8s 471ms/step - loss: 0.5870 - accuracy: 0.6767 - val_loss: 0.6824 - val_accuracy: 0.5773\n",
      "Epoch 99/250\n",
      "18/18 [==============================] - 8s 469ms/step - loss: 0.5858 - accuracy: 0.6767 - val_loss: 0.6950 - val_accuracy: 0.5155\n",
      "Epoch 100/250\n",
      "18/18 [==============================] - 8s 468ms/step - loss: 0.5861 - accuracy: 0.6784 - val_loss: 0.6539 - val_accuracy: 0.5567\n",
      "Epoch 101/250\n",
      "18/18 [==============================] - 8s 469ms/step - loss: 0.5885 - accuracy: 0.6780 - val_loss: 0.6668 - val_accuracy: 0.5155\n",
      "Epoch 102/250\n",
      "18/18 [==============================] - 8s 468ms/step - loss: 0.5769 - accuracy: 0.6797 - val_loss: 0.6984 - val_accuracy: 0.6289\n",
      "Epoch 103/250\n",
      "18/18 [==============================] - 8s 469ms/step - loss: 0.5804 - accuracy: 0.6876 - val_loss: 0.7268 - val_accuracy: 0.4948\n",
      "Epoch 104/250\n",
      "18/18 [==============================] - 8s 474ms/step - loss: 0.5838 - accuracy: 0.6906 - val_loss: 0.7196 - val_accuracy: 0.5052\n",
      "Epoch 105/250\n",
      "18/18 [==============================] - 8s 470ms/step - loss: 0.5700 - accuracy: 0.6989 - val_loss: 0.7127 - val_accuracy: 0.5361\n",
      "Epoch 106/250\n",
      "18/18 [==============================] - 9s 490ms/step - loss: 0.5547 - accuracy: 0.7142 - val_loss: 0.7448 - val_accuracy: 0.5361\n",
      "Epoch 107/250\n",
      "18/18 [==============================] - 9s 487ms/step - loss: 0.5530 - accuracy: 0.7137 - val_loss: 0.7952 - val_accuracy: 0.5052\n",
      "Epoch 108/250\n",
      "18/18 [==============================] - 9s 485ms/step - loss: 0.5772 - accuracy: 0.6906 - val_loss: 0.7254 - val_accuracy: 0.5258\n",
      "Epoch 109/250\n",
      "18/18 [==============================] - 9s 489ms/step - loss: 0.5587 - accuracy: 0.6924 - val_loss: 0.7026 - val_accuracy: 0.5670\n",
      "Epoch 110/250\n",
      "18/18 [==============================] - 9s 490ms/step - loss: 0.5580 - accuracy: 0.7085 - val_loss: 0.7286 - val_accuracy: 0.5052\n",
      "Epoch 111/250\n",
      "18/18 [==============================] - 9s 480ms/step - loss: 0.5701 - accuracy: 0.7072 - val_loss: 0.7478 - val_accuracy: 0.4948\n",
      "Epoch 112/250\n",
      "18/18 [==============================] - 9s 482ms/step - loss: 0.5586 - accuracy: 0.7081 - val_loss: 0.7529 - val_accuracy: 0.5567\n",
      "Epoch 113/250\n",
      "18/18 [==============================] - 9s 481ms/step - loss: 0.5663 - accuracy: 0.7002 - val_loss: 0.8803 - val_accuracy: 0.3814\n",
      "Epoch 114/250\n",
      "18/18 [==============================] - 9s 478ms/step - loss: 0.5516 - accuracy: 0.7207 - val_loss: 0.7354 - val_accuracy: 0.5052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/250\n",
      "18/18 [==============================] - 9s 482ms/step - loss: 0.5440 - accuracy: 0.7111 - val_loss: 0.7463 - val_accuracy: 0.5052\n",
      "Epoch 116/250\n",
      "18/18 [==============================] - 9s 486ms/step - loss: 0.5388 - accuracy: 0.7316 - val_loss: 0.7224 - val_accuracy: 0.6186\n",
      "Epoch 117/250\n",
      "18/18 [==============================] - 9s 490ms/step - loss: 0.5526 - accuracy: 0.7172 - val_loss: 0.8975 - val_accuracy: 0.3608\n",
      "Epoch 118/250\n",
      "18/18 [==============================] - 9s 489ms/step - loss: 0.5468 - accuracy: 0.7120 - val_loss: 0.6559 - val_accuracy: 0.5670\n",
      "Epoch 119/250\n",
      "18/18 [==============================] - 9s 498ms/step - loss: 0.5424 - accuracy: 0.7285 - val_loss: 0.7173 - val_accuracy: 0.6701\n",
      "Epoch 120/250\n",
      "18/18 [==============================] - 9s 475ms/step - loss: 0.5350 - accuracy: 0.7255 - val_loss: 0.6951 - val_accuracy: 0.5464\n",
      "Epoch 121/250\n",
      "18/18 [==============================] - 8s 468ms/step - loss: 0.5242 - accuracy: 0.7394 - val_loss: 0.7852 - val_accuracy: 0.5052\n",
      "Epoch 122/250\n",
      "18/18 [==============================] - 8s 470ms/step - loss: 0.5410 - accuracy: 0.7185 - val_loss: 0.7068 - val_accuracy: 0.5464\n",
      "Epoch 123/250\n",
      "18/18 [==============================] - 8s 470ms/step - loss: 0.5327 - accuracy: 0.7264 - val_loss: 0.6862 - val_accuracy: 0.5979\n",
      "Epoch 124/250\n",
      "18/18 [==============================] - 8s 469ms/step - loss: 0.5226 - accuracy: 0.7312 - val_loss: 0.6770 - val_accuracy: 0.5773\n",
      "Epoch 125/250\n",
      "18/18 [==============================] - 8s 469ms/step - loss: 0.5063 - accuracy: 0.7420 - val_loss: 0.7566 - val_accuracy: 0.6495\n",
      "Epoch 126/250\n",
      "18/18 [==============================] - 9s 500ms/step - loss: 0.5187 - accuracy: 0.7394 - val_loss: 0.9492 - val_accuracy: 0.4227\n",
      "Epoch 127/250\n",
      "18/18 [==============================] - 9s 490ms/step - loss: 0.5307 - accuracy: 0.7386 - val_loss: 0.7855 - val_accuracy: 0.4845\n",
      "Epoch 128/250\n",
      "18/18 [==============================] - 9s 486ms/step - loss: 0.5167 - accuracy: 0.7438 - val_loss: 0.6985 - val_accuracy: 0.5670\n",
      "Epoch 129/250\n",
      "18/18 [==============================] - 9s 473ms/step - loss: 0.4955 - accuracy: 0.7542 - val_loss: 0.7613 - val_accuracy: 0.5052\n",
      "Epoch 130/250\n",
      "18/18 [==============================] - 9s 475ms/step - loss: 0.5317 - accuracy: 0.7329 - val_loss: 0.7378 - val_accuracy: 0.5876\n",
      "Epoch 131/250\n",
      "18/18 [==============================] - 9s 475ms/step - loss: 0.4900 - accuracy: 0.7564 - val_loss: 0.7566 - val_accuracy: 0.7010\n",
      "Epoch 132/250\n",
      "18/18 [==============================] - 9s 475ms/step - loss: 0.5128 - accuracy: 0.7442 - val_loss: 0.6842 - val_accuracy: 0.6186\n",
      "Epoch 133/250\n",
      "18/18 [==============================] - 9s 476ms/step - loss: 0.5026 - accuracy: 0.7564 - val_loss: 0.7164 - val_accuracy: 0.6598\n",
      "Epoch 134/250\n",
      "18/18 [==============================] - 8s 469ms/step - loss: 0.5073 - accuracy: 0.7542 - val_loss: 0.7058 - val_accuracy: 0.5979\n",
      "Epoch 135/250\n",
      "18/18 [==============================] - 8s 468ms/step - loss: 0.4926 - accuracy: 0.7612 - val_loss: 1.0200 - val_accuracy: 0.4227\n",
      "Epoch 136/250\n",
      "18/18 [==============================] - 8s 468ms/step - loss: 0.4965 - accuracy: 0.7647 - val_loss: 0.7896 - val_accuracy: 0.5155\n",
      "Epoch 137/250\n",
      "18/18 [==============================] - 8s 470ms/step - loss: 0.5017 - accuracy: 0.7499 - val_loss: 0.6992 - val_accuracy: 0.5567\n",
      "Epoch 138/250\n",
      "18/18 [==============================] - 8s 473ms/step - loss: 0.4893 - accuracy: 0.7560 - val_loss: 0.7472 - val_accuracy: 0.6392\n",
      "Epoch 139/250\n",
      "18/18 [==============================] - 9s 494ms/step - loss: 0.4749 - accuracy: 0.7712 - val_loss: 0.7348 - val_accuracy: 0.5773\n",
      "Epoch 140/250\n",
      "18/18 [==============================] - 9s 480ms/step - loss: 0.4925 - accuracy: 0.7547 - val_loss: 0.6796 - val_accuracy: 0.5773\n",
      "Epoch 141/250\n",
      "18/18 [==============================] - 8s 470ms/step - loss: 0.4955 - accuracy: 0.7512 - val_loss: 0.6933 - val_accuracy: 0.6701\n",
      "Epoch 142/250\n",
      "18/18 [==============================] - 9s 481ms/step - loss: 0.5013 - accuracy: 0.7521 - val_loss: 0.7416 - val_accuracy: 0.5052\n",
      "Epoch 143/250\n",
      "18/18 [==============================] - 9s 485ms/step - loss: 0.4813 - accuracy: 0.7603 - val_loss: 0.6525 - val_accuracy: 0.6289\n",
      "Epoch 144/250\n",
      "18/18 [==============================] - 8s 471ms/step - loss: 0.4849 - accuracy: 0.7678 - val_loss: 0.8238 - val_accuracy: 0.4948\n",
      "Epoch 145/250\n",
      "18/18 [==============================] - 8s 470ms/step - loss: 0.4673 - accuracy: 0.7717 - val_loss: 0.8756 - val_accuracy: 0.5155\n",
      "Epoch 146/250\n",
      "18/18 [==============================] - 9s 504ms/step - loss: 0.4705 - accuracy: 0.7717 - val_loss: 0.8087 - val_accuracy: 0.5464\n",
      "Epoch 147/250\n",
      "18/18 [==============================] - 8s 468ms/step - loss: 0.4775 - accuracy: 0.7765 - val_loss: 0.8755 - val_accuracy: 0.5361\n",
      "Epoch 148/250\n",
      "18/18 [==============================] - 9s 481ms/step - loss: 0.4752 - accuracy: 0.7599 - val_loss: 0.7346 - val_accuracy: 0.6082\n",
      "Epoch 149/250\n",
      "18/18 [==============================] - 9s 480ms/step - loss: 0.4768 - accuracy: 0.7791 - val_loss: 0.7482 - val_accuracy: 0.5773\n",
      "Epoch 150/250\n",
      "18/18 [==============================] - 9s 475ms/step - loss: 0.4615 - accuracy: 0.7865 - val_loss: 0.8024 - val_accuracy: 0.5361\n",
      "Epoch 151/250\n",
      "18/18 [==============================] - 9s 480ms/step - loss: 0.4573 - accuracy: 0.7756 - val_loss: 0.9762 - val_accuracy: 0.4845\n",
      "Epoch 152/250\n",
      "18/18 [==============================] - 9s 485ms/step - loss: 0.4717 - accuracy: 0.7730 - val_loss: 0.9688 - val_accuracy: 0.5052\n",
      "Epoch 153/250\n",
      "18/18 [==============================] - 9s 497ms/step - loss: 0.4587 - accuracy: 0.7847 - val_loss: 0.6945 - val_accuracy: 0.6701\n",
      "Epoch 154/250\n",
      "18/18 [==============================] - 9s 474ms/step - loss: 0.4575 - accuracy: 0.7956 - val_loss: 0.9553 - val_accuracy: 0.5052\n",
      "Epoch 155/250\n",
      "18/18 [==============================] - 9s 474ms/step - loss: 0.4429 - accuracy: 0.7865 - val_loss: 0.8731 - val_accuracy: 0.5052\n",
      "Epoch 156/250\n",
      "18/18 [==============================] - 9s 477ms/step - loss: 0.4766 - accuracy: 0.7721 - val_loss: 0.7341 - val_accuracy: 0.5567\n",
      "Epoch 157/250\n",
      "18/18 [==============================] - 9s 494ms/step - loss: 0.4558 - accuracy: 0.7821 - val_loss: 0.8744 - val_accuracy: 0.5258\n",
      "Epoch 158/250\n",
      "18/18 [==============================] - 8s 470ms/step - loss: 0.4477 - accuracy: 0.7826 - val_loss: 0.9725 - val_accuracy: 0.4742\n",
      "Epoch 159/250\n",
      "18/18 [==============================] - 8s 465ms/step - loss: 0.4431 - accuracy: 0.7930 - val_loss: 0.8421 - val_accuracy: 0.5567\n",
      "Epoch 160/250\n",
      "18/18 [==============================] - 8s 470ms/step - loss: 0.4470 - accuracy: 0.7900 - val_loss: 0.9533 - val_accuracy: 0.4948\n",
      "Epoch 161/250\n",
      "18/18 [==============================] - 8s 469ms/step - loss: 0.4309 - accuracy: 0.7983 - val_loss: 0.7997 - val_accuracy: 0.5979\n",
      "Epoch 162/250\n",
      "18/18 [==============================] - 8s 468ms/step - loss: 0.4281 - accuracy: 0.7978 - val_loss: 0.9413 - val_accuracy: 0.5155\n",
      "Epoch 163/250\n",
      "18/18 [==============================] - 8s 466ms/step - loss: 0.4368 - accuracy: 0.7904 - val_loss: 0.9272 - val_accuracy: 0.5979\n",
      "Epoch 164/250\n",
      "18/18 [==============================] - 8s 464ms/step - loss: 0.4407 - accuracy: 0.7926 - val_loss: 0.8350 - val_accuracy: 0.5670\n",
      "Epoch 165/250\n",
      "18/18 [==============================] - 8s 467ms/step - loss: 0.4270 - accuracy: 0.7996 - val_loss: 0.9518 - val_accuracy: 0.5155\n",
      "Epoch 166/250\n",
      "18/18 [==============================] - 8s 469ms/step - loss: 0.4403 - accuracy: 0.7813 - val_loss: 0.7604 - val_accuracy: 0.5773\n",
      "Epoch 167/250\n",
      "18/18 [==============================] - 8s 467ms/step - loss: 0.4134 - accuracy: 0.8122 - val_loss: 0.8397 - val_accuracy: 0.5464\n",
      "Epoch 168/250\n",
      "18/18 [==============================] - 8s 465ms/step - loss: 0.4175 - accuracy: 0.8118 - val_loss: 0.8426 - val_accuracy: 0.5979\n",
      "Epoch 169/250\n",
      "18/18 [==============================] - 8s 467ms/step - loss: 0.4152 - accuracy: 0.8026 - val_loss: 1.1638 - val_accuracy: 0.5361\n",
      "Epoch 170/250\n",
      "18/18 [==============================] - 8s 465ms/step - loss: 0.4261 - accuracy: 0.8078 - val_loss: 0.9188 - val_accuracy: 0.5258\n",
      "Epoch 171/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 8s 468ms/step - loss: 0.4195 - accuracy: 0.8096 - val_loss: 0.8566 - val_accuracy: 0.5876\n",
      "Epoch 172/250\n",
      "18/18 [==============================] - 8s 467ms/step - loss: 0.4220 - accuracy: 0.8083 - val_loss: 0.7385 - val_accuracy: 0.5876\n",
      "Epoch 173/250\n",
      "18/18 [==============================] - 8s 470ms/step - loss: 0.4146 - accuracy: 0.8131 - val_loss: 0.7263 - val_accuracy: 0.5979\n",
      "Epoch 174/250\n",
      "18/18 [==============================] - 8s 469ms/step - loss: 0.4141 - accuracy: 0.8118 - val_loss: 0.8487 - val_accuracy: 0.5361\n",
      "Epoch 175/250\n",
      "18/18 [==============================] - 8s 469ms/step - loss: 0.4237 - accuracy: 0.8096 - val_loss: 0.8660 - val_accuracy: 0.5773\n",
      "Epoch 176/250\n",
      "18/18 [==============================] - 8s 469ms/step - loss: 0.4050 - accuracy: 0.8100 - val_loss: 0.8667 - val_accuracy: 0.5258\n",
      "Epoch 177/250\n",
      "18/18 [==============================] - 8s 464ms/step - loss: 0.3950 - accuracy: 0.8283 - val_loss: 0.7356 - val_accuracy: 0.5979\n",
      "Epoch 178/250\n",
      "18/18 [==============================] - 8s 468ms/step - loss: 0.4099 - accuracy: 0.8100 - val_loss: 1.0939 - val_accuracy: 0.5258\n",
      "Epoch 179/250\n",
      "18/18 [==============================] - 8s 468ms/step - loss: 0.4062 - accuracy: 0.8135 - val_loss: 1.1504 - val_accuracy: 0.4742\n",
      "Epoch 180/250\n",
      "18/18 [==============================] - 8s 467ms/step - loss: 0.4114 - accuracy: 0.8113 - val_loss: 0.9613 - val_accuracy: 0.5052\n",
      "Epoch 181/250\n",
      "18/18 [==============================] - 8s 466ms/step - loss: 0.4004 - accuracy: 0.8218 - val_loss: 0.8155 - val_accuracy: 0.6392\n",
      "Epoch 182/250\n",
      "18/18 [==============================] - 8s 466ms/step - loss: 0.4065 - accuracy: 0.8166 - val_loss: 0.8451 - val_accuracy: 0.5979\n",
      "Epoch 183/250\n",
      "18/18 [==============================] - 8s 469ms/step - loss: 0.3995 - accuracy: 0.8283 - val_loss: 1.1099 - val_accuracy: 0.5567\n",
      "Epoch 184/250\n",
      "18/18 [==============================] - 8s 465ms/step - loss: 0.4025 - accuracy: 0.8131 - val_loss: 1.1816 - val_accuracy: 0.5464\n",
      "Epoch 185/250\n",
      "18/18 [==============================] - 8s 461ms/step - loss: 0.3841 - accuracy: 0.8318 - val_loss: 0.8967 - val_accuracy: 0.5876\n",
      "Epoch 186/250\n",
      "18/18 [==============================] - 8s 467ms/step - loss: 0.3961 - accuracy: 0.8231 - val_loss: 0.7589 - val_accuracy: 0.6082\n",
      "Epoch 187/250\n",
      "18/18 [==============================] - 9s 481ms/step - loss: 0.3798 - accuracy: 0.8405 - val_loss: 0.9142 - val_accuracy: 0.5876\n",
      "Epoch 188/250\n",
      "18/18 [==============================] - 8s 463ms/step - loss: 0.3906 - accuracy: 0.8275 - val_loss: 0.8813 - val_accuracy: 0.5979\n",
      "Epoch 189/250\n",
      "18/18 [==============================] - 8s 467ms/step - loss: 0.4005 - accuracy: 0.8227 - val_loss: 0.8539 - val_accuracy: 0.5876\n",
      "Epoch 190/250\n",
      "18/18 [==============================] - 8s 466ms/step - loss: 0.3710 - accuracy: 0.8322 - val_loss: 1.0150 - val_accuracy: 0.5361\n",
      "Epoch 191/250\n",
      "18/18 [==============================] - 8s 466ms/step - loss: 0.3850 - accuracy: 0.8261 - val_loss: 0.7897 - val_accuracy: 0.6186\n",
      "Epoch 192/250\n",
      "18/18 [==============================] - 8s 471ms/step - loss: 0.3733 - accuracy: 0.8314 - val_loss: 0.8089 - val_accuracy: 0.6186\n",
      "Epoch 193/250\n",
      "18/18 [==============================] - 8s 465ms/step - loss: 0.3798 - accuracy: 0.8383 - val_loss: 0.9240 - val_accuracy: 0.5773\n",
      "Epoch 194/250\n",
      "18/18 [==============================] - 8s 470ms/step - loss: 0.3619 - accuracy: 0.8318 - val_loss: 0.7908 - val_accuracy: 0.6186\n",
      "Epoch 195/250\n",
      "18/18 [==============================] - 8s 469ms/step - loss: 0.3695 - accuracy: 0.8314 - val_loss: 0.9421 - val_accuracy: 0.5876\n",
      "Epoch 196/250\n",
      "18/18 [==============================] - 8s 465ms/step - loss: 0.3741 - accuracy: 0.8362 - val_loss: 0.7591 - val_accuracy: 0.6186\n",
      "Epoch 197/250\n",
      "18/18 [==============================] - 8s 468ms/step - loss: 0.3860 - accuracy: 0.8187 - val_loss: 0.8601 - val_accuracy: 0.5567\n",
      "Epoch 198/250\n",
      "18/18 [==============================] - 8s 464ms/step - loss: 0.3926 - accuracy: 0.8222 - val_loss: 0.8244 - val_accuracy: 0.5876\n",
      "Epoch 199/250\n",
      "18/18 [==============================] - 8s 466ms/step - loss: 0.3607 - accuracy: 0.8379 - val_loss: 0.7998 - val_accuracy: 0.6082\n",
      "Epoch 200/250\n",
      "18/18 [==============================] - 8s 466ms/step - loss: 0.3605 - accuracy: 0.8314 - val_loss: 0.8316 - val_accuracy: 0.5773\n",
      "Epoch 201/250\n",
      "18/18 [==============================] - 8s 468ms/step - loss: 0.3677 - accuracy: 0.8423 - val_loss: 0.8959 - val_accuracy: 0.5876\n",
      "Epoch 202/250\n",
      "18/18 [==============================] - 8s 466ms/step - loss: 0.3573 - accuracy: 0.8427 - val_loss: 1.0248 - val_accuracy: 0.5052\n",
      "Epoch 203/250\n",
      "18/18 [==============================] - 8s 471ms/step - loss: 0.3531 - accuracy: 0.8484 - val_loss: 1.0118 - val_accuracy: 0.5670\n",
      "Epoch 204/250\n",
      "18/18 [==============================] - 8s 465ms/step - loss: 0.3688 - accuracy: 0.8427 - val_loss: 1.3540 - val_accuracy: 0.4845\n",
      "Epoch 205/250\n",
      "18/18 [==============================] - 8s 467ms/step - loss: 0.3676 - accuracy: 0.8383 - val_loss: 0.8531 - val_accuracy: 0.6082\n",
      "Epoch 206/250\n",
      "18/18 [==============================] - 8s 465ms/step - loss: 0.3572 - accuracy: 0.8488 - val_loss: 0.9026 - val_accuracy: 0.5567\n",
      "Epoch 207/250\n",
      "18/18 [==============================] - 8s 465ms/step - loss: 0.3510 - accuracy: 0.8549 - val_loss: 0.9523 - val_accuracy: 0.5773\n",
      "Epoch 208/250\n",
      "18/18 [==============================] - 8s 464ms/step - loss: 0.3787 - accuracy: 0.8261 - val_loss: 1.2207 - val_accuracy: 0.5052\n",
      "Epoch 209/250\n",
      "18/18 [==============================] - 8s 467ms/step - loss: 0.3528 - accuracy: 0.8566 - val_loss: 0.9471 - val_accuracy: 0.4845\n",
      "Epoch 210/250\n",
      "18/18 [==============================] - 8s 465ms/step - loss: 0.3564 - accuracy: 0.8484 - val_loss: 0.9074 - val_accuracy: 0.5670\n",
      "Epoch 211/250\n",
      "18/18 [==============================] - 8s 463ms/step - loss: 0.3336 - accuracy: 0.8588 - val_loss: 1.1257 - val_accuracy: 0.5361\n",
      "Epoch 212/250\n",
      "18/18 [==============================] - 8s 465ms/step - loss: 0.3378 - accuracy: 0.8606 - val_loss: 0.9841 - val_accuracy: 0.6186\n",
      "Epoch 213/250\n",
      "18/18 [==============================] - 8s 467ms/step - loss: 0.3363 - accuracy: 0.8562 - val_loss: 1.0746 - val_accuracy: 0.5773\n",
      "Epoch 214/250\n",
      "18/18 [==============================] - 8s 464ms/step - loss: 0.3635 - accuracy: 0.8453 - val_loss: 1.0876 - val_accuracy: 0.5052\n",
      "Epoch 215/250\n",
      "18/18 [==============================] - 8s 461ms/step - loss: 0.3627 - accuracy: 0.8388 - val_loss: 1.0436 - val_accuracy: 0.5155\n",
      "Epoch 216/250\n",
      "18/18 [==============================] - 8s 466ms/step - loss: 0.3708 - accuracy: 0.8383 - val_loss: 1.0370 - val_accuracy: 0.5567\n",
      "Epoch 217/250\n",
      "18/18 [==============================] - 8s 470ms/step - loss: 0.3396 - accuracy: 0.8523 - val_loss: 1.1764 - val_accuracy: 0.4845\n",
      "Epoch 218/250\n",
      "18/18 [==============================] - 8s 464ms/step - loss: 0.3495 - accuracy: 0.8427 - val_loss: 0.9014 - val_accuracy: 0.6289\n",
      "Epoch 219/250\n",
      "18/18 [==============================] - 8s 464ms/step - loss: 0.3493 - accuracy: 0.8479 - val_loss: 1.0436 - val_accuracy: 0.5361\n",
      "Epoch 220/250\n",
      "18/18 [==============================] - 8s 461ms/step - loss: 0.3343 - accuracy: 0.8575 - val_loss: 0.8675 - val_accuracy: 0.6082\n",
      "Epoch 221/250\n",
      "18/18 [==============================] - 8s 463ms/step - loss: 0.3170 - accuracy: 0.8636 - val_loss: 0.8240 - val_accuracy: 0.5979\n",
      "Epoch 222/250\n",
      "18/18 [==============================] - 8s 464ms/step - loss: 0.3412 - accuracy: 0.8527 - val_loss: 0.8751 - val_accuracy: 0.5876\n",
      "Epoch 223/250\n",
      "18/18 [==============================] - 8s 463ms/step - loss: 0.3470 - accuracy: 0.8436 - val_loss: 0.8578 - val_accuracy: 0.5876\n",
      "Epoch 224/250\n",
      "18/18 [==============================] - 8s 462ms/step - loss: 0.3417 - accuracy: 0.8501 - val_loss: 0.8786 - val_accuracy: 0.5670\n",
      "Epoch 225/250\n",
      "18/18 [==============================] - 8s 462ms/step - loss: 0.3355 - accuracy: 0.8619 - val_loss: 0.9763 - val_accuracy: 0.5464\n",
      "Epoch 226/250\n",
      "18/18 [==============================] - 8s 462ms/step - loss: 0.3418 - accuracy: 0.8580 - val_loss: 0.9505 - val_accuracy: 0.5361\n",
      "Epoch 227/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 8s 466ms/step - loss: 0.3276 - accuracy: 0.8632 - val_loss: 0.9346 - val_accuracy: 0.5567\n",
      "Epoch 228/250\n",
      "18/18 [==============================] - 8s 465ms/step - loss: 0.3161 - accuracy: 0.8641 - val_loss: 1.1652 - val_accuracy: 0.5670\n",
      "Epoch 229/250\n",
      "18/18 [==============================] - 8s 468ms/step - loss: 0.3370 - accuracy: 0.8527 - val_loss: 0.8820 - val_accuracy: 0.6289\n",
      "Epoch 230/250\n",
      "18/18 [==============================] - 8s 466ms/step - loss: 0.3235 - accuracy: 0.8562 - val_loss: 0.8868 - val_accuracy: 0.6392\n",
      "Epoch 231/250\n",
      "18/18 [==============================] - 8s 467ms/step - loss: 0.3195 - accuracy: 0.8606 - val_loss: 1.1999 - val_accuracy: 0.4948\n",
      "Epoch 232/250\n",
      "18/18 [==============================] - 8s 466ms/step - loss: 0.3194 - accuracy: 0.8641 - val_loss: 1.3348 - val_accuracy: 0.4433\n",
      "Epoch 233/250\n",
      "18/18 [==============================] - 8s 464ms/step - loss: 0.3455 - accuracy: 0.8514 - val_loss: 0.9010 - val_accuracy: 0.5052\n",
      "Epoch 234/250\n",
      "18/18 [==============================] - 8s 471ms/step - loss: 0.3432 - accuracy: 0.8619 - val_loss: 0.9085 - val_accuracy: 0.4845\n",
      "Epoch 235/250\n",
      "18/18 [==============================] - 8s 468ms/step - loss: 0.3283 - accuracy: 0.8649 - val_loss: 1.0370 - val_accuracy: 0.4845\n",
      "Epoch 236/250\n",
      "18/18 [==============================] - 8s 472ms/step - loss: 0.3240 - accuracy: 0.8632 - val_loss: 0.8262 - val_accuracy: 0.5464\n",
      "Epoch 237/250\n",
      "18/18 [==============================] - 8s 464ms/step - loss: 0.3219 - accuracy: 0.8610 - val_loss: 0.9486 - val_accuracy: 0.5567\n",
      "Epoch 238/250\n",
      "18/18 [==============================] - 8s 465ms/step - loss: 0.3133 - accuracy: 0.8702 - val_loss: 0.7966 - val_accuracy: 0.6082\n",
      "Epoch 239/250\n",
      "18/18 [==============================] - 8s 468ms/step - loss: 0.3009 - accuracy: 0.8680 - val_loss: 0.9250 - val_accuracy: 0.5670\n",
      "Epoch 240/250\n",
      "18/18 [==============================] - 9s 476ms/step - loss: 0.3196 - accuracy: 0.8736 - val_loss: 1.2618 - val_accuracy: 0.4433\n",
      "Epoch 241/250\n",
      "18/18 [==============================] - 8s 464ms/step - loss: 0.3339 - accuracy: 0.8593 - val_loss: 0.9934 - val_accuracy: 0.5155\n",
      "Epoch 242/250\n",
      "18/18 [==============================] - 8s 461ms/step - loss: 0.3252 - accuracy: 0.8566 - val_loss: 0.9449 - val_accuracy: 0.5979\n",
      "Epoch 243/250\n",
      "18/18 [==============================] - 8s 467ms/step - loss: 0.3239 - accuracy: 0.8580 - val_loss: 1.0468 - val_accuracy: 0.4948\n",
      "Epoch 244/250\n",
      "18/18 [==============================] - 9s 474ms/step - loss: 0.3054 - accuracy: 0.8649 - val_loss: 1.0211 - val_accuracy: 0.5876\n",
      "Epoch 245/250\n",
      "18/18 [==============================] - 9s 474ms/step - loss: 0.3003 - accuracy: 0.8702 - val_loss: 0.9815 - val_accuracy: 0.6289\n",
      "Epoch 246/250\n",
      "18/18 [==============================] - 8s 473ms/step - loss: 0.3279 - accuracy: 0.8593 - val_loss: 1.0680 - val_accuracy: 0.5567\n",
      "Epoch 247/250\n",
      "18/18 [==============================] - 8s 470ms/step - loss: 0.2995 - accuracy: 0.8654 - val_loss: 0.8458 - val_accuracy: 0.6082\n",
      "Epoch 248/250\n",
      "18/18 [==============================] - 10s 528ms/step - loss: 0.3136 - accuracy: 0.8614 - val_loss: 0.9824 - val_accuracy: 0.6082\n",
      "Epoch 249/250\n",
      "18/18 [==============================] - 9s 489ms/step - loss: 0.2998 - accuracy: 0.8797 - val_loss: 0.9482 - val_accuracy: 0.5773\n",
      "Epoch 250/250\n",
      "18/18 [==============================] - 9s 486ms/step - loss: 0.2794 - accuracy: 0.8885 - val_loss: 1.0175 - val_accuracy: 0.5567\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18043bab988>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience = 100)\n",
    "mc = ModelCheckpoint('best_bi-GRU_SPX_buy_model_with_stats_RealVal.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
    "dim = 70\n",
    "v = v.reshape(-1,52,6)\n",
    "vt = vt.reshape(-1,52,6)\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Bidirectional, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "buy_model = Sequential()\n",
    "buy_model.add(Dropout(0.2))\n",
    "buy_model.add(Bidirectional(GRU(128, input_shape=(52,6), return_sequences = True, kernel_constraint = max_norm(3.))))\n",
    "buy_model.add(Dropout(0.5))\n",
    "buy_model.add(BatchNormalization())\n",
    "buy_model.add(Bidirectional(GRU(128, return_sequences = True, kernel_constraint = max_norm(3.))))\n",
    "buy_model.add(Dropout(0.5))\n",
    "buy_model.add(BatchNormalization())\n",
    "buy_model.add(Bidirectional(GRU(64, kernel_constraint = max_norm(3.))))\n",
    "buy_model.add(Dropout(0.5))\n",
    "buy_model.add(BatchNormalization())\n",
    "buy_model.add(Dense(32, activation='relu', kernel_constraint = max_norm(3.)))\n",
    "buy_model.add(Dropout(0.5))\n",
    "buy_model.add(BatchNormalization())\n",
    "buy_model.add(Dense(2, activation='sigmoid'))\n",
    "buy_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "buy_model.fit(v,buy_lab, epochs=250, batch_size=128, verbose=1, use_multiprocessing=True, \n",
    "          validation_data=(vt,bside_lab),\n",
    "         callbacks=[mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buy Side--------------------------\n",
      "0.7010309278350515\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      1.00      0.82        67\n",
      "           1       1.00      0.03      0.06        30\n",
      "\n",
      "    accuracy                           0.70        97\n",
      "   macro avg       0.85      0.52      0.44        97\n",
      "weighted avg       0.79      0.70      0.59        97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "bmodel = load_model('best_bi-GRU_SPX_buy_model_with_stats_RealVal.h5')\n",
    "pp = np.argmax(bmodel.predict(vt), axis=1)\n",
    "kk = np.max(bmodel.predict(vt), axis=1)\n",
    "from sklearn.metrics import precision_score, \\\n",
    "    recall_score, confusion_matrix, classification_report, \\\n",
    "    accuracy_score, f1_score\n",
    "print('Buy Side--------------------------')\n",
    "print(accuracy_score(bside_lab, pp))\n",
    "print(classification_report(bside_lab, pp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 1 1 1 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1]\n",
      "[0.6122777  0.61284125 0.63208896 0.6793514  0.6215278  0.58885115\n",
      " 0.60562634 0.63727355 0.604082   0.62207586 0.6146509  0.6380168\n",
      " 0.5756972  0.5995049  0.63096106 0.64049387 0.57765913 0.6215456\n",
      " 0.5972481  0.6066892  0.5835854  0.6264858  0.59853643 0.58289987\n",
      " 0.6448701  0.55524176 0.5651584  0.666909   0.6384873  0.6286603\n",
      " 0.58591795 0.5809804  0.52967006 0.66460127 0.6107118  0.66742927\n",
      " 0.5720729  0.56973135 0.55168545 0.60111845 0.5721045  0.66831434\n",
      " 0.5315737  0.64738953 0.6247201  0.5496865  0.58937895 0.6599535\n",
      " 0.5247733  0.6658948  0.60822964 0.63853157 0.63929266 0.6437035\n",
      " 0.57086635 0.67256796 0.62127405 0.6173498  0.6308403  0.6150157\n",
      " 0.6511694  0.6244651  0.6077228  0.63261986 0.65423673 0.62131846\n",
      " 0.6549855  0.6359623  0.55082744 0.69796276 0.684688   0.57358223\n",
      " 0.64301705 0.6240442  0.5716526  0.67936504 0.6601324  0.55115104\n",
      " 0.65759367 0.62217057 0.5782377  0.6489861  0.6530541  0.5300475\n",
      " 0.67655796 0.60714316 0.5814179  0.6462817  0.6313557  0.55021757\n",
      " 0.6350537  0.56973517 0.5622212  0.6135081  0.6065646  0.5918552\n",
      " 0.5930122 ]\n"
     ]
    }
   ],
   "source": [
    "print(pp)\n",
    "print(bside_lab)\n",
    "print(kk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sell Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "12/12 - 13s - loss: 0.9053 - accuracy: 0.4945 - val_loss: 0.6910 - val_accuracy: 0.5247\n",
      "Epoch 2/250\n",
      "12/12 - 5s - loss: 0.7545 - accuracy: 0.5731 - val_loss: 0.6952 - val_accuracy: 0.5432\n",
      "Epoch 3/250\n",
      "12/12 - 5s - loss: 0.6904 - accuracy: 0.5917 - val_loss: 0.6985 - val_accuracy: 0.5556\n",
      "Epoch 4/250\n",
      "12/12 - 5s - loss: 0.7243 - accuracy: 0.5752 - val_loss: 0.6981 - val_accuracy: 0.5247\n",
      "Epoch 5/250\n",
      "12/12 - 5s - loss: 0.6938 - accuracy: 0.5959 - val_loss: 0.6912 - val_accuracy: 0.5432\n",
      "Epoch 6/250\n",
      "12/12 - 5s - loss: 0.6762 - accuracy: 0.6090 - val_loss: 0.6843 - val_accuracy: 0.5802\n",
      "Epoch 7/250\n",
      "12/12 - 5s - loss: 0.6323 - accuracy: 0.6510 - val_loss: 0.6891 - val_accuracy: 0.5123\n",
      "Epoch 8/250\n",
      "12/12 - 5s - loss: 0.6521 - accuracy: 0.6248 - val_loss: 0.6870 - val_accuracy: 0.5309\n",
      "Epoch 9/250\n",
      "12/12 - 5s - loss: 0.6173 - accuracy: 0.6600 - val_loss: 0.6927 - val_accuracy: 0.5247\n",
      "Epoch 10/250\n",
      "12/12 - 5s - loss: 0.5926 - accuracy: 0.6641 - val_loss: 0.6885 - val_accuracy: 0.5432\n",
      "Epoch 11/250\n",
      "12/12 - 5s - loss: 0.6052 - accuracy: 0.6524 - val_loss: 0.7007 - val_accuracy: 0.5556\n",
      "Epoch 12/250\n",
      "12/12 - 5s - loss: 0.5670 - accuracy: 0.7000 - val_loss: 0.7385 - val_accuracy: 0.5185\n",
      "Epoch 13/250\n",
      "12/12 - 5s - loss: 0.5579 - accuracy: 0.7069 - val_loss: 0.7406 - val_accuracy: 0.5432\n",
      "Epoch 14/250\n",
      "12/12 - 5s - loss: 0.5327 - accuracy: 0.7317 - val_loss: 0.7358 - val_accuracy: 0.5556\n",
      "Epoch 15/250\n",
      "12/12 - 5s - loss: 0.5280 - accuracy: 0.7248 - val_loss: 0.7366 - val_accuracy: 0.5988\n",
      "Epoch 16/250\n",
      "12/12 - 5s - loss: 0.5132 - accuracy: 0.7393 - val_loss: 0.8012 - val_accuracy: 0.5062\n",
      "Epoch 17/250\n",
      "12/12 - 5s - loss: 0.4899 - accuracy: 0.7600 - val_loss: 0.8310 - val_accuracy: 0.5309\n",
      "Epoch 18/250\n",
      "12/12 - 5s - loss: 0.4992 - accuracy: 0.7593 - val_loss: 0.8160 - val_accuracy: 0.5556\n",
      "Epoch 19/250\n",
      "12/12 - 5s - loss: 0.4663 - accuracy: 0.7710 - val_loss: 0.8072 - val_accuracy: 0.5741\n",
      "Epoch 20/250\n",
      "12/12 - 5s - loss: 0.4900 - accuracy: 0.7572 - val_loss: 0.8056 - val_accuracy: 0.5926\n",
      "Epoch 21/250\n",
      "12/12 - 5s - loss: 0.4548 - accuracy: 0.7821 - val_loss: 0.8504 - val_accuracy: 0.6173\n",
      "Epoch 22/250\n",
      "12/12 - 5s - loss: 0.4202 - accuracy: 0.8124 - val_loss: 0.8918 - val_accuracy: 0.6173\n",
      "Epoch 23/250\n",
      "12/12 - 5s - loss: 0.4122 - accuracy: 0.8172 - val_loss: 0.9322 - val_accuracy: 0.5802\n",
      "Epoch 24/250\n",
      "12/12 - 5s - loss: 0.4044 - accuracy: 0.8179 - val_loss: 0.9347 - val_accuracy: 0.5802\n",
      "Epoch 25/250\n",
      "12/12 - 6s - loss: 0.3714 - accuracy: 0.8386 - val_loss: 0.9410 - val_accuracy: 0.6358\n",
      "Epoch 26/250\n",
      "12/12 - 5s - loss: 0.3817 - accuracy: 0.8200 - val_loss: 0.9814 - val_accuracy: 0.6173\n",
      "Epoch 27/250\n",
      "12/12 - 6s - loss: 0.3773 - accuracy: 0.8283 - val_loss: 1.0370 - val_accuracy: 0.5802\n",
      "Epoch 28/250\n",
      "12/12 - 5s - loss: 0.3372 - accuracy: 0.8476 - val_loss: 1.0541 - val_accuracy: 0.6543\n",
      "Epoch 29/250\n",
      "12/12 - 6s - loss: 0.3285 - accuracy: 0.8614 - val_loss: 1.0954 - val_accuracy: 0.6667\n",
      "Epoch 30/250\n",
      "12/12 - 6s - loss: 0.3337 - accuracy: 0.8524 - val_loss: 1.1653 - val_accuracy: 0.5864\n",
      "Epoch 31/250\n",
      "12/12 - 6s - loss: 0.3027 - accuracy: 0.8669 - val_loss: 1.1298 - val_accuracy: 0.6543\n",
      "Epoch 32/250\n",
      "12/12 - 5s - loss: 0.2846 - accuracy: 0.8745 - val_loss: 1.2003 - val_accuracy: 0.6667\n",
      "Epoch 33/250\n",
      "12/12 - 5s - loss: 0.2953 - accuracy: 0.8738 - val_loss: 1.2267 - val_accuracy: 0.5988\n",
      "Epoch 34/250\n",
      "12/12 - 5s - loss: 0.2687 - accuracy: 0.8876 - val_loss: 1.1603 - val_accuracy: 0.6481\n",
      "Epoch 35/250\n",
      "12/12 - 5s - loss: 0.2543 - accuracy: 0.9007 - val_loss: 1.2122 - val_accuracy: 0.5988\n",
      "Epoch 36/250\n",
      "12/12 - 6s - loss: 0.2259 - accuracy: 0.9069 - val_loss: 1.3202 - val_accuracy: 0.6111\n",
      "Epoch 37/250\n",
      "12/12 - 5s - loss: 0.2195 - accuracy: 0.9138 - val_loss: 1.3508 - val_accuracy: 0.5926\n",
      "Epoch 38/250\n",
      "12/12 - 6s - loss: 0.2193 - accuracy: 0.9124 - val_loss: 1.3260 - val_accuracy: 0.6358\n",
      "Epoch 39/250\n",
      "12/12 - 5s - loss: 0.2206 - accuracy: 0.9097 - val_loss: 1.5069 - val_accuracy: 0.5617\n",
      "Epoch 40/250\n",
      "12/12 - 5s - loss: 0.1950 - accuracy: 0.9221 - val_loss: 1.4705 - val_accuracy: 0.5864\n",
      "Epoch 41/250\n",
      "12/12 - 5s - loss: 0.1769 - accuracy: 0.9345 - val_loss: 1.5274 - val_accuracy: 0.6296\n",
      "Epoch 42/250\n",
      "12/12 - 6s - loss: 0.1945 - accuracy: 0.9200 - val_loss: 1.6495 - val_accuracy: 0.5864\n",
      "Epoch 43/250\n",
      "12/12 - 5s - loss: 0.1957 - accuracy: 0.9221 - val_loss: 1.5542 - val_accuracy: 0.6235\n",
      "Epoch 44/250\n",
      "12/12 - 5s - loss: 0.2104 - accuracy: 0.9172 - val_loss: 1.5687 - val_accuracy: 0.6235\n",
      "Epoch 45/250\n",
      "12/12 - 5s - loss: 0.1789 - accuracy: 0.9338 - val_loss: 1.4640 - val_accuracy: 0.6296\n",
      "Epoch 46/250\n",
      "12/12 - 5s - loss: 0.1568 - accuracy: 0.9483 - val_loss: 1.5107 - val_accuracy: 0.6481\n",
      "Epoch 47/250\n",
      "12/12 - 6s - loss: 0.1655 - accuracy: 0.9372 - val_loss: 1.5376 - val_accuracy: 0.6605\n",
      "Epoch 48/250\n",
      "12/12 - 5s - loss: 0.1294 - accuracy: 0.9503 - val_loss: 1.7026 - val_accuracy: 0.6481\n",
      "Epoch 49/250\n",
      "12/12 - 6s - loss: 0.1576 - accuracy: 0.9379 - val_loss: 1.6416 - val_accuracy: 0.6543\n",
      "Epoch 50/250\n",
      "12/12 - 5s - loss: 0.1345 - accuracy: 0.9503 - val_loss: 1.7277 - val_accuracy: 0.6420\n",
      "Epoch 51/250\n",
      "12/12 - 6s - loss: 0.1312 - accuracy: 0.9517 - val_loss: 1.8141 - val_accuracy: 0.6481\n",
      "Epoch 52/250\n",
      "12/12 - 5s - loss: 0.1334 - accuracy: 0.9524 - val_loss: 1.7996 - val_accuracy: 0.5988\n",
      "Epoch 53/250\n",
      "12/12 - 6s - loss: 0.1341 - accuracy: 0.9579 - val_loss: 1.7278 - val_accuracy: 0.6481\n",
      "Epoch 54/250\n",
      "12/12 - 6s - loss: 0.1205 - accuracy: 0.9566 - val_loss: 1.6503 - val_accuracy: 0.6358\n",
      "Epoch 55/250\n",
      "12/12 - 5s - loss: 0.1118 - accuracy: 0.9607 - val_loss: 1.6813 - val_accuracy: 0.6296\n",
      "Epoch 56/250\n",
      "12/12 - 6s - loss: 0.1043 - accuracy: 0.9648 - val_loss: 1.7496 - val_accuracy: 0.6358\n",
      "Epoch 57/250\n",
      "12/12 - 5s - loss: 0.1088 - accuracy: 0.9600 - val_loss: 1.7246 - val_accuracy: 0.6358\n",
      "Epoch 58/250\n",
      "12/12 - 6s - loss: 0.1083 - accuracy: 0.9607 - val_loss: 1.8335 - val_accuracy: 0.6358\n",
      "Epoch 59/250\n",
      "12/12 - 5s - loss: 0.1104 - accuracy: 0.9634 - val_loss: 1.8149 - val_accuracy: 0.6173\n",
      "Epoch 60/250\n",
      "12/12 - 6s - loss: 0.1075 - accuracy: 0.9634 - val_loss: 1.8204 - val_accuracy: 0.6358\n",
      "Epoch 61/250\n",
      "12/12 - 6s - loss: 0.0857 - accuracy: 0.9697 - val_loss: 1.8652 - val_accuracy: 0.6420\n",
      "Epoch 62/250\n",
      "12/12 - 5s - loss: 0.0977 - accuracy: 0.9655 - val_loss: 1.8360 - val_accuracy: 0.6235\n",
      "Epoch 63/250\n",
      "12/12 - 5s - loss: 0.0749 - accuracy: 0.9772 - val_loss: 1.8142 - val_accuracy: 0.6543\n",
      "Epoch 64/250\n",
      "12/12 - 6s - loss: 0.0642 - accuracy: 0.9786 - val_loss: 1.9106 - val_accuracy: 0.6111\n",
      "Epoch 65/250\n",
      "12/12 - 6s - loss: 0.0585 - accuracy: 0.9834 - val_loss: 1.9645 - val_accuracy: 0.6111\n",
      "Epoch 66/250\n",
      "12/12 - 5s - loss: 0.0839 - accuracy: 0.9697 - val_loss: 2.1382 - val_accuracy: 0.6420\n",
      "Epoch 67/250\n",
      "12/12 - 5s - loss: 0.0723 - accuracy: 0.9745 - val_loss: 1.9967 - val_accuracy: 0.6358\n",
      "Epoch 68/250\n",
      "12/12 - 5s - loss: 0.0907 - accuracy: 0.9697 - val_loss: 2.0700 - val_accuracy: 0.6111\n",
      "Epoch 69/250\n",
      "12/12 - 6s - loss: 0.0742 - accuracy: 0.9759 - val_loss: 2.0280 - val_accuracy: 0.6235\n",
      "Epoch 70/250\n",
      "12/12 - 6s - loss: 0.0787 - accuracy: 0.9717 - val_loss: 2.0245 - val_accuracy: 0.6358\n",
      "Epoch 71/250\n",
      "12/12 - 5s - loss: 0.0664 - accuracy: 0.9779 - val_loss: 2.1091 - val_accuracy: 0.6481\n",
      "Epoch 72/250\n",
      "12/12 - 6s - loss: 0.0672 - accuracy: 0.9772 - val_loss: 2.1345 - val_accuracy: 0.6235\n",
      "Epoch 73/250\n",
      "12/12 - 6s - loss: 0.0584 - accuracy: 0.9786 - val_loss: 2.1137 - val_accuracy: 0.6667\n",
      "Epoch 74/250\n",
      "12/12 - 6s - loss: 0.0631 - accuracy: 0.9779 - val_loss: 1.9999 - val_accuracy: 0.6543\n",
      "Epoch 75/250\n",
      "12/12 - 5s - loss: 0.0526 - accuracy: 0.9834 - val_loss: 2.1246 - val_accuracy: 0.6481\n",
      "Epoch 76/250\n",
      "12/12 - 6s - loss: 0.0452 - accuracy: 0.9834 - val_loss: 2.2162 - val_accuracy: 0.6173\n",
      "Epoch 77/250\n",
      "12/12 - 5s - loss: 0.0539 - accuracy: 0.9807 - val_loss: 2.0945 - val_accuracy: 0.6543\n",
      "Epoch 78/250\n",
      "12/12 - 6s - loss: 0.0515 - accuracy: 0.9834 - val_loss: 2.2096 - val_accuracy: 0.5926\n",
      "Epoch 79/250\n",
      "12/12 - 5s - loss: 0.0607 - accuracy: 0.9793 - val_loss: 2.1637 - val_accuracy: 0.6667\n",
      "Epoch 80/250\n",
      "12/12 - 5s - loss: 0.0751 - accuracy: 0.9717 - val_loss: 1.9691 - val_accuracy: 0.6543\n",
      "Epoch 81/250\n",
      "12/12 - 5s - loss: 0.0643 - accuracy: 0.9759 - val_loss: 2.0043 - val_accuracy: 0.6481\n",
      "Epoch 82/250\n",
      "12/12 - 6s - loss: 0.0667 - accuracy: 0.9766 - val_loss: 2.0980 - val_accuracy: 0.6481\n",
      "Epoch 83/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 5s - loss: 0.0517 - accuracy: 0.9848 - val_loss: 1.9738 - val_accuracy: 0.6481\n",
      "Epoch 84/250\n",
      "12/12 - 6s - loss: 0.0969 - accuracy: 0.9724 - val_loss: 1.9288 - val_accuracy: 0.6543\n",
      "Epoch 85/250\n",
      "12/12 - 6s - loss: 0.0915 - accuracy: 0.9738 - val_loss: 1.9557 - val_accuracy: 0.5926\n",
      "Epoch 86/250\n",
      "12/12 - 6s - loss: 0.0729 - accuracy: 0.9800 - val_loss: 1.9582 - val_accuracy: 0.6481\n",
      "Epoch 87/250\n",
      "12/12 - 5s - loss: 0.0602 - accuracy: 0.9738 - val_loss: 1.7724 - val_accuracy: 0.6481\n",
      "Epoch 88/250\n",
      "12/12 - 5s - loss: 0.0484 - accuracy: 0.9834 - val_loss: 1.8315 - val_accuracy: 0.6481\n",
      "Epoch 89/250\n",
      "12/12 - 5s - loss: 0.0432 - accuracy: 0.9869 - val_loss: 1.9844 - val_accuracy: 0.6543\n",
      "Epoch 90/250\n",
      "12/12 - 5s - loss: 0.0325 - accuracy: 0.9924 - val_loss: 2.1113 - val_accuracy: 0.6049\n",
      "Epoch 91/250\n",
      "12/12 - 5s - loss: 0.0335 - accuracy: 0.9924 - val_loss: 2.0078 - val_accuracy: 0.6481\n",
      "Epoch 92/250\n",
      "12/12 - 5s - loss: 0.0274 - accuracy: 0.9910 - val_loss: 2.1536 - val_accuracy: 0.6358\n",
      "Epoch 93/250\n",
      "12/12 - 5s - loss: 0.0476 - accuracy: 0.9897 - val_loss: 2.2659 - val_accuracy: 0.6296\n",
      "Epoch 94/250\n",
      "12/12 - 5s - loss: 0.0316 - accuracy: 0.9890 - val_loss: 2.0539 - val_accuracy: 0.6605\n",
      "Epoch 95/250\n",
      "12/12 - 6s - loss: 0.0301 - accuracy: 0.9897 - val_loss: 2.0297 - val_accuracy: 0.6728\n",
      "Epoch 96/250\n",
      "12/12 - 5s - loss: 0.0282 - accuracy: 0.9938 - val_loss: 2.0796 - val_accuracy: 0.6235\n",
      "Epoch 97/250\n",
      "12/12 - 5s - loss: 0.0418 - accuracy: 0.9848 - val_loss: 2.1001 - val_accuracy: 0.6667\n",
      "Epoch 98/250\n",
      "12/12 - 5s - loss: 0.0449 - accuracy: 0.9876 - val_loss: 2.1120 - val_accuracy: 0.6481\n",
      "Epoch 99/250\n",
      "12/12 - 5s - loss: 0.0300 - accuracy: 0.9883 - val_loss: 2.0402 - val_accuracy: 0.6543\n",
      "Epoch 100/250\n",
      "12/12 - 5s - loss: 0.0316 - accuracy: 0.9897 - val_loss: 2.1163 - val_accuracy: 0.6543\n",
      "Epoch 101/250\n",
      "12/12 - 5s - loss: 0.0375 - accuracy: 0.9869 - val_loss: 2.0859 - val_accuracy: 0.6728\n",
      "Epoch 102/250\n",
      "12/12 - 5s - loss: 0.0368 - accuracy: 0.9869 - val_loss: 2.1746 - val_accuracy: 0.6358\n",
      "Epoch 103/250\n",
      "12/12 - 5s - loss: 0.0466 - accuracy: 0.9848 - val_loss: 2.1000 - val_accuracy: 0.6543\n",
      "Epoch 104/250\n",
      "12/12 - 5s - loss: 0.0403 - accuracy: 0.9876 - val_loss: 2.2153 - val_accuracy: 0.6420\n",
      "Epoch 105/250\n",
      "12/12 - 5s - loss: 0.0551 - accuracy: 0.9855 - val_loss: 2.2199 - val_accuracy: 0.6420\n",
      "Epoch 106/250\n",
      "12/12 - 5s - loss: 0.0336 - accuracy: 0.9890 - val_loss: 2.0702 - val_accuracy: 0.6605\n",
      "Epoch 107/250\n",
      "12/12 - 5s - loss: 0.0367 - accuracy: 0.9876 - val_loss: 2.1404 - val_accuracy: 0.6790\n",
      "Epoch 108/250\n",
      "12/12 - 5s - loss: 0.0329 - accuracy: 0.9869 - val_loss: 2.1359 - val_accuracy: 0.6790\n",
      "Epoch 109/250\n",
      "12/12 - 5s - loss: 0.0467 - accuracy: 0.9821 - val_loss: 2.3153 - val_accuracy: 0.6481\n",
      "Epoch 110/250\n",
      "12/12 - 5s - loss: 0.0446 - accuracy: 0.9862 - val_loss: 2.1322 - val_accuracy: 0.6543\n",
      "Epoch 111/250\n",
      "12/12 - 5s - loss: 0.0289 - accuracy: 0.9917 - val_loss: 2.0684 - val_accuracy: 0.6790\n",
      "Epoch 112/250\n",
      "12/12 - 5s - loss: 0.0437 - accuracy: 0.9883 - val_loss: 2.2489 - val_accuracy: 0.6420\n",
      "Epoch 113/250\n",
      "12/12 - 6s - loss: 0.0276 - accuracy: 0.9924 - val_loss: 2.1150 - val_accuracy: 0.6173\n",
      "Epoch 114/250\n",
      "12/12 - 5s - loss: 0.0356 - accuracy: 0.9890 - val_loss: 2.2169 - val_accuracy: 0.6296\n",
      "Epoch 115/250\n",
      "12/12 - 5s - loss: 0.0239 - accuracy: 0.9931 - val_loss: 2.1118 - val_accuracy: 0.6543\n",
      "Epoch 116/250\n",
      "12/12 - 5s - loss: 0.0264 - accuracy: 0.9931 - val_loss: 2.2458 - val_accuracy: 0.6728\n",
      "Epoch 117/250\n",
      "12/12 - 5s - loss: 0.0246 - accuracy: 0.9910 - val_loss: 2.4703 - val_accuracy: 0.6358\n",
      "Epoch 118/250\n",
      "12/12 - 5s - loss: 0.0285 - accuracy: 0.9924 - val_loss: 2.2762 - val_accuracy: 0.6235\n",
      "Epoch 119/250\n",
      "12/12 - 5s - loss: 0.0191 - accuracy: 0.9959 - val_loss: 2.3122 - val_accuracy: 0.6235\n",
      "Epoch 120/250\n",
      "12/12 - 5s - loss: 0.0117 - accuracy: 0.9972 - val_loss: 2.2850 - val_accuracy: 0.6420\n",
      "Epoch 121/250\n",
      "12/12 - 5s - loss: 0.0111 - accuracy: 0.9979 - val_loss: 2.1828 - val_accuracy: 0.6667\n",
      "Epoch 122/250\n",
      "12/12 - 5s - loss: 0.0144 - accuracy: 0.9959 - val_loss: 2.2442 - val_accuracy: 0.6481\n",
      "Epoch 123/250\n",
      "12/12 - 5s - loss: 0.0103 - accuracy: 0.9979 - val_loss: 2.3003 - val_accuracy: 0.6420\n",
      "Epoch 124/250\n",
      "12/12 - 5s - loss: 0.0127 - accuracy: 0.9966 - val_loss: 2.3585 - val_accuracy: 0.6420\n",
      "Epoch 125/250\n",
      "12/12 - 5s - loss: 0.0085 - accuracy: 0.9979 - val_loss: 2.3752 - val_accuracy: 0.6420\n",
      "Epoch 126/250\n",
      "12/12 - 5s - loss: 0.0092 - accuracy: 0.9979 - val_loss: 2.4886 - val_accuracy: 0.6296\n",
      "Epoch 127/250\n",
      "12/12 - 5s - loss: 0.0116 - accuracy: 0.9972 - val_loss: 2.5523 - val_accuracy: 0.6358\n",
      "Epoch 128/250\n",
      "12/12 - 5s - loss: 0.0117 - accuracy: 0.9979 - val_loss: 2.4866 - val_accuracy: 0.6420\n",
      "Epoch 129/250\n",
      "12/12 - 5s - loss: 0.0128 - accuracy: 0.9972 - val_loss: 2.4698 - val_accuracy: 0.6481\n",
      "Epoch 130/250\n",
      "12/12 - 5s - loss: 0.0122 - accuracy: 0.9986 - val_loss: 2.4718 - val_accuracy: 0.6481\n",
      "Epoch 131/250\n",
      "12/12 - 5s - loss: 0.0075 - accuracy: 0.9986 - val_loss: 2.5038 - val_accuracy: 0.6481\n",
      "Epoch 132/250\n",
      "12/12 - 5s - loss: 0.0063 - accuracy: 0.9993 - val_loss: 2.6486 - val_accuracy: 0.6481\n",
      "Epoch 133/250\n",
      "12/12 - 5s - loss: 0.0084 - accuracy: 0.9966 - val_loss: 2.6254 - val_accuracy: 0.6358\n",
      "Epoch 134/250\n",
      "12/12 - 5s - loss: 0.0074 - accuracy: 0.9986 - val_loss: 2.5883 - val_accuracy: 0.6235\n",
      "Epoch 135/250\n",
      "12/12 - 5s - loss: 0.0084 - accuracy: 0.9972 - val_loss: 2.6456 - val_accuracy: 0.6358\n",
      "Epoch 136/250\n",
      "12/12 - 5s - loss: 0.0065 - accuracy: 0.9979 - val_loss: 2.6439 - val_accuracy: 0.6173\n",
      "Epoch 137/250\n",
      "12/12 - 5s - loss: 0.0231 - accuracy: 0.9917 - val_loss: 2.5683 - val_accuracy: 0.6358\n",
      "Epoch 138/250\n",
      "12/12 - 5s - loss: 0.0141 - accuracy: 0.9959 - val_loss: 2.5127 - val_accuracy: 0.6605\n",
      "Epoch 139/250\n",
      "12/12 - 5s - loss: 0.0190 - accuracy: 0.9952 - val_loss: 2.5999 - val_accuracy: 0.6420\n",
      "Epoch 140/250\n",
      "12/12 - 5s - loss: 0.0284 - accuracy: 0.9897 - val_loss: 2.5613 - val_accuracy: 0.6235\n",
      "Epoch 141/250\n",
      "12/12 - 5s - loss: 0.0291 - accuracy: 0.9890 - val_loss: 2.4727 - val_accuracy: 0.6420\n",
      "Epoch 142/250\n",
      "12/12 - 5s - loss: 0.0328 - accuracy: 0.9924 - val_loss: 2.5566 - val_accuracy: 0.6111\n",
      "Epoch 143/250\n",
      "12/12 - 5s - loss: 0.0512 - accuracy: 0.9869 - val_loss: 2.5114 - val_accuracy: 0.6049\n",
      "Epoch 144/250\n",
      "12/12 - 5s - loss: 0.0483 - accuracy: 0.9869 - val_loss: 2.5012 - val_accuracy: 0.6173\n",
      "Epoch 145/250\n",
      "12/12 - 5s - loss: 0.0235 - accuracy: 0.9910 - val_loss: 2.3737 - val_accuracy: 0.6173\n",
      "Epoch 146/250\n",
      "12/12 - 5s - loss: 0.0238 - accuracy: 0.9924 - val_loss: 2.4935 - val_accuracy: 0.6420\n",
      "Epoch 147/250\n",
      "12/12 - 5s - loss: 0.0438 - accuracy: 0.9903 - val_loss: 2.3713 - val_accuracy: 0.6481\n",
      "Epoch 148/250\n",
      "12/12 - 6s - loss: 0.0594 - accuracy: 0.9828 - val_loss: 2.4455 - val_accuracy: 0.6173\n",
      "Epoch 149/250\n",
      "12/12 - 6s - loss: 0.0315 - accuracy: 0.9890 - val_loss: 2.6105 - val_accuracy: 0.6173\n",
      "Epoch 150/250\n",
      "12/12 - 6s - loss: 0.0335 - accuracy: 0.9897 - val_loss: 2.6070 - val_accuracy: 0.6235\n",
      "Epoch 151/250\n",
      "12/12 - 6s - loss: 0.0282 - accuracy: 0.9890 - val_loss: 2.7606 - val_accuracy: 0.6420\n",
      "Epoch 152/250\n",
      "12/12 - 6s - loss: 0.0338 - accuracy: 0.9890 - val_loss: 2.5881 - val_accuracy: 0.6358\n",
      "Epoch 153/250\n",
      "12/12 - 6s - loss: 0.0334 - accuracy: 0.9876 - val_loss: 2.3810 - val_accuracy: 0.6049\n",
      "Epoch 154/250\n",
      "12/12 - 6s - loss: 0.0262 - accuracy: 0.9924 - val_loss: 2.5145 - val_accuracy: 0.6235\n",
      "Epoch 155/250\n",
      "12/12 - 6s - loss: 0.0183 - accuracy: 0.9938 - val_loss: 2.5373 - val_accuracy: 0.6420\n",
      "Epoch 156/250\n",
      "12/12 - 6s - loss: 0.0534 - accuracy: 0.9876 - val_loss: 2.5688 - val_accuracy: 0.6358\n",
      "Epoch 157/250\n",
      "12/12 - 6s - loss: 0.0403 - accuracy: 0.9910 - val_loss: 2.6844 - val_accuracy: 0.6420\n",
      "Epoch 158/250\n",
      "12/12 - 6s - loss: 0.0282 - accuracy: 0.9917 - val_loss: 2.6418 - val_accuracy: 0.6358\n",
      "Epoch 159/250\n",
      "12/12 - 6s - loss: 0.0251 - accuracy: 0.9938 - val_loss: 2.6554 - val_accuracy: 0.6296\n",
      "Epoch 160/250\n",
      "12/12 - 5s - loss: 0.0197 - accuracy: 0.9931 - val_loss: 2.6206 - val_accuracy: 0.6481\n",
      "Epoch 161/250\n",
      "12/12 - 6s - loss: 0.0094 - accuracy: 0.9986 - val_loss: 2.6325 - val_accuracy: 0.6543\n",
      "Epoch 162/250\n",
      "12/12 - 5s - loss: 0.0143 - accuracy: 0.9972 - val_loss: 2.5538 - val_accuracy: 0.6667\n",
      "Epoch 163/250\n",
      "12/12 - 5s - loss: 0.0142 - accuracy: 0.9952 - val_loss: 2.4913 - val_accuracy: 0.6667\n",
      "Epoch 164/250\n",
      "12/12 - 5s - loss: 0.0071 - accuracy: 0.9972 - val_loss: 2.3579 - val_accuracy: 0.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/250\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-b5067b5898bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m sell_model.fit(X_trains_gru, y_trains, epochs=250, batch_size=128, verbose=2, use_multiprocessing=True, \n\u001b[0;32m     18\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tests_gru\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tests\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m          callbacks=[mcs])\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3024\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1961\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mcs = ModelCheckpoint('best_bi-GRU_SPX_sell_model_with_stats_RealVal.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
    "sell_model = Sequential()\n",
    "sell_model.add(Bidirectional(GRU(128, input_shape=(100,7), return_sequences = True)))\n",
    "sell_model.add(Dropout(0.2))\n",
    "sell_model.add(BatchNormalization())\n",
    "sell_model.add(GRU(64, return_sequences = True))\n",
    "sell_model.add(Dropout(0.2))\n",
    "sell_model.add(BatchNormalization())\n",
    "sell_model.add(GRU(32))\n",
    "sell_model.add(Dropout(0.2))\n",
    "sell_model.add(BatchNormalization())\n",
    "sell_model.add(Dense(16, activation='relu'))\n",
    "sell_model.add(Dropout(0.2))\n",
    "sell_model.add(BatchNormalization())\n",
    "sell_model.add(Dense(2, activation='sigmoid'))\n",
    "sell_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "sell_model.fit(X_trains_gru, y_trains, epochs=250, batch_size=128, verbose=2, use_multiprocessing=True, \n",
    "          validation_data=(X_tests_gru, y_tests),\n",
    "         callbacks=[mcs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buy Quality Model (Meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 - 6s - loss: 0.9165 - accuracy: 0.5231 - val_loss: 0.6956 - val_accuracy: 0.4957\n",
      "Epoch 2/100\n",
      "17/17 - 0s - loss: 0.8524 - accuracy: 0.5107 - val_loss: 0.6875 - val_accuracy: 0.5726\n",
      "Epoch 3/100\n",
      "17/17 - 0s - loss: 0.8266 - accuracy: 0.5159 - val_loss: 0.6795 - val_accuracy: 0.6026\n",
      "Epoch 4/100\n",
      "17/17 - 0s - loss: 0.7920 - accuracy: 0.5288 - val_loss: 0.6829 - val_accuracy: 0.5983\n",
      "Epoch 5/100\n",
      "17/17 - 0s - loss: 0.7740 - accuracy: 0.5226 - val_loss: 0.6783 - val_accuracy: 0.5983\n",
      "Epoch 6/100\n",
      "17/17 - 0s - loss: 0.7762 - accuracy: 0.5426 - val_loss: 0.6806 - val_accuracy: 0.5726\n",
      "Epoch 7/100\n",
      "17/17 - 0s - loss: 0.7516 - accuracy: 0.5411 - val_loss: 0.6768 - val_accuracy: 0.5769\n",
      "Epoch 8/100\n",
      "17/17 - 0s - loss: 0.7452 - accuracy: 0.5345 - val_loss: 0.6757 - val_accuracy: 0.5726\n",
      "Epoch 9/100\n",
      "17/17 - 0s - loss: 0.7232 - accuracy: 0.5635 - val_loss: 0.6766 - val_accuracy: 0.5769\n",
      "Epoch 10/100\n",
      "17/17 - 0s - loss: 0.7398 - accuracy: 0.5354 - val_loss: 0.6760 - val_accuracy: 0.5769\n",
      "Epoch 11/100\n",
      "17/17 - 0s - loss: 0.7351 - accuracy: 0.5202 - val_loss: 0.6739 - val_accuracy: 0.5940\n",
      "Epoch 12/100\n",
      "17/17 - 0s - loss: 0.7345 - accuracy: 0.5411 - val_loss: 0.6728 - val_accuracy: 0.5897\n",
      "Epoch 13/100\n",
      "17/17 - 0s - loss: 0.7194 - accuracy: 0.5587 - val_loss: 0.6736 - val_accuracy: 0.6026\n",
      "Epoch 14/100\n",
      "17/17 - 0s - loss: 0.7426 - accuracy: 0.5388 - val_loss: 0.6770 - val_accuracy: 0.6111\n",
      "Epoch 15/100\n",
      "17/17 - 0s - loss: 0.7148 - accuracy: 0.5521 - val_loss: 0.6728 - val_accuracy: 0.5940\n",
      "Epoch 16/100\n",
      "17/17 - 0s - loss: 0.7069 - accuracy: 0.5668 - val_loss: 0.6757 - val_accuracy: 0.5897\n",
      "Epoch 17/100\n",
      "17/17 - 0s - loss: 0.6929 - accuracy: 0.5877 - val_loss: 0.6712 - val_accuracy: 0.6068\n",
      "Epoch 18/100\n",
      "17/17 - 0s - loss: 0.7052 - accuracy: 0.5744 - val_loss: 0.6687 - val_accuracy: 0.6111\n",
      "Epoch 19/100\n",
      "17/17 - 0s - loss: 0.7013 - accuracy: 0.5659 - val_loss: 0.6693 - val_accuracy: 0.6026\n",
      "Epoch 20/100\n",
      "17/17 - 0s - loss: 0.6933 - accuracy: 0.5663 - val_loss: 0.6722 - val_accuracy: 0.5940\n",
      "Epoch 21/100\n",
      "17/17 - 0s - loss: 0.6949 - accuracy: 0.5659 - val_loss: 0.6688 - val_accuracy: 0.6068\n",
      "Epoch 22/100\n",
      "17/17 - 0s - loss: 0.6943 - accuracy: 0.5730 - val_loss: 0.6688 - val_accuracy: 0.6026\n",
      "Epoch 23/100\n",
      "17/17 - 0s - loss: 0.6914 - accuracy: 0.5701 - val_loss: 0.6714 - val_accuracy: 0.5940\n",
      "Epoch 24/100\n",
      "17/17 - 0s - loss: 0.6815 - accuracy: 0.5801 - val_loss: 0.6696 - val_accuracy: 0.6068\n",
      "Epoch 25/100\n",
      "17/17 - 0s - loss: 0.6903 - accuracy: 0.5678 - val_loss: 0.6685 - val_accuracy: 0.6111\n",
      "Epoch 26/100\n",
      "17/17 - 0s - loss: 0.6844 - accuracy: 0.5858 - val_loss: 0.6826 - val_accuracy: 0.5427\n",
      "Epoch 27/100\n",
      "17/17 - 0s - loss: 0.6823 - accuracy: 0.5873 - val_loss: 0.6695 - val_accuracy: 0.5983\n",
      "Epoch 28/100\n",
      "17/17 - 0s - loss: 0.6855 - accuracy: 0.5811 - val_loss: 0.6694 - val_accuracy: 0.5983\n",
      "Epoch 29/100\n",
      "17/17 - 0s - loss: 0.6871 - accuracy: 0.5754 - val_loss: 0.6737 - val_accuracy: 0.6068\n",
      "Epoch 30/100\n",
      "17/17 - 0s - loss: 0.6824 - accuracy: 0.5839 - val_loss: 0.6645 - val_accuracy: 0.5940\n",
      "Epoch 31/100\n",
      "17/17 - 0s - loss: 0.6767 - accuracy: 0.5930 - val_loss: 0.6740 - val_accuracy: 0.5513\n",
      "Epoch 32/100\n",
      "17/17 - 0s - loss: 0.6829 - accuracy: 0.5744 - val_loss: 0.6632 - val_accuracy: 0.6368\n",
      "Epoch 33/100\n",
      "17/17 - 0s - loss: 0.6735 - accuracy: 0.5896 - val_loss: 0.6605 - val_accuracy: 0.6197\n",
      "Epoch 34/100\n",
      "17/17 - 0s - loss: 0.6775 - accuracy: 0.5906 - val_loss: 0.6594 - val_accuracy: 0.6026\n",
      "Epoch 35/100\n",
      "17/17 - 0s - loss: 0.6767 - accuracy: 0.5896 - val_loss: 0.6596 - val_accuracy: 0.5940\n",
      "Epoch 36/100\n",
      "17/17 - 0s - loss: 0.6758 - accuracy: 0.5977 - val_loss: 0.6716 - val_accuracy: 0.5641\n",
      "Epoch 37/100\n",
      "17/17 - 0s - loss: 0.6716 - accuracy: 0.5963 - val_loss: 0.6638 - val_accuracy: 0.5726\n",
      "Epoch 38/100\n",
      "17/17 - 0s - loss: 0.6737 - accuracy: 0.6015 - val_loss: 0.6591 - val_accuracy: 0.6068\n",
      "Epoch 39/100\n",
      "17/17 - 0s - loss: 0.6704 - accuracy: 0.5987 - val_loss: 0.6629 - val_accuracy: 0.6325\n",
      "Epoch 40/100\n",
      "17/17 - 0s - loss: 0.6688 - accuracy: 0.5958 - val_loss: 0.6582 - val_accuracy: 0.6068\n",
      "Epoch 41/100\n",
      "17/17 - 0s - loss: 0.6681 - accuracy: 0.6010 - val_loss: 0.6594 - val_accuracy: 0.5940\n",
      "Epoch 42/100\n",
      "17/17 - 0s - loss: 0.6666 - accuracy: 0.5958 - val_loss: 0.6541 - val_accuracy: 0.6197\n",
      "Epoch 43/100\n",
      "17/17 - 0s - loss: 0.6663 - accuracy: 0.6139 - val_loss: 0.6565 - val_accuracy: 0.6282\n",
      "Epoch 44/100\n",
      "17/17 - 0s - loss: 0.6576 - accuracy: 0.6129 - val_loss: 0.6615 - val_accuracy: 0.5983\n",
      "Epoch 45/100\n",
      "17/17 - 0s - loss: 0.6630 - accuracy: 0.5949 - val_loss: 0.6613 - val_accuracy: 0.5855\n",
      "Epoch 46/100\n",
      "17/17 - 0s - loss: 0.6619 - accuracy: 0.6072 - val_loss: 0.6699 - val_accuracy: 0.5812\n",
      "Epoch 47/100\n",
      "17/17 - 0s - loss: 0.6616 - accuracy: 0.6063 - val_loss: 0.6598 - val_accuracy: 0.6068\n",
      "Epoch 48/100\n",
      "17/17 - 0s - loss: 0.6591 - accuracy: 0.6163 - val_loss: 0.6581 - val_accuracy: 0.5855\n",
      "Epoch 49/100\n",
      "17/17 - 0s - loss: 0.6618 - accuracy: 0.6167 - val_loss: 0.6704 - val_accuracy: 0.6154\n",
      "Epoch 50/100\n",
      "17/17 - 0s - loss: 0.6589 - accuracy: 0.6115 - val_loss: 0.6611 - val_accuracy: 0.5769\n",
      "Epoch 51/100\n",
      "17/17 - 0s - loss: 0.6476 - accuracy: 0.6210 - val_loss: 0.6706 - val_accuracy: 0.5812\n",
      "Epoch 52/100\n",
      "17/17 - 0s - loss: 0.6556 - accuracy: 0.6139 - val_loss: 0.6603 - val_accuracy: 0.5855\n",
      "Epoch 53/100\n",
      "17/17 - 0s - loss: 0.6449 - accuracy: 0.6210 - val_loss: 0.6538 - val_accuracy: 0.6026\n",
      "Epoch 54/100\n",
      "17/17 - 0s - loss: 0.6514 - accuracy: 0.6191 - val_loss: 0.6613 - val_accuracy: 0.5769\n",
      "Epoch 55/100\n",
      "17/17 - 0s - loss: 0.6493 - accuracy: 0.6153 - val_loss: 0.6563 - val_accuracy: 0.6068\n",
      "Epoch 56/100\n",
      "17/17 - 0s - loss: 0.6448 - accuracy: 0.6320 - val_loss: 0.6539 - val_accuracy: 0.6239\n",
      "Epoch 57/100\n",
      "17/17 - 0s - loss: 0.6440 - accuracy: 0.6291 - val_loss: 0.6714 - val_accuracy: 0.6239\n",
      "Epoch 58/100\n",
      "17/17 - 0s - loss: 0.6370 - accuracy: 0.6282 - val_loss: 0.6584 - val_accuracy: 0.5897\n",
      "Epoch 59/100\n",
      "17/17 - 0s - loss: 0.6420 - accuracy: 0.6186 - val_loss: 0.6636 - val_accuracy: 0.6154\n",
      "Epoch 60/100\n",
      "17/17 - 0s - loss: 0.6344 - accuracy: 0.6372 - val_loss: 0.6594 - val_accuracy: 0.6111\n",
      "Epoch 61/100\n",
      "17/17 - 0s - loss: 0.6413 - accuracy: 0.6201 - val_loss: 0.6479 - val_accuracy: 0.6239\n",
      "Epoch 62/100\n",
      "17/17 - 0s - loss: 0.6334 - accuracy: 0.6310 - val_loss: 0.6453 - val_accuracy: 0.6068\n",
      "Epoch 63/100\n",
      "17/17 - 0s - loss: 0.6331 - accuracy: 0.6367 - val_loss: 0.6513 - val_accuracy: 0.6111\n",
      "Epoch 64/100\n",
      "17/17 - 0s - loss: 0.6372 - accuracy: 0.6229 - val_loss: 0.6336 - val_accuracy: 0.6538\n",
      "Epoch 65/100\n",
      "17/17 - 0s - loss: 0.6269 - accuracy: 0.6462 - val_loss: 0.6510 - val_accuracy: 0.6239\n",
      "Epoch 66/100\n",
      "17/17 - 0s - loss: 0.6279 - accuracy: 0.6586 - val_loss: 0.6270 - val_accuracy: 0.6496\n",
      "Epoch 67/100\n",
      "17/17 - 0s - loss: 0.6261 - accuracy: 0.6381 - val_loss: 0.6752 - val_accuracy: 0.5684\n",
      "Epoch 68/100\n",
      "17/17 - 0s - loss: 0.6248 - accuracy: 0.6424 - val_loss: 0.6467 - val_accuracy: 0.5940\n",
      "Epoch 69/100\n",
      "17/17 - 0s - loss: 0.6221 - accuracy: 0.6410 - val_loss: 0.6340 - val_accuracy: 0.6368\n",
      "Epoch 70/100\n",
      "17/17 - 0s - loss: 0.6181 - accuracy: 0.6524 - val_loss: 0.6783 - val_accuracy: 0.5940\n",
      "Epoch 71/100\n",
      "17/17 - 0s - loss: 0.6131 - accuracy: 0.6519 - val_loss: 0.6517 - val_accuracy: 0.5983\n",
      "Epoch 72/100\n",
      "17/17 - 0s - loss: 0.6094 - accuracy: 0.6567 - val_loss: 0.6486 - val_accuracy: 0.6111\n",
      "Epoch 73/100\n",
      "17/17 - 0s - loss: 0.6175 - accuracy: 0.6529 - val_loss: 0.6300 - val_accuracy: 0.6154\n",
      "Epoch 74/100\n",
      "17/17 - 0s - loss: 0.6062 - accuracy: 0.6671 - val_loss: 0.6243 - val_accuracy: 0.6453\n",
      "Epoch 75/100\n",
      "17/17 - 0s - loss: 0.6051 - accuracy: 0.6576 - val_loss: 0.6411 - val_accuracy: 0.6410\n",
      "Epoch 76/100\n",
      "17/17 - 0s - loss: 0.6104 - accuracy: 0.6543 - val_loss: 0.6471 - val_accuracy: 0.6154\n",
      "Epoch 77/100\n",
      "17/17 - 0s - loss: 0.5994 - accuracy: 0.6614 - val_loss: 0.6355 - val_accuracy: 0.6410\n",
      "Epoch 78/100\n",
      "17/17 - 0s - loss: 0.6015 - accuracy: 0.6705 - val_loss: 0.6756 - val_accuracy: 0.6026\n",
      "Epoch 79/100\n",
      "17/17 - 0s - loss: 0.5966 - accuracy: 0.6757 - val_loss: 0.6696 - val_accuracy: 0.6026\n",
      "Epoch 80/100\n",
      "17/17 - 0s - loss: 0.5942 - accuracy: 0.6724 - val_loss: 0.6466 - val_accuracy: 0.6282\n",
      "Epoch 81/100\n",
      "17/17 - 0s - loss: 0.5945 - accuracy: 0.6690 - val_loss: 0.6449 - val_accuracy: 0.6282\n",
      "Epoch 82/100\n",
      "17/17 - 0s - loss: 0.5835 - accuracy: 0.6824 - val_loss: 0.6735 - val_accuracy: 0.6325\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 - 0s - loss: 0.5865 - accuracy: 0.6786 - val_loss: 0.6306 - val_accuracy: 0.6282\n",
      "Epoch 84/100\n",
      "17/17 - 0s - loss: 0.5771 - accuracy: 0.6762 - val_loss: 0.6499 - val_accuracy: 0.6581\n",
      "Epoch 85/100\n",
      "17/17 - 0s - loss: 0.5758 - accuracy: 0.6805 - val_loss: 0.6332 - val_accuracy: 0.6368\n",
      "Epoch 86/100\n",
      "17/17 - 0s - loss: 0.5781 - accuracy: 0.6814 - val_loss: 0.6450 - val_accuracy: 0.6111\n",
      "Epoch 87/100\n",
      "17/17 - 0s - loss: 0.5672 - accuracy: 0.6890 - val_loss: 0.6255 - val_accuracy: 0.6538\n",
      "Epoch 88/100\n",
      "17/17 - 0s - loss: 0.5779 - accuracy: 0.6857 - val_loss: 0.6439 - val_accuracy: 0.6197\n",
      "Epoch 89/100\n",
      "17/17 - 0s - loss: 0.5739 - accuracy: 0.6824 - val_loss: 0.6462 - val_accuracy: 0.5897\n",
      "Epoch 90/100\n",
      "17/17 - 0s - loss: 0.5696 - accuracy: 0.6890 - val_loss: 0.6438 - val_accuracy: 0.6496\n",
      "Epoch 91/100\n",
      "17/17 - 0s - loss: 0.5666 - accuracy: 0.6857 - val_loss: 0.6419 - val_accuracy: 0.6282\n",
      "Epoch 92/100\n",
      "17/17 - 0s - loss: 0.5603 - accuracy: 0.6919 - val_loss: 0.6623 - val_accuracy: 0.6325\n",
      "Epoch 93/100\n",
      "17/17 - 0s - loss: 0.5595 - accuracy: 0.6919 - val_loss: 0.6220 - val_accuracy: 0.6282\n",
      "Epoch 94/100\n",
      "17/17 - 0s - loss: 0.5526 - accuracy: 0.6847 - val_loss: 0.6200 - val_accuracy: 0.6410\n",
      "Epoch 95/100\n",
      "17/17 - 0s - loss: 0.5471 - accuracy: 0.7133 - val_loss: 0.6377 - val_accuracy: 0.6410\n",
      "Epoch 96/100\n",
      "17/17 - 0s - loss: 0.5459 - accuracy: 0.7085 - val_loss: 0.6565 - val_accuracy: 0.6197\n",
      "Epoch 97/100\n",
      "17/17 - 0s - loss: 0.5528 - accuracy: 0.6961 - val_loss: 0.6449 - val_accuracy: 0.6624\n",
      "Epoch 98/100\n",
      "17/17 - 0s - loss: 0.5464 - accuracy: 0.7000 - val_loss: 0.6338 - val_accuracy: 0.6667\n",
      "Epoch 99/100\n",
      "17/17 - 0s - loss: 0.5303 - accuracy: 0.7199 - val_loss: 0.6012 - val_accuracy: 0.6624\n",
      "Epoch 100/100\n",
      "17/17 - 0s - loss: 0.5361 - accuracy: 0.7137 - val_loss: 0.6719 - val_accuracy: 0.6624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20026300648>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcbq = ModelCheckpoint('best_bi-GRU_SPX_buy-quality_model_with_stats_RealVal.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
    "bq_model = Sequential()\n",
    "bq_model.add(GRU(128, input_shape=(X_trainbq_gru.shape[1],1), return_sequences = True))\n",
    "bq_model.add(Dropout(0.2))\n",
    "bq_model.add(GRU(64, return_sequences = True))\n",
    "bq_model.add(Dropout(0.2))\n",
    "bq_model.add(BatchNormalization())\n",
    "bq_model.add(GRU(32))\n",
    "bq_model.add(Dropout(0.2))\n",
    "bq_model.add(BatchNormalization())\n",
    "bq_model.add(Dense(16, activation='relu'))\n",
    "bq_model.add(Dropout(0.2))\n",
    "bq_model.add(BatchNormalization())\n",
    "bq_model.add(Dense(2, activation='sigmoid'))\n",
    "bq_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "bq_model.fit(X_trainbq_gru, y_trainbq, epochs=100, batch_size=128, verbose=2, use_multiprocessing=True, \n",
    "          validation_data=(X_testbq_gru, y_testbq),\n",
    "         callbacks=[es, mcbq])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sell Quality Model (Meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 - 7s - loss: 0.9245 - accuracy: 0.5318 - val_loss: 0.7111 - val_accuracy: 0.4756\n",
      "Epoch 2/100\n",
      "12/12 - 0s - loss: 0.8689 - accuracy: 0.5014 - val_loss: 0.7017 - val_accuracy: 0.4573\n",
      "Epoch 3/100\n",
      "12/12 - 0s - loss: 0.8556 - accuracy: 0.5210 - val_loss: 0.6982 - val_accuracy: 0.5366\n",
      "Epoch 4/100\n",
      "12/12 - 0s - loss: 0.8194 - accuracy: 0.5325 - val_loss: 0.6962 - val_accuracy: 0.5061\n",
      "Epoch 5/100\n",
      "12/12 - 0s - loss: 0.8059 - accuracy: 0.5495 - val_loss: 0.7001 - val_accuracy: 0.5183\n",
      "Epoch 6/100\n",
      "12/12 - 0s - loss: 0.7850 - accuracy: 0.5501 - val_loss: 0.6948 - val_accuracy: 0.5549\n",
      "Epoch 7/100\n",
      "12/12 - 0s - loss: 0.7550 - accuracy: 0.5488 - val_loss: 0.6907 - val_accuracy: 0.5671\n",
      "Epoch 8/100\n",
      "12/12 - 0s - loss: 0.7762 - accuracy: 0.5495 - val_loss: 0.6925 - val_accuracy: 0.5854\n",
      "Epoch 9/100\n",
      "12/12 - 0s - loss: 0.7804 - accuracy: 0.5339 - val_loss: 0.6903 - val_accuracy: 0.6037\n",
      "Epoch 10/100\n",
      "12/12 - 0s - loss: 0.7634 - accuracy: 0.5454 - val_loss: 0.6835 - val_accuracy: 0.5610\n",
      "Epoch 11/100\n",
      "12/12 - 0s - loss: 0.7520 - accuracy: 0.5481 - val_loss: 0.6874 - val_accuracy: 0.5610\n",
      "Epoch 12/100\n",
      "12/12 - 0s - loss: 0.7295 - accuracy: 0.5684 - val_loss: 0.6871 - val_accuracy: 0.5610\n",
      "Epoch 13/100\n",
      "12/12 - 0s - loss: 0.7381 - accuracy: 0.5549 - val_loss: 0.6835 - val_accuracy: 0.5671\n",
      "Epoch 14/100\n",
      "12/12 - 0s - loss: 0.7368 - accuracy: 0.5650 - val_loss: 0.6785 - val_accuracy: 0.5854\n",
      "Epoch 15/100\n",
      "12/12 - 0s - loss: 0.7346 - accuracy: 0.5549 - val_loss: 0.6807 - val_accuracy: 0.5610\n",
      "Epoch 16/100\n",
      "12/12 - 0s - loss: 0.7331 - accuracy: 0.5576 - val_loss: 0.6739 - val_accuracy: 0.5793\n",
      "Epoch 17/100\n",
      "12/12 - 0s - loss: 0.7182 - accuracy: 0.5684 - val_loss: 0.6754 - val_accuracy: 0.5915\n",
      "Epoch 18/100\n",
      "12/12 - 0s - loss: 0.7350 - accuracy: 0.5644 - val_loss: 0.6818 - val_accuracy: 0.5610\n",
      "Epoch 19/100\n",
      "12/12 - 0s - loss: 0.7190 - accuracy: 0.5630 - val_loss: 0.6787 - val_accuracy: 0.5488\n",
      "Epoch 20/100\n",
      "12/12 - 0s - loss: 0.7289 - accuracy: 0.5379 - val_loss: 0.6782 - val_accuracy: 0.5732\n",
      "Epoch 21/100\n",
      "12/12 - 0s - loss: 0.7131 - accuracy: 0.5556 - val_loss: 0.6773 - val_accuracy: 0.6159\n",
      "Epoch 22/100\n",
      "12/12 - 0s - loss: 0.7062 - accuracy: 0.5725 - val_loss: 0.6784 - val_accuracy: 0.5793\n",
      "Epoch 23/100\n",
      "12/12 - 0s - loss: 0.7069 - accuracy: 0.5671 - val_loss: 0.6740 - val_accuracy: 0.5854\n",
      "Epoch 24/100\n",
      "12/12 - 0s - loss: 0.7020 - accuracy: 0.5596 - val_loss: 0.6753 - val_accuracy: 0.5671\n",
      "Epoch 25/100\n",
      "12/12 - 0s - loss: 0.7039 - accuracy: 0.5732 - val_loss: 0.6708 - val_accuracy: 0.6098\n",
      "Epoch 26/100\n",
      "12/12 - 0s - loss: 0.6845 - accuracy: 0.5813 - val_loss: 0.6785 - val_accuracy: 0.5976\n",
      "Epoch 27/100\n",
      "12/12 - 0s - loss: 0.7023 - accuracy: 0.5515 - val_loss: 0.6774 - val_accuracy: 0.5915\n",
      "Epoch 28/100\n",
      "12/12 - 0s - loss: 0.6753 - accuracy: 0.5888 - val_loss: 0.6812 - val_accuracy: 0.5427\n",
      "Epoch 29/100\n",
      "12/12 - 0s - loss: 0.6841 - accuracy: 0.5772 - val_loss: 0.6741 - val_accuracy: 0.5732\n",
      "Epoch 30/100\n",
      "12/12 - 0s - loss: 0.6804 - accuracy: 0.5854 - val_loss: 0.6781 - val_accuracy: 0.5671\n",
      "Epoch 31/100\n",
      "12/12 - 0s - loss: 0.6851 - accuracy: 0.5786 - val_loss: 0.6796 - val_accuracy: 0.5732\n",
      "Epoch 32/100\n",
      "12/12 - 0s - loss: 0.6848 - accuracy: 0.5759 - val_loss: 0.6721 - val_accuracy: 0.5610\n",
      "Epoch 33/100\n",
      "12/12 - 0s - loss: 0.6911 - accuracy: 0.5705 - val_loss: 0.6697 - val_accuracy: 0.5854\n",
      "Epoch 34/100\n",
      "12/12 - 0s - loss: 0.6861 - accuracy: 0.5732 - val_loss: 0.6782 - val_accuracy: 0.5915\n",
      "Epoch 35/100\n",
      "12/12 - 0s - loss: 0.6690 - accuracy: 0.5901 - val_loss: 0.6670 - val_accuracy: 0.5671\n",
      "Epoch 36/100\n",
      "12/12 - 0s - loss: 0.6781 - accuracy: 0.5820 - val_loss: 0.6666 - val_accuracy: 0.5732\n",
      "Epoch 37/100\n",
      "12/12 - 0s - loss: 0.6878 - accuracy: 0.5874 - val_loss: 0.6840 - val_accuracy: 0.5793\n",
      "Epoch 38/100\n",
      "12/12 - 0s - loss: 0.6657 - accuracy: 0.5908 - val_loss: 0.6646 - val_accuracy: 0.5854\n",
      "Epoch 39/100\n",
      "12/12 - 0s - loss: 0.6721 - accuracy: 0.5766 - val_loss: 0.6627 - val_accuracy: 0.5793\n",
      "Epoch 40/100\n",
      "12/12 - 0s - loss: 0.6628 - accuracy: 0.5915 - val_loss: 0.6723 - val_accuracy: 0.5732\n",
      "Epoch 41/100\n",
      "12/12 - 0s - loss: 0.6653 - accuracy: 0.5935 - val_loss: 0.6651 - val_accuracy: 0.5610\n",
      "Epoch 42/100\n",
      "12/12 - 0s - loss: 0.6702 - accuracy: 0.5921 - val_loss: 0.6687 - val_accuracy: 0.5549\n",
      "Epoch 43/100\n",
      "12/12 - 0s - loss: 0.6641 - accuracy: 0.6077 - val_loss: 0.6681 - val_accuracy: 0.5976\n",
      "Epoch 44/100\n",
      "12/12 - 0s - loss: 0.6711 - accuracy: 0.5942 - val_loss: 0.6640 - val_accuracy: 0.5671\n",
      "Epoch 45/100\n",
      "12/12 - 0s - loss: 0.6690 - accuracy: 0.5793 - val_loss: 0.6591 - val_accuracy: 0.5671\n",
      "Epoch 46/100\n",
      "12/12 - 0s - loss: 0.6602 - accuracy: 0.5684 - val_loss: 0.6602 - val_accuracy: 0.5671\n",
      "Epoch 47/100\n",
      "12/12 - 0s - loss: 0.6572 - accuracy: 0.5921 - val_loss: 0.6623 - val_accuracy: 0.5976\n",
      "Epoch 48/100\n",
      "12/12 - 0s - loss: 0.6524 - accuracy: 0.5969 - val_loss: 0.6607 - val_accuracy: 0.5915\n",
      "Epoch 49/100\n",
      "12/12 - 0s - loss: 0.6629 - accuracy: 0.5833 - val_loss: 0.6602 - val_accuracy: 0.5793\n",
      "Epoch 50/100\n",
      "12/12 - 0s - loss: 0.6563 - accuracy: 0.5976 - val_loss: 0.6546 - val_accuracy: 0.5915\n",
      "Epoch 51/100\n",
      "12/12 - 0s - loss: 0.6573 - accuracy: 0.6077 - val_loss: 0.6643 - val_accuracy: 0.5793\n",
      "Epoch 52/100\n",
      "12/12 - 0s - loss: 0.6469 - accuracy: 0.6111 - val_loss: 0.6463 - val_accuracy: 0.6037\n",
      "Epoch 53/100\n",
      "12/12 - 0s - loss: 0.6560 - accuracy: 0.5921 - val_loss: 0.6851 - val_accuracy: 0.5793\n",
      "Epoch 54/100\n",
      "12/12 - 0s - loss: 0.6577 - accuracy: 0.6009 - val_loss: 0.6731 - val_accuracy: 0.5610\n",
      "Epoch 55/100\n",
      "12/12 - 0s - loss: 0.6490 - accuracy: 0.5982 - val_loss: 0.6526 - val_accuracy: 0.5793\n",
      "Epoch 56/100\n",
      "12/12 - 0s - loss: 0.6375 - accuracy: 0.6104 - val_loss: 0.6519 - val_accuracy: 0.5793\n",
      "Epoch 57/100\n",
      "12/12 - 0s - loss: 0.6445 - accuracy: 0.6009 - val_loss: 0.6628 - val_accuracy: 0.5915\n",
      "Epoch 58/100\n",
      "12/12 - 0s - loss: 0.6482 - accuracy: 0.5915 - val_loss: 0.6499 - val_accuracy: 0.5854\n",
      "Epoch 59/100\n",
      "12/12 - 0s - loss: 0.6379 - accuracy: 0.6091 - val_loss: 0.6588 - val_accuracy: 0.6098\n",
      "Epoch 60/100\n",
      "12/12 - 0s - loss: 0.6390 - accuracy: 0.5962 - val_loss: 0.6627 - val_accuracy: 0.5793\n",
      "Epoch 61/100\n",
      "12/12 - 0s - loss: 0.6458 - accuracy: 0.6145 - val_loss: 0.6517 - val_accuracy: 0.5915\n",
      "Epoch 62/100\n",
      "12/12 - 0s - loss: 0.6573 - accuracy: 0.5894 - val_loss: 0.6530 - val_accuracy: 0.6037\n",
      "Epoch 63/100\n",
      "12/12 - 0s - loss: 0.6362 - accuracy: 0.6220 - val_loss: 0.6422 - val_accuracy: 0.5976\n",
      "Epoch 64/100\n",
      "12/12 - 0s - loss: 0.6358 - accuracy: 0.6159 - val_loss: 0.6562 - val_accuracy: 0.5732\n",
      "Epoch 65/100\n",
      "12/12 - 0s - loss: 0.6332 - accuracy: 0.6138 - val_loss: 0.6596 - val_accuracy: 0.6159\n",
      "Epoch 66/100\n",
      "12/12 - 0s - loss: 0.6324 - accuracy: 0.6165 - val_loss: 0.6447 - val_accuracy: 0.5793\n",
      "Epoch 67/100\n",
      "12/12 - 0s - loss: 0.6281 - accuracy: 0.6274 - val_loss: 0.6809 - val_accuracy: 0.5549\n",
      "Epoch 68/100\n",
      "12/12 - 0s - loss: 0.6361 - accuracy: 0.6138 - val_loss: 0.6863 - val_accuracy: 0.5366\n",
      "Epoch 69/100\n",
      "12/12 - 0s - loss: 0.6392 - accuracy: 0.6213 - val_loss: 0.6558 - val_accuracy: 0.5549\n",
      "Epoch 70/100\n",
      "12/12 - 0s - loss: 0.6184 - accuracy: 0.6436 - val_loss: 0.6474 - val_accuracy: 0.5793\n",
      "Epoch 71/100\n",
      "12/12 - 0s - loss: 0.6335 - accuracy: 0.6165 - val_loss: 0.6518 - val_accuracy: 0.5915\n",
      "Epoch 72/100\n",
      "12/12 - 0s - loss: 0.6182 - accuracy: 0.6416 - val_loss: 0.6527 - val_accuracy: 0.5793\n",
      "Epoch 73/100\n",
      "12/12 - 0s - loss: 0.6268 - accuracy: 0.6423 - val_loss: 0.6536 - val_accuracy: 0.5915\n",
      "Epoch 74/100\n",
      "12/12 - 0s - loss: 0.6228 - accuracy: 0.6260 - val_loss: 0.6464 - val_accuracy: 0.5976\n",
      "Epoch 75/100\n",
      "12/12 - 0s - loss: 0.6236 - accuracy: 0.6362 - val_loss: 0.6360 - val_accuracy: 0.6037\n",
      "Epoch 76/100\n",
      "12/12 - 0s - loss: 0.6255 - accuracy: 0.6314 - val_loss: 0.6474 - val_accuracy: 0.5976\n",
      "Epoch 77/100\n",
      "12/12 - 0s - loss: 0.6267 - accuracy: 0.6375 - val_loss: 0.6447 - val_accuracy: 0.5793\n",
      "Epoch 78/100\n",
      "12/12 - 0s - loss: 0.6205 - accuracy: 0.6348 - val_loss: 0.6411 - val_accuracy: 0.5976\n",
      "Epoch 79/100\n",
      "12/12 - 0s - loss: 0.6234 - accuracy: 0.6280 - val_loss: 0.6330 - val_accuracy: 0.6098\n",
      "Epoch 80/100\n",
      "12/12 - 0s - loss: 0.6109 - accuracy: 0.6389 - val_loss: 0.6466 - val_accuracy: 0.5976\n",
      "Epoch 81/100\n",
      "12/12 - 0s - loss: 0.6075 - accuracy: 0.6402 - val_loss: 0.6323 - val_accuracy: 0.6037\n",
      "Epoch 82/100\n",
      "12/12 - 0s - loss: 0.6134 - accuracy: 0.6484 - val_loss: 0.6315 - val_accuracy: 0.6220\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 0s - loss: 0.6119 - accuracy: 0.6491 - val_loss: 0.6384 - val_accuracy: 0.6098\n",
      "Epoch 84/100\n",
      "12/12 - 0s - loss: 0.6100 - accuracy: 0.6396 - val_loss: 0.6405 - val_accuracy: 0.5976\n",
      "Epoch 85/100\n",
      "12/12 - 0s - loss: 0.6211 - accuracy: 0.6457 - val_loss: 0.6280 - val_accuracy: 0.6280\n",
      "Epoch 86/100\n",
      "12/12 - 0s - loss: 0.6166 - accuracy: 0.6470 - val_loss: 0.6334 - val_accuracy: 0.6220\n",
      "Epoch 87/100\n",
      "12/12 - 0s - loss: 0.6137 - accuracy: 0.6511 - val_loss: 0.6339 - val_accuracy: 0.6159\n",
      "Epoch 88/100\n",
      "12/12 - 0s - loss: 0.6086 - accuracy: 0.6402 - val_loss: 0.6334 - val_accuracy: 0.6037\n",
      "Epoch 89/100\n",
      "12/12 - 0s - loss: 0.6024 - accuracy: 0.6524 - val_loss: 0.6219 - val_accuracy: 0.6524\n",
      "Epoch 90/100\n",
      "12/12 - 0s - loss: 0.6040 - accuracy: 0.6565 - val_loss: 0.6253 - val_accuracy: 0.6037\n",
      "Epoch 91/100\n",
      "12/12 - 0s - loss: 0.6013 - accuracy: 0.6646 - val_loss: 0.6378 - val_accuracy: 0.6159\n",
      "Epoch 92/100\n",
      "12/12 - 0s - loss: 0.6086 - accuracy: 0.6450 - val_loss: 0.6229 - val_accuracy: 0.6098\n",
      "Epoch 93/100\n",
      "12/12 - 0s - loss: 0.5987 - accuracy: 0.6409 - val_loss: 0.6347 - val_accuracy: 0.5915\n",
      "Epoch 94/100\n",
      "12/12 - 0s - loss: 0.5920 - accuracy: 0.6721 - val_loss: 0.6157 - val_accuracy: 0.6220\n",
      "Epoch 95/100\n",
      "12/12 - 0s - loss: 0.5921 - accuracy: 0.6660 - val_loss: 0.6399 - val_accuracy: 0.6098\n",
      "Epoch 96/100\n",
      "12/12 - 0s - loss: 0.5891 - accuracy: 0.6640 - val_loss: 0.6363 - val_accuracy: 0.6098\n",
      "Epoch 97/100\n",
      "12/12 - 0s - loss: 0.5987 - accuracy: 0.6572 - val_loss: 0.6040 - val_accuracy: 0.6341\n",
      "Epoch 98/100\n",
      "12/12 - 0s - loss: 0.6068 - accuracy: 0.6321 - val_loss: 0.6038 - val_accuracy: 0.6524\n",
      "Epoch 99/100\n",
      "12/12 - 0s - loss: 0.6032 - accuracy: 0.6538 - val_loss: 0.6197 - val_accuracy: 0.6037\n",
      "Epoch 100/100\n",
      "12/12 - 0s - loss: 0.5916 - accuracy: 0.6551 - val_loss: 0.6074 - val_accuracy: 0.6341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ffc17d9588>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcsq = ModelCheckpoint('best_bi-GRU_SPX_sell-quality_model_with_stats_RealVal.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
    "sq_model = Sequential()\n",
    "sq_model.add(GRU(128, input_shape=(X_trainsq_gru.shape[1],1), return_sequences = True))\n",
    "sq_model.add(Dropout(0.2))\n",
    "sq_model.add(GRU(64, return_sequences = True))\n",
    "sq_model.add(Dropout(0.2))\n",
    "sq_model.add(BatchNormalization())\n",
    "sq_model.add(GRU(32))\n",
    "sq_model.add(Dropout(0.2))\n",
    "sq_model.add(BatchNormalization())\n",
    "sq_model.add(Dense(16, activation='relu'))\n",
    "sq_model.add(Dropout(0.2))\n",
    "sq_model.add(BatchNormalization())\n",
    "sq_model.add(Dense(2, activation='sigmoid'))\n",
    "sq_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "sq_model.fit(X_trainsq_gru, y_trainsq, epochs=100, batch_size=128, verbose=2, use_multiprocessing=True, \n",
    "          validation_data=(X_testsq_gru, y_testsq),\n",
    "         callbacks=[es, mcsq])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Price Movement Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "17/17 - 8s - loss: 41.4729 - val_loss: 44.3895\n",
      "Epoch 2/200\n",
      "17/17 - 1s - loss: 41.3494 - val_loss: 44.3221\n",
      "Epoch 3/200\n",
      "17/17 - 1s - loss: 41.2457 - val_loss: 43.9198\n",
      "Epoch 4/200\n",
      "17/17 - 1s - loss: 41.1383 - val_loss: 43.9481\n",
      "Epoch 5/200\n",
      "17/17 - 1s - loss: 40.9891 - val_loss: 43.9348\n",
      "Epoch 6/200\n",
      "17/17 - 1s - loss: 40.8694 - val_loss: 43.4636\n",
      "Epoch 7/200\n",
      "17/17 - 1s - loss: 40.7141 - val_loss: 43.7131\n",
      "Epoch 8/200\n",
      "17/17 - 1s - loss: 40.5197 - val_loss: 43.2691\n",
      "Epoch 9/200\n",
      "17/17 - 1s - loss: 40.3422 - val_loss: 43.2422\n",
      "Epoch 10/200\n",
      "17/17 - 1s - loss: 40.1020 - val_loss: 42.8433\n",
      "Epoch 11/200\n",
      "17/17 - 1s - loss: 39.8743 - val_loss: 42.1852\n",
      "Epoch 12/200\n",
      "17/17 - 1s - loss: 39.6691 - val_loss: 41.5321\n",
      "Epoch 13/200\n",
      "17/17 - 1s - loss: 39.3603 - val_loss: 41.2531\n",
      "Epoch 14/200\n",
      "17/17 - 1s - loss: 39.0608 - val_loss: 42.8333\n",
      "Epoch 15/200\n",
      "17/17 - 1s - loss: 38.8235 - val_loss: 42.4305\n",
      "Epoch 16/200\n",
      "17/17 - 1s - loss: 38.4448 - val_loss: 40.7436\n",
      "Epoch 17/200\n",
      "17/17 - 1s - loss: 38.0854 - val_loss: 40.9095\n",
      "Epoch 18/200\n",
      "17/17 - 1s - loss: 37.7397 - val_loss: 40.0779\n",
      "Epoch 19/200\n",
      "17/17 - 1s - loss: 37.3619 - val_loss: 41.2482\n",
      "Epoch 20/200\n",
      "17/17 - 1s - loss: 37.0256 - val_loss: 40.6000\n",
      "Epoch 21/200\n",
      "17/17 - 1s - loss: 36.5867 - val_loss: 39.9231\n",
      "Epoch 22/200\n",
      "17/17 - 1s - loss: 36.2674 - val_loss: 37.5604\n",
      "Epoch 23/200\n",
      "17/17 - 1s - loss: 35.7883 - val_loss: 38.5346\n",
      "Epoch 24/200\n",
      "17/17 - 1s - loss: 35.3991 - val_loss: 37.5385\n",
      "Epoch 25/200\n",
      "17/17 - 1s - loss: 34.9571 - val_loss: 37.1290\n",
      "Epoch 26/200\n",
      "17/17 - 1s - loss: 34.3613 - val_loss: 37.7538\n",
      "Epoch 27/200\n",
      "17/17 - 1s - loss: 34.0406 - val_loss: 36.5523\n",
      "Epoch 28/200\n",
      "17/17 - 1s - loss: 33.5624 - val_loss: 34.9850\n",
      "Epoch 29/200\n",
      "17/17 - 1s - loss: 33.1635 - val_loss: 34.8660\n",
      "Epoch 30/200\n",
      "17/17 - 1s - loss: 32.5706 - val_loss: 34.0623\n",
      "Epoch 31/200\n",
      "17/17 - 1s - loss: 32.1963 - val_loss: 31.7522\n",
      "Epoch 32/200\n",
      "17/17 - 1s - loss: 31.7040 - val_loss: 30.8117\n",
      "Epoch 33/200\n",
      "17/17 - 1s - loss: 31.3453 - val_loss: 32.9683\n",
      "Epoch 34/200\n",
      "17/17 - 1s - loss: 30.7345 - val_loss: 33.2318\n",
      "Epoch 35/200\n",
      "17/17 - 1s - loss: 30.2508 - val_loss: 32.0447\n",
      "Epoch 36/200\n",
      "17/17 - 1s - loss: 29.8745 - val_loss: 31.7135\n",
      "Epoch 37/200\n",
      "17/17 - 1s - loss: 29.3019 - val_loss: 33.5286\n",
      "Epoch 38/200\n",
      "17/17 - 1s - loss: 28.7988 - val_loss: 29.8261\n",
      "Epoch 39/200\n",
      "17/17 - 1s - loss: 28.3507 - val_loss: 30.0855\n",
      "Epoch 40/200\n",
      "17/17 - 1s - loss: 28.2031 - val_loss: 31.8511\n",
      "Epoch 41/200\n",
      "17/17 - 1s - loss: 27.5288 - val_loss: 29.1596\n",
      "Epoch 42/200\n",
      "17/17 - 1s - loss: 27.1166 - val_loss: 28.8133\n",
      "Epoch 43/200\n",
      "17/17 - 1s - loss: 26.5085 - val_loss: 27.8189\n",
      "Epoch 44/200\n",
      "17/17 - 1s - loss: 26.2601 - val_loss: 29.9100\n",
      "Epoch 45/200\n",
      "17/17 - 1s - loss: 26.0187 - val_loss: 29.5093\n",
      "Epoch 46/200\n",
      "17/17 - 1s - loss: 25.3424 - val_loss: 28.0375\n",
      "Epoch 47/200\n",
      "17/17 - 1s - loss: 24.9944 - val_loss: 28.2463\n",
      "Epoch 48/200\n",
      "17/17 - 1s - loss: 24.6603 - val_loss: 27.6594\n",
      "Epoch 49/200\n",
      "17/17 - 1s - loss: 24.3289 - val_loss: 27.0922\n",
      "Epoch 50/200\n",
      "17/17 - 1s - loss: 23.7616 - val_loss: 25.7229\n",
      "Epoch 51/200\n",
      "17/17 - 1s - loss: 23.6945 - val_loss: 25.3549\n",
      "Epoch 52/200\n",
      "17/17 - 1s - loss: 23.4707 - val_loss: 25.8834\n",
      "Epoch 53/200\n",
      "17/17 - 1s - loss: 23.0744 - val_loss: 25.0301\n",
      "Epoch 54/200\n",
      "17/17 - 1s - loss: 23.0966 - val_loss: 25.9473\n",
      "Epoch 55/200\n",
      "17/17 - 1s - loss: 22.6865 - val_loss: 25.5984\n",
      "Epoch 56/200\n",
      "17/17 - 1s - loss: 22.0814 - val_loss: 24.5558\n",
      "Epoch 57/200\n",
      "17/17 - 1s - loss: 21.9642 - val_loss: 24.4238\n",
      "Epoch 58/200\n",
      "17/17 - 1s - loss: 21.7404 - val_loss: 24.4582\n",
      "Epoch 59/200\n",
      "17/17 - 1s - loss: 21.7014 - val_loss: 25.1520\n",
      "Epoch 60/200\n",
      "17/17 - 1s - loss: 21.4518 - val_loss: 25.2785\n",
      "Epoch 61/200\n",
      "17/17 - 1s - loss: 21.0996 - val_loss: 25.3257\n",
      "Epoch 62/200\n",
      "17/17 - 1s - loss: 20.8713 - val_loss: 24.1656\n",
      "Epoch 63/200\n",
      "17/17 - 1s - loss: 20.6435 - val_loss: 23.5884\n",
      "Epoch 64/200\n",
      "17/17 - 1s - loss: 20.5924 - val_loss: 24.1144\n",
      "Epoch 65/200\n",
      "17/17 - 1s - loss: 20.4265 - val_loss: 23.9278\n",
      "Epoch 66/200\n",
      "17/17 - 1s - loss: 20.4395 - val_loss: 24.2324\n",
      "Epoch 67/200\n",
      "17/17 - 1s - loss: 20.5963 - val_loss: 24.4879\n",
      "Epoch 68/200\n",
      "17/17 - 1s - loss: 20.0731 - val_loss: 22.9734\n",
      "Epoch 69/200\n",
      "17/17 - 1s - loss: 19.8322 - val_loss: 24.0764\n",
      "Epoch 70/200\n",
      "17/17 - 1s - loss: 19.8048 - val_loss: 23.2043\n",
      "Epoch 71/200\n",
      "17/17 - 1s - loss: 19.3338 - val_loss: 22.5653\n",
      "Epoch 72/200\n",
      "17/17 - 1s - loss: 19.1269 - val_loss: 22.9480\n",
      "Epoch 73/200\n",
      "17/17 - 1s - loss: 19.1759 - val_loss: 22.5081\n",
      "Epoch 74/200\n",
      "17/17 - 1s - loss: 18.9165 - val_loss: 23.7620\n",
      "Epoch 75/200\n",
      "17/17 - 1s - loss: 19.1255 - val_loss: 22.4402\n",
      "Epoch 76/200\n",
      "17/17 - 1s - loss: 19.1198 - val_loss: 23.1444\n",
      "Epoch 77/200\n",
      "17/17 - 1s - loss: 18.6682 - val_loss: 22.4991\n",
      "Epoch 78/200\n",
      "17/17 - 1s - loss: 18.6750 - val_loss: 21.7577\n",
      "Epoch 79/200\n",
      "17/17 - 1s - loss: 18.1549 - val_loss: 22.2826\n",
      "Epoch 80/200\n",
      "17/17 - 1s - loss: 18.0666 - val_loss: 21.7102\n",
      "Epoch 81/200\n",
      "17/17 - 1s - loss: 17.9674 - val_loss: 22.2884\n",
      "Epoch 82/200\n",
      "17/17 - 1s - loss: 18.0294 - val_loss: 22.7577\n",
      "Epoch 83/200\n",
      "17/17 - 1s - loss: 17.8766 - val_loss: 22.0830\n",
      "Epoch 84/200\n",
      "17/17 - 1s - loss: 17.9226 - val_loss: 22.0569\n",
      "Epoch 85/200\n",
      "17/17 - 1s - loss: 17.6976 - val_loss: 22.6082\n",
      "Epoch 86/200\n",
      "17/17 - 1s - loss: 18.1186 - val_loss: 22.3020\n",
      "Epoch 87/200\n",
      "17/17 - 1s - loss: 17.7635 - val_loss: 21.8341\n",
      "Epoch 88/200\n",
      "17/17 - 1s - loss: 17.6473 - val_loss: 21.9093\n",
      "Epoch 89/200\n",
      "17/17 - 1s - loss: 17.4567 - val_loss: 22.3544\n",
      "Epoch 90/200\n",
      "17/17 - 1s - loss: 17.4882 - val_loss: 21.8475\n",
      "Epoch 91/200\n",
      "17/17 - 1s - loss: 17.1125 - val_loss: 21.8468\n",
      "Epoch 92/200\n",
      "17/17 - 1s - loss: 17.0759 - val_loss: 22.2452\n",
      "Epoch 93/200\n",
      "17/17 - 1s - loss: 17.2449 - val_loss: 21.6749\n",
      "Epoch 94/200\n",
      "17/17 - 1s - loss: 17.2781 - val_loss: 22.1514\n",
      "Epoch 95/200\n",
      "17/17 - 1s - loss: 16.7113 - val_loss: 21.8522\n",
      "Epoch 96/200\n",
      "17/17 - 1s - loss: 16.9199 - val_loss: 21.5446\n",
      "Epoch 97/200\n",
      "17/17 - 1s - loss: 17.1572 - val_loss: 21.6490\n",
      "Epoch 98/200\n",
      "17/17 - 1s - loss: 16.9129 - val_loss: 21.1109\n",
      "Epoch 99/200\n",
      "17/17 - 1s - loss: 16.5955 - val_loss: 21.8737\n",
      "Epoch 100/200\n",
      "17/17 - 1s - loss: 16.3120 - val_loss: 22.3580\n",
      "Epoch 101/200\n",
      "17/17 - 1s - loss: 16.6720 - val_loss: 21.2899\n",
      "Epoch 102/200\n",
      "17/17 - 1s - loss: 16.5031 - val_loss: 21.8977\n",
      "Epoch 103/200\n",
      "17/17 - 1s - loss: 16.5972 - val_loss: 22.3719\n",
      "Epoch 104/200\n",
      "17/17 - 1s - loss: 16.4653 - val_loss: 21.3892\n",
      "Epoch 105/200\n",
      "17/17 - 1s - loss: 16.6706 - val_loss: 21.2918\n",
      "Epoch 106/200\n",
      "17/17 - 1s - loss: 16.6820 - val_loss: 21.1727\n",
      "Epoch 107/200\n",
      "17/17 - 1s - loss: 16.4349 - val_loss: 22.5553\n",
      "Epoch 108/200\n",
      "17/17 - 1s - loss: 16.3916 - val_loss: 22.1464\n",
      "Epoch 109/200\n",
      "17/17 - 1s - loss: 16.1243 - val_loss: 21.5853\n",
      "Epoch 110/200\n",
      "17/17 - 1s - loss: 16.1007 - val_loss: 22.0235\n",
      "Epoch 111/200\n",
      "17/17 - 1s - loss: 16.0024 - val_loss: 21.5643\n",
      "Epoch 112/200\n",
      "17/17 - 1s - loss: 15.8993 - val_loss: 21.3951\n",
      "Epoch 113/200\n",
      "17/17 - 1s - loss: 15.7891 - val_loss: 21.6001\n",
      "Epoch 114/200\n",
      "17/17 - 1s - loss: 15.5980 - val_loss: 21.5460\n",
      "Epoch 115/200\n",
      "17/17 - 1s - loss: 15.6867 - val_loss: 21.2249\n",
      "Epoch 116/200\n",
      "17/17 - 1s - loss: 16.0293 - val_loss: 21.1177\n",
      "Epoch 117/200\n",
      "17/17 - 1s - loss: 15.8603 - val_loss: 20.9991\n",
      "Epoch 118/200\n",
      "17/17 - 1s - loss: 15.6756 - val_loss: 22.0168\n",
      "Epoch 119/200\n",
      "17/17 - 1s - loss: 15.7900 - val_loss: 21.2688\n",
      "Epoch 120/200\n",
      "17/17 - 1s - loss: 15.8177 - val_loss: 20.5887\n",
      "Epoch 121/200\n",
      "17/17 - 1s - loss: 15.5162 - val_loss: 20.7472\n",
      "Epoch 122/200\n",
      "17/17 - 1s - loss: 15.3862 - val_loss: 20.9587\n",
      "Epoch 123/200\n",
      "17/17 - 1s - loss: 15.4995 - val_loss: 20.8216\n",
      "Epoch 124/200\n",
      "17/17 - 1s - loss: 15.6566 - val_loss: 21.2169\n",
      "Epoch 125/200\n",
      "17/17 - 1s - loss: 15.1903 - val_loss: 20.9154\n",
      "Epoch 126/200\n",
      "17/17 - 1s - loss: 14.8839 - val_loss: 21.4806\n",
      "Epoch 127/200\n",
      "17/17 - 1s - loss: 15.6071 - val_loss: 21.1969\n",
      "Epoch 128/200\n",
      "17/17 - 1s - loss: 15.4631 - val_loss: 20.3599\n",
      "Epoch 129/200\n",
      "17/17 - 1s - loss: 15.0571 - val_loss: 20.7278\n",
      "Epoch 130/200\n",
      "17/17 - 1s - loss: 15.1904 - val_loss: 21.4875\n",
      "Epoch 131/200\n",
      "17/17 - 1s - loss: 15.2893 - val_loss: 21.3452\n",
      "Epoch 132/200\n",
      "17/17 - 1s - loss: 15.0209 - val_loss: 20.7054\n",
      "Epoch 133/200\n",
      "17/17 - 1s - loss: 15.0150 - val_loss: 21.0083\n",
      "Epoch 134/200\n",
      "17/17 - 1s - loss: 14.7277 - val_loss: 20.9215\n",
      "Epoch 135/200\n",
      "17/17 - 1s - loss: 15.0326 - val_loss: 20.5658\n",
      "Epoch 136/200\n",
      "17/17 - 1s - loss: 14.8054 - val_loss: 20.4738\n",
      "Epoch 137/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 - 1s - loss: 14.7747 - val_loss: 21.0655\n",
      "Epoch 138/200\n",
      "17/17 - 1s - loss: 14.8787 - val_loss: 20.2887\n",
      "Epoch 139/200\n",
      "17/17 - 1s - loss: 14.8275 - val_loss: 20.3505\n",
      "Epoch 140/200\n",
      "17/17 - 1s - loss: 14.6210 - val_loss: 20.4417\n",
      "Epoch 141/200\n",
      "17/17 - 1s - loss: 14.6848 - val_loss: 20.0233\n",
      "Epoch 142/200\n",
      "17/17 - 1s - loss: 14.3750 - val_loss: 20.8136\n",
      "Epoch 143/200\n",
      "17/17 - 1s - loss: 14.7809 - val_loss: 20.0406\n",
      "Epoch 144/200\n",
      "17/17 - 1s - loss: 14.2019 - val_loss: 20.4669\n",
      "Epoch 145/200\n",
      "17/17 - 1s - loss: 14.6583 - val_loss: 19.6098\n",
      "Epoch 146/200\n",
      "17/17 - 1s - loss: 14.5015 - val_loss: 20.1712\n",
      "Epoch 147/200\n",
      "17/17 - 1s - loss: 14.2476 - val_loss: 19.9084\n",
      "Epoch 148/200\n",
      "17/17 - 1s - loss: 14.0915 - val_loss: 19.3136\n",
      "Epoch 149/200\n",
      "17/17 - 1s - loss: 14.2595 - val_loss: 20.2822\n",
      "Epoch 150/200\n",
      "17/17 - 1s - loss: 14.6401 - val_loss: 20.0752\n",
      "Epoch 151/200\n",
      "17/17 - 1s - loss: 14.1246 - val_loss: 20.4095\n",
      "Epoch 152/200\n",
      "17/17 - 1s - loss: 14.2086 - val_loss: 19.9393\n",
      "Epoch 153/200\n",
      "17/17 - 1s - loss: 14.0604 - val_loss: 20.2622\n",
      "Epoch 154/200\n",
      "17/17 - 1s - loss: 13.9119 - val_loss: 20.2247\n",
      "Epoch 155/200\n",
      "17/17 - 1s - loss: 14.5218 - val_loss: 20.3408\n",
      "Epoch 156/200\n",
      "17/17 - 1s - loss: 13.8898 - val_loss: 19.6453\n",
      "Epoch 157/200\n",
      "17/17 - 1s - loss: 14.5144 - val_loss: 19.4412\n",
      "Epoch 158/200\n",
      "17/17 - 1s - loss: 13.7573 - val_loss: 20.4080\n",
      "Epoch 159/200\n",
      "17/17 - 1s - loss: 13.6663 - val_loss: 19.6207\n",
      "Epoch 160/200\n",
      "17/17 - 1s - loss: 14.1283 - val_loss: 19.9156\n",
      "Epoch 161/200\n",
      "17/17 - 1s - loss: 13.8165 - val_loss: 19.7429\n",
      "Epoch 162/200\n",
      "17/17 - 1s - loss: 13.9883 - val_loss: 20.0918\n",
      "Epoch 163/200\n",
      "17/17 - 1s - loss: 13.8543 - val_loss: 19.7204\n",
      "Epoch 164/200\n",
      "17/17 - 1s - loss: 13.9143 - val_loss: 19.6124\n",
      "Epoch 165/200\n",
      "17/17 - 1s - loss: 13.9242 - val_loss: 19.3002\n",
      "Epoch 166/200\n",
      "17/17 - 1s - loss: 13.7445 - val_loss: 20.0472\n",
      "Epoch 167/200\n",
      "17/17 - 1s - loss: 13.6697 - val_loss: 19.4699\n",
      "Epoch 168/200\n",
      "17/17 - 1s - loss: 13.3685 - val_loss: 19.4829\n",
      "Epoch 169/200\n",
      "17/17 - 1s - loss: 13.7107 - val_loss: 19.3018\n",
      "Epoch 170/200\n",
      "17/17 - 1s - loss: 13.6464 - val_loss: 19.8141\n",
      "Epoch 171/200\n",
      "17/17 - 1s - loss: 13.6900 - val_loss: 19.5420\n",
      "Epoch 172/200\n",
      "17/17 - 1s - loss: 13.3815 - val_loss: 19.7224\n",
      "Epoch 173/200\n",
      "17/17 - 1s - loss: 13.6129 - val_loss: 19.1096\n",
      "Epoch 174/200\n",
      "17/17 - 1s - loss: 14.1501 - val_loss: 18.9398\n",
      "Epoch 175/200\n",
      "17/17 - 1s - loss: 13.7348 - val_loss: 20.4078\n",
      "Epoch 176/200\n",
      "17/17 - 1s - loss: 13.9485 - val_loss: 18.7088\n",
      "Epoch 177/200\n",
      "17/17 - 1s - loss: 13.5488 - val_loss: 19.6482\n",
      "Epoch 178/200\n",
      "17/17 - 1s - loss: 13.2107 - val_loss: 18.5755\n",
      "Epoch 179/200\n",
      "17/17 - 1s - loss: 13.3952 - val_loss: 19.4860\n",
      "Epoch 180/200\n",
      "17/17 - 1s - loss: 13.2102 - val_loss: 19.1369\n",
      "Epoch 181/200\n",
      "17/17 - 1s - loss: 13.2342 - val_loss: 19.8283\n",
      "Epoch 182/200\n",
      "17/17 - 1s - loss: 13.5287 - val_loss: 18.5598\n",
      "Epoch 183/200\n",
      "17/17 - 1s - loss: 13.5211 - val_loss: 19.2278\n",
      "Epoch 184/200\n",
      "17/17 - 1s - loss: 13.1904 - val_loss: 19.2020\n",
      "Epoch 185/200\n",
      "17/17 - 1s - loss: 13.4638 - val_loss: 19.9916\n",
      "Epoch 186/200\n",
      "17/17 - 1s - loss: 13.4109 - val_loss: 19.5952\n",
      "Epoch 187/200\n",
      "17/17 - 1s - loss: 13.0248 - val_loss: 19.6532\n",
      "Epoch 188/200\n",
      "17/17 - 1s - loss: 13.2696 - val_loss: 19.1030\n",
      "Epoch 189/200\n",
      "17/17 - 1s - loss: 13.3256 - val_loss: 19.0322\n",
      "Epoch 190/200\n",
      "17/17 - 1s - loss: 12.9927 - val_loss: 19.4571\n",
      "Epoch 191/200\n",
      "17/17 - 1s - loss: 12.6618 - val_loss: 18.5960\n",
      "Epoch 192/200\n",
      "17/17 - 1s - loss: 13.4154 - val_loss: 19.8677\n",
      "Epoch 193/200\n",
      "17/17 - 1s - loss: 13.0478 - val_loss: 19.4726\n",
      "Epoch 194/200\n",
      "17/17 - 1s - loss: 12.6829 - val_loss: 19.5299\n",
      "Epoch 195/200\n",
      "17/17 - 1s - loss: 13.3703 - val_loss: 19.7164\n",
      "Epoch 196/200\n",
      "17/17 - 1s - loss: 13.2666 - val_loss: 20.2652\n",
      "Epoch 197/200\n",
      "17/17 - 1s - loss: 12.9679 - val_loss: 19.2182\n",
      "Epoch 198/200\n",
      "17/17 - 1s - loss: 12.7393 - val_loss: 19.4954\n",
      "Epoch 199/200\n",
      "17/17 - 1s - loss: 13.3207 - val_loss: 19.2286\n",
      "Epoch 200/200\n",
      "17/17 - 1s - loss: 12.9519 - val_loss: 19.3264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2003714d748>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience = 100)\n",
    "\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Bidirectional, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "mctp = ModelCheckpoint('best_bi-GRU_SPX_Price-Up-Move_model_with_stats_RealVal.h5', monitor='val_loss', mode='min', verbose=0, save_best_only=True)\n",
    "X_traintp_gru = X_train_tp.reshape(-1,X_train_tp.shape[1],1)\n",
    "X_testtp_gru = X_test_tp.reshape(-1,X_test_tp.shape[1],1)\n",
    "tp_model = Sequential()\n",
    "tp_model.add(Bidirectional(GRU(128, input_shape=(X_train_tp.shape[1],1), return_sequences = True)))\n",
    "tp_model.add(Dropout(0.2))\n",
    "tp_model.add(GRU(64, return_sequences = True))\n",
    "tp_model.add(Dropout(0.2))\n",
    "tp_model.add(BatchNormalization())\n",
    "tp_model.add(GRU(32))\n",
    "tp_model.add(Dropout(0.2))\n",
    "tp_model.add(BatchNormalization())\n",
    "tp_model.add(Dense(16, activation='relu'))\n",
    "tp_model.add(Dropout(0.2))\n",
    "tp_model.add(BatchNormalization())\n",
    "tp_model.add(Dense(1))\n",
    "tp_model.compile(loss='mae', optimizer='adam')\n",
    "tp_model.fit(X_traintp_gru, y_train_tp, epochs=200, batch_size=128, verbose=2, use_multiprocessing=True, \n",
    "          validation_data=(X_testtp_gru, y_test_tp),\n",
    "         callbacks=[es, mctp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "17/17 - 8s - loss: 284.0846 - val_loss: 326.5873\n",
      "Epoch 2/200\n",
      "17/17 - 1s - loss: 283.9823 - val_loss: 326.5122\n",
      "Epoch 3/200\n",
      "17/17 - 1s - loss: 283.9082 - val_loss: 326.3602\n",
      "Epoch 4/200\n",
      "17/17 - 1s - loss: 283.8444 - val_loss: 326.4265\n",
      "Epoch 5/200\n",
      "17/17 - 1s - loss: 283.7201 - val_loss: 326.7257\n",
      "Epoch 6/200\n",
      "17/17 - 1s - loss: 283.5816 - val_loss: 326.6800\n",
      "Epoch 7/200\n",
      "17/17 - 1s - loss: 283.4949 - val_loss: 326.6987\n",
      "Epoch 8/200\n",
      "17/17 - 1s - loss: 283.3654 - val_loss: 326.5738\n",
      "Epoch 9/200\n",
      "17/17 - 1s - loss: 283.2101 - val_loss: 326.2130\n",
      "Epoch 10/200\n",
      "17/17 - 1s - loss: 283.0598 - val_loss: 325.9526\n",
      "Epoch 11/200\n",
      "17/17 - 1s - loss: 282.8660 - val_loss: 325.9821\n",
      "Epoch 12/200\n",
      "17/17 - 1s - loss: 282.6482 - val_loss: 325.9539\n",
      "Epoch 13/200\n",
      "17/17 - 1s - loss: 282.4154 - val_loss: 325.7554\n",
      "Epoch 14/200\n",
      "17/17 - 1s - loss: 282.2167 - val_loss: 325.2423\n",
      "Epoch 15/200\n",
      "17/17 - 1s - loss: 281.9593 - val_loss: 324.6219\n",
      "Epoch 16/200\n",
      "17/17 - 1s - loss: 281.6870 - val_loss: 324.3797\n",
      "Epoch 17/200\n",
      "17/17 - 1s - loss: 281.4314 - val_loss: 323.8962\n",
      "Epoch 18/200\n",
      "17/17 - 1s - loss: 281.1388 - val_loss: 323.6519\n",
      "Epoch 19/200\n",
      "17/17 - 1s - loss: 280.7999 - val_loss: 323.0299\n",
      "Epoch 20/200\n",
      "17/17 - 1s - loss: 280.5007 - val_loss: 323.0551\n",
      "Epoch 21/200\n",
      "17/17 - 1s - loss: 280.1410 - val_loss: 322.7955\n",
      "Epoch 22/200\n",
      "17/17 - 1s - loss: 279.7836 - val_loss: 323.0977\n",
      "Epoch 23/200\n",
      "17/17 - 1s - loss: 279.4820 - val_loss: 322.6254\n",
      "Epoch 24/200\n",
      "17/17 - 1s - loss: 279.0291 - val_loss: 321.8225\n",
      "Epoch 25/200\n",
      "17/17 - 1s - loss: 278.6601 - val_loss: 322.3235\n",
      "Epoch 26/200\n",
      "17/17 - 1s - loss: 278.2920 - val_loss: 321.9591\n",
      "Epoch 27/200\n",
      "17/17 - 1s - loss: 277.8972 - val_loss: 321.0261\n",
      "Epoch 28/200\n",
      "17/17 - 1s - loss: 277.4909 - val_loss: 321.0002\n",
      "Epoch 29/200\n",
      "17/17 - 1s - loss: 277.0994 - val_loss: 320.0239\n",
      "Epoch 30/200\n",
      "17/17 - 1s - loss: 276.8185 - val_loss: 319.8183\n",
      "Epoch 31/200\n",
      "17/17 - 1s - loss: 276.2554 - val_loss: 319.2073\n",
      "Epoch 32/200\n",
      "17/17 - 1s - loss: 275.9975 - val_loss: 318.6203\n",
      "Epoch 33/200\n",
      "17/17 - 1s - loss: 275.7091 - val_loss: 319.8723\n",
      "Epoch 34/200\n",
      "17/17 - 1s - loss: 275.2610 - val_loss: 318.4278\n",
      "Epoch 35/200\n",
      "17/17 - 1s - loss: 274.7676 - val_loss: 318.2422\n",
      "Epoch 36/200\n",
      "17/17 - 1s - loss: 274.3835 - val_loss: 318.2465\n",
      "Epoch 37/200\n",
      "17/17 - 1s - loss: 274.0223 - val_loss: 316.9712\n",
      "Epoch 38/200\n",
      "17/17 - 1s - loss: 273.7881 - val_loss: 317.1479\n",
      "Epoch 39/200\n",
      "17/17 - 1s - loss: 273.4144 - val_loss: 315.8043\n",
      "Epoch 40/200\n",
      "17/17 - 1s - loss: 273.1689 - val_loss: 316.8277\n",
      "Epoch 41/200\n",
      "17/17 - 1s - loss: 272.5719 - val_loss: 315.9546\n",
      "Epoch 42/200\n",
      "17/17 - 1s - loss: 272.2977 - val_loss: 315.9310\n",
      "Epoch 43/200\n",
      "17/17 - 1s - loss: 271.8476 - val_loss: 315.1385\n",
      "Epoch 44/200\n",
      "17/17 - 1s - loss: 271.4089 - val_loss: 315.4001\n",
      "Epoch 45/200\n",
      "17/17 - 1s - loss: 271.1208 - val_loss: 314.1082\n",
      "Epoch 46/200\n",
      "17/17 - 1s - loss: 270.8429 - val_loss: 314.1505\n",
      "Epoch 47/200\n",
      "17/17 - 1s - loss: 270.2343 - val_loss: 314.1213\n",
      "Epoch 48/200\n",
      "17/17 - 1s - loss: 269.8970 - val_loss: 313.7791\n",
      "Epoch 49/200\n",
      "17/17 - 1s - loss: 269.6273 - val_loss: 312.7878\n",
      "Epoch 50/200\n",
      "17/17 - 1s - loss: 269.4423 - val_loss: 313.6626\n",
      "Epoch 51/200\n",
      "17/17 - 1s - loss: 268.9070 - val_loss: 313.1375\n",
      "Epoch 52/200\n",
      "17/17 - 1s - loss: 268.8188 - val_loss: 311.9029\n",
      "Epoch 53/200\n",
      "17/17 - 1s - loss: 268.7953 - val_loss: 312.3493\n",
      "Epoch 54/200\n",
      "17/17 - 1s - loss: 268.1865 - val_loss: 311.7194\n",
      "Epoch 55/200\n",
      "17/17 - 1s - loss: 267.4378 - val_loss: 311.6203\n",
      "Epoch 56/200\n",
      "17/17 - 1s - loss: 267.5012 - val_loss: 311.2471\n",
      "Epoch 57/200\n",
      "17/17 - 1s - loss: 267.3873 - val_loss: 311.0292\n",
      "Epoch 58/200\n",
      "17/17 - 1s - loss: 266.6586 - val_loss: 310.3619\n",
      "Epoch 59/200\n",
      "17/17 - 1s - loss: 266.6127 - val_loss: 310.3216\n",
      "Epoch 60/200\n",
      "17/17 - 1s - loss: 266.4785 - val_loss: 310.8446\n",
      "Epoch 61/200\n",
      "17/17 - 1s - loss: 266.2472 - val_loss: 310.5559\n",
      "Epoch 62/200\n",
      "17/17 - 1s - loss: 266.0787 - val_loss: 310.4142\n",
      "Epoch 63/200\n",
      "17/17 - 1s - loss: 265.0828 - val_loss: 309.8206\n",
      "Epoch 64/200\n",
      "17/17 - 1s - loss: 265.0736 - val_loss: 309.9368\n",
      "Epoch 65/200\n",
      "17/17 - 1s - loss: 264.8185 - val_loss: 308.7260\n",
      "Epoch 66/200\n",
      "17/17 - 1s - loss: 264.3551 - val_loss: 307.7437\n",
      "Epoch 67/200\n",
      "17/17 - 1s - loss: 263.6198 - val_loss: 307.4325\n",
      "Epoch 68/200\n",
      "17/17 - 1s - loss: 263.3004 - val_loss: 307.9802\n",
      "Epoch 69/200\n",
      "17/17 - 1s - loss: 263.2556 - val_loss: 307.5176\n",
      "Epoch 70/200\n",
      "17/17 - 1s - loss: 262.8716 - val_loss: 308.2973\n",
      "Epoch 71/200\n",
      "17/17 - 1s - loss: 262.7700 - val_loss: 307.4516\n",
      "Epoch 72/200\n",
      "17/17 - 1s - loss: 262.4469 - val_loss: 306.5769\n",
      "Epoch 73/200\n",
      "17/17 - 1s - loss: 262.1045 - val_loss: 306.6862\n",
      "Epoch 74/200\n",
      "17/17 - 1s - loss: 261.0907 - val_loss: 306.7675\n",
      "Epoch 75/200\n",
      "17/17 - 1s - loss: 260.7000 - val_loss: 306.6233\n",
      "Epoch 76/200\n",
      "17/17 - 1s - loss: 260.9896 - val_loss: 307.3412\n",
      "Epoch 77/200\n",
      "17/17 - 1s - loss: 260.4819 - val_loss: 306.8366\n",
      "Epoch 78/200\n",
      "17/17 - 1s - loss: 260.6375 - val_loss: 308.6064\n",
      "Epoch 79/200\n",
      "17/17 - 1s - loss: 259.6417 - val_loss: 307.5407\n",
      "Epoch 80/200\n",
      "17/17 - 1s - loss: 259.7765 - val_loss: 306.3673\n",
      "Epoch 81/200\n",
      "17/17 - 1s - loss: 258.9677 - val_loss: 304.5150\n",
      "Epoch 82/200\n",
      "17/17 - 1s - loss: 258.2610 - val_loss: 304.6844\n",
      "Epoch 83/200\n",
      "17/17 - 1s - loss: 258.0214 - val_loss: 304.7007\n",
      "Epoch 84/200\n",
      "17/17 - 1s - loss: 257.7664 - val_loss: 305.4465\n",
      "Epoch 85/200\n",
      "17/17 - 1s - loss: 257.6566 - val_loss: 305.6644\n",
      "Epoch 86/200\n",
      "17/17 - 1s - loss: 257.2379 - val_loss: 304.5324\n",
      "Epoch 87/200\n",
      "17/17 - 1s - loss: 257.1824 - val_loss: 306.0964\n",
      "Epoch 88/200\n",
      "17/17 - 1s - loss: 256.5748 - val_loss: 303.4393\n",
      "Epoch 89/200\n",
      "17/17 - 1s - loss: 255.8578 - val_loss: 303.0458\n",
      "Epoch 90/200\n",
      "17/17 - 1s - loss: 255.5542 - val_loss: 304.8972\n",
      "Epoch 91/200\n",
      "17/17 - 1s - loss: 255.3736 - val_loss: 302.5587\n",
      "Epoch 92/200\n",
      "17/17 - 1s - loss: 254.3983 - val_loss: 302.9626\n",
      "Epoch 93/200\n",
      "17/17 - 1s - loss: 254.7872 - val_loss: 304.6957\n",
      "Epoch 94/200\n",
      "17/17 - 1s - loss: 253.5384 - val_loss: 301.5967\n",
      "Epoch 95/200\n",
      "17/17 - 1s - loss: 254.1430 - val_loss: 302.8985\n",
      "Epoch 96/200\n",
      "17/17 - 1s - loss: 253.1147 - val_loss: 301.0565\n",
      "Epoch 97/200\n",
      "17/17 - 1s - loss: 253.0138 - val_loss: 302.2456\n",
      "Epoch 98/200\n",
      "17/17 - 1s - loss: 253.0051 - val_loss: 303.3480\n",
      "Epoch 99/200\n",
      "17/17 - 1s - loss: 252.2230 - val_loss: 301.0284\n",
      "Epoch 100/200\n",
      "17/17 - 1s - loss: 252.9061 - val_loss: 300.5789\n",
      "Epoch 101/200\n",
      "17/17 - 1s - loss: 252.2439 - val_loss: 301.6404\n",
      "Epoch 102/200\n",
      "17/17 - 1s - loss: 251.1629 - val_loss: 301.2766\n",
      "Epoch 103/200\n",
      "17/17 - 1s - loss: 250.0036 - val_loss: 300.5238\n",
      "Epoch 104/200\n",
      "17/17 - 1s - loss: 251.7407 - val_loss: 299.6398\n",
      "Epoch 105/200\n",
      "17/17 - 1s - loss: 249.5178 - val_loss: 301.4553\n",
      "Epoch 106/200\n",
      "17/17 - 1s - loss: 250.4708 - val_loss: 299.0861\n",
      "Epoch 107/200\n",
      "17/17 - 1s - loss: 248.5410 - val_loss: 298.4743\n",
      "Epoch 108/200\n",
      "17/17 - 1s - loss: 249.8336 - val_loss: 297.9955\n",
      "Epoch 109/200\n",
      "17/17 - 1s - loss: 248.6832 - val_loss: 300.0494\n",
      "Epoch 110/200\n",
      "17/17 - 1s - loss: 247.8893 - val_loss: 299.8006\n",
      "Epoch 111/200\n",
      "17/17 - 1s - loss: 247.6073 - val_loss: 297.4431\n",
      "Epoch 112/200\n",
      "17/17 - 1s - loss: 247.2298 - val_loss: 297.4355\n",
      "Epoch 113/200\n",
      "17/17 - 1s - loss: 247.2267 - val_loss: 297.1767\n",
      "Epoch 114/200\n",
      "17/17 - 1s - loss: 246.4243 - val_loss: 295.4051\n",
      "Epoch 115/200\n",
      "17/17 - 1s - loss: 246.3310 - val_loss: 296.0578\n",
      "Epoch 116/200\n",
      "17/17 - 1s - loss: 245.5228 - val_loss: 294.8597\n",
      "Epoch 117/200\n",
      "17/17 - 1s - loss: 244.8503 - val_loss: 297.0832\n",
      "Epoch 118/200\n",
      "17/17 - 1s - loss: 244.4712 - val_loss: 297.3714\n",
      "Epoch 119/200\n",
      "17/17 - 1s - loss: 245.5879 - val_loss: 295.0412\n",
      "Epoch 120/200\n",
      "17/17 - 1s - loss: 243.9359 - val_loss: 297.4792\n",
      "Epoch 121/200\n",
      "17/17 - 1s - loss: 244.7769 - val_loss: 295.3760\n",
      "Epoch 122/200\n",
      "17/17 - 1s - loss: 243.6533 - val_loss: 294.2325\n",
      "Epoch 123/200\n",
      "17/17 - 1s - loss: 243.0490 - val_loss: 295.1791\n",
      "Epoch 124/200\n",
      "17/17 - 1s - loss: 241.8996 - val_loss: 293.8421\n",
      "Epoch 125/200\n",
      "17/17 - 1s - loss: 241.7237 - val_loss: 291.6622\n",
      "Epoch 126/200\n",
      "17/17 - 1s - loss: 241.9108 - val_loss: 290.3094\n",
      "Epoch 127/200\n",
      "17/17 - 1s - loss: 241.2320 - val_loss: 293.2785\n",
      "Epoch 128/200\n",
      "17/17 - 1s - loss: 240.1425 - val_loss: 294.7499\n",
      "Epoch 129/200\n",
      "17/17 - 1s - loss: 240.9381 - val_loss: 293.1594\n",
      "Epoch 130/200\n",
      "17/17 - 1s - loss: 239.9738 - val_loss: 292.4814\n",
      "Epoch 131/200\n",
      "17/17 - 1s - loss: 239.4683 - val_loss: 293.5066\n",
      "Epoch 132/200\n",
      "17/17 - 1s - loss: 239.1398 - val_loss: 291.9594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/200\n",
      "17/17 - 1s - loss: 237.9181 - val_loss: 292.5412\n",
      "Epoch 134/200\n",
      "17/17 - 1s - loss: 237.2702 - val_loss: 292.2721\n",
      "Epoch 135/200\n",
      "17/17 - 1s - loss: 239.3143 - val_loss: 289.8653\n",
      "Epoch 136/200\n",
      "17/17 - 1s - loss: 238.0971 - val_loss: 293.8074\n",
      "Epoch 137/200\n",
      "17/17 - 1s - loss: 237.2554 - val_loss: 291.6493\n",
      "Epoch 138/200\n",
      "17/17 - 1s - loss: 234.9737 - val_loss: 289.3879\n",
      "Epoch 139/200\n",
      "17/17 - 1s - loss: 234.3279 - val_loss: 293.6606\n",
      "Epoch 140/200\n",
      "17/17 - 1s - loss: 235.0464 - val_loss: 290.0910\n",
      "Epoch 141/200\n",
      "17/17 - 1s - loss: 234.2966 - val_loss: 292.4181\n",
      "Epoch 142/200\n",
      "17/17 - 1s - loss: 233.3663 - val_loss: 289.0634\n",
      "Epoch 143/200\n",
      "17/17 - 1s - loss: 232.8388 - val_loss: 290.3386\n",
      "Epoch 144/200\n",
      "17/17 - 1s - loss: 233.5675 - val_loss: 290.6149\n",
      "Epoch 145/200\n",
      "17/17 - 1s - loss: 230.5228 - val_loss: 286.1985\n",
      "Epoch 146/200\n",
      "17/17 - 1s - loss: 231.7977 - val_loss: 290.8873\n",
      "Epoch 147/200\n",
      "17/17 - 1s - loss: 230.9214 - val_loss: 289.5058\n",
      "Epoch 148/200\n",
      "17/17 - 1s - loss: 232.3825 - val_loss: 284.1336\n",
      "Epoch 149/200\n",
      "17/17 - 1s - loss: 231.8775 - val_loss: 287.5804\n",
      "Epoch 150/200\n",
      "17/17 - 1s - loss: 229.3368 - val_loss: 289.7739\n",
      "Epoch 151/200\n",
      "17/17 - 1s - loss: 229.5903 - val_loss: 281.7180\n",
      "Epoch 152/200\n",
      "17/17 - 1s - loss: 231.8953 - val_loss: 286.0429\n",
      "Epoch 153/200\n",
      "17/17 - 1s - loss: 232.4677 - val_loss: 286.3815\n",
      "Epoch 154/200\n",
      "17/17 - 1s - loss: 230.0278 - val_loss: 280.5891\n",
      "Epoch 155/200\n",
      "17/17 - 1s - loss: 228.0311 - val_loss: 276.8027\n",
      "Epoch 156/200\n",
      "17/17 - 1s - loss: 227.6272 - val_loss: 279.5691\n",
      "Epoch 157/200\n",
      "17/17 - 1s - loss: 226.3775 - val_loss: 280.0419\n",
      "Epoch 158/200\n",
      "17/17 - 1s - loss: 226.5940 - val_loss: 280.3412\n",
      "Epoch 159/200\n",
      "17/17 - 1s - loss: 225.5685 - val_loss: 280.1199\n",
      "Epoch 160/200\n",
      "17/17 - 1s - loss: 223.9493 - val_loss: 280.8682\n",
      "Epoch 161/200\n",
      "17/17 - 1s - loss: 223.1300 - val_loss: 282.1913\n",
      "Epoch 162/200\n",
      "17/17 - 1s - loss: 224.3738 - val_loss: 281.3474\n",
      "Epoch 163/200\n",
      "17/17 - 1s - loss: 221.2360 - val_loss: 281.5079\n",
      "Epoch 164/200\n",
      "17/17 - 1s - loss: 218.7876 - val_loss: 280.9129\n",
      "Epoch 165/200\n",
      "17/17 - 1s - loss: 220.1960 - val_loss: 279.1744\n",
      "Epoch 166/200\n",
      "17/17 - 1s - loss: 221.3553 - val_loss: 284.0917\n",
      "Epoch 167/200\n",
      "17/17 - 1s - loss: 219.9645 - val_loss: 281.2941\n",
      "Epoch 168/200\n",
      "17/17 - 1s - loss: 219.4064 - val_loss: 282.4690\n",
      "Epoch 169/200\n",
      "17/17 - 1s - loss: 221.1288 - val_loss: 280.0454\n",
      "Epoch 170/200\n",
      "17/17 - 1s - loss: 218.7901 - val_loss: 280.7636\n",
      "Epoch 171/200\n",
      "17/17 - 1s - loss: 217.0890 - val_loss: 280.5795\n",
      "Epoch 172/200\n",
      "17/17 - 1s - loss: 218.5186 - val_loss: 279.0738\n",
      "Epoch 173/200\n",
      "17/17 - 1s - loss: 218.9219 - val_loss: 277.4740\n",
      "Epoch 174/200\n",
      "17/17 - 1s - loss: 217.6878 - val_loss: 285.2910\n",
      "Epoch 175/200\n",
      "17/17 - 1s - loss: 217.6551 - val_loss: 287.4048\n",
      "Epoch 176/200\n",
      "17/17 - 1s - loss: 216.6827 - val_loss: 274.1727\n",
      "Epoch 177/200\n",
      "17/17 - 1s - loss: 212.5576 - val_loss: 273.9155\n",
      "Epoch 178/200\n",
      "17/17 - 1s - loss: 213.0931 - val_loss: 273.2650\n",
      "Epoch 179/200\n",
      "17/17 - 1s - loss: 211.3622 - val_loss: 273.1692\n",
      "Epoch 180/200\n",
      "17/17 - 1s - loss: 214.4623 - val_loss: 276.8379\n",
      "Epoch 181/200\n",
      "17/17 - 1s - loss: 211.3156 - val_loss: 273.6433\n",
      "Epoch 182/200\n",
      "17/17 - 1s - loss: 212.1261 - val_loss: 280.0751\n",
      "Epoch 183/200\n",
      "17/17 - 1s - loss: 210.4531 - val_loss: 275.7176\n",
      "Epoch 184/200\n",
      "17/17 - 1s - loss: 207.9596 - val_loss: 273.4581\n",
      "Epoch 185/200\n",
      "17/17 - 1s - loss: 208.6354 - val_loss: 269.6739\n",
      "Epoch 186/200\n",
      "17/17 - 1s - loss: 208.0621 - val_loss: 266.8228\n",
      "Epoch 187/200\n",
      "17/17 - 1s - loss: 206.9224 - val_loss: 276.0951\n",
      "Epoch 188/200\n",
      "17/17 - 1s - loss: 207.2851 - val_loss: 275.8336\n",
      "Epoch 189/200\n",
      "17/17 - 1s - loss: 203.5862 - val_loss: 277.9067\n",
      "Epoch 190/200\n",
      "17/17 - 1s - loss: 204.0296 - val_loss: 269.4650\n",
      "Epoch 191/200\n",
      "17/17 - 1s - loss: 204.3503 - val_loss: 265.8791\n",
      "Epoch 192/200\n",
      "17/17 - 1s - loss: 202.3192 - val_loss: 274.1399\n",
      "Epoch 193/200\n",
      "17/17 - 1s - loss: 205.3529 - val_loss: 273.2512\n",
      "Epoch 194/200\n",
      "17/17 - 1s - loss: 202.4630 - val_loss: 272.7918\n",
      "Epoch 195/200\n",
      "17/17 - 1s - loss: 202.4149 - val_loss: 267.6935\n",
      "Epoch 196/200\n",
      "17/17 - 1s - loss: 203.4863 - val_loss: 273.9910\n",
      "Epoch 197/200\n",
      "17/17 - 1s - loss: 201.9865 - val_loss: 272.4152\n",
      "Epoch 198/200\n",
      "17/17 - 1s - loss: 203.1901 - val_loss: 269.4584\n",
      "Epoch 199/200\n",
      "17/17 - 1s - loss: 202.3835 - val_loss: 272.3190\n",
      "Epoch 200/200\n",
      "17/17 - 1s - loss: 197.4056 - val_loss: 274.9099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x200435e3248>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcsl = ModelCheckpoint('best_bi-GRU_SPX_Price-Down-Move_model_with_stats_RealVal.h5', monitor='val_loss', mode='min', verbose=0, save_best_only=True)\n",
    "X_trainsl_gru = X_train_sl.reshape(-1,X_train_sl.shape[1],1)\n",
    "X_testsl_gru = X_test_sl.reshape(-1,X_test_sl.shape[1],1)\n",
    "sl_model = Sequential()\n",
    "sl_model.add(Bidirectional(GRU(128, input_shape=(X_train_sl.shape[1],1), return_sequences = True)))\n",
    "sl_model.add(Dropout(0.2))\n",
    "sl_model.add(GRU(64, return_sequences = True))\n",
    "sl_model.add(Dropout(0.2))\n",
    "sl_model.add(BatchNormalization())\n",
    "sl_model.add(GRU(32))\n",
    "sl_model.add(Dropout(0.2))\n",
    "sl_model.add(BatchNormalization())\n",
    "sl_model.add(Dense(16, activation='relu'))\n",
    "sl_model.add(Dropout(0.2))\n",
    "sl_model.add(BatchNormalization())\n",
    "sl_model.add(Dense(1))\n",
    "sl_model.compile(loss='mae', optimizer='adam')\n",
    "sl_model.fit(X_trainsl_gru, y_train_sl, epochs=200, batch_size=128, verbose=2, use_multiprocessing=True, \n",
    "          validation_data=(X_testsl_gru, y_test_sl),\n",
    "         callbacks=[es, mcsl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "160\n",
      "149\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('last_10_test.csv')\n",
    "df.Close = list(reversed(df.Close.tolist()))\n",
    "df.Open = list(reversed(df.Open.tolist()))\n",
    "df.High = list(reversed(df.High.tolist()))\n",
    "df.Low = list(reversed(df.Low.tolist()))\n",
    "df = label_calc(df)\n",
    "df = indicator_fill(df)\n",
    "print(len(df))\n",
    "df = df[20:]\n",
    "print(len(df))\n",
    "df = df[df.buy_label!='Pass']\n",
    "\n",
    "df['cl'] = df.Close\n",
    "df['hi'] = df.High\n",
    "df['lo'] = df.Low\n",
    "df['op'] = df.Open\n",
    "df['sma'] = df['20_SMA']\n",
    "df['fema'] = df['5_EMA']\n",
    "\n",
    "\n",
    "df['ma_diff'] = df['5_EMA'] - df['20_SMA']\n",
    "df['tclose_diff'] = df['Close'] - df['20_SMA']\n",
    "df['fclose_diff'] = df['Close'] - df['5_EMA']\n",
    "df.Close = list(np.squeeze(cf.fit_transform(np.array(df.Close).reshape(-1,1)), axis = 1))\n",
    "df.High = list(np.squeeze(cf.transform(np.array(df.High).reshape(-1,1)), axis = 1))\n",
    "df.Open = list(np.squeeze(cf.transform(np.array(df.Open).reshape(-1,1)), axis = 1))\n",
    "df.Low = list(np.squeeze(cf.transform(np.array(df.Low).reshape(-1,1)), axis = 1))\n",
    "df['20_SMA'] = list(np.squeeze(cf.transform(np.array(df['20_SMA']).reshape(-1,1)), axis = 1))\n",
    "df['5_EMA'] = list(np.squeeze(cf.transform(np.array(df['5_EMA']).reshape(-1,1)), axis = 1))\n",
    "\n",
    "#df['20_SMA'] = list(np.squeeze(cf.transform(np.array(df['20_SMA']).reshape(-1,1)), axis = 1))\n",
    "df = df[1:]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\n",
      "Vector_Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df['us'] = df.apply(lambda x: upper_shadow(x.Open, x.Close, x.High), axis = 1)\n",
    "df['ls'] = df.apply(lambda x: lower_shadow(x.Open, x.Close, x.Low), axis = 1)\n",
    "df['rb'] = df.apply(lambda x: real_body(x.Open, x.Close), axis = 1)\n",
    "v,bs,ss,bq,sq,tp,sl = df_vectorize(df)\n",
    "df = df[10:]\n",
    "v = np.array(v)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139, 190)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "v = np.nan_to_num(v)\n",
    "v_v = v[:,40:50]\n",
    "v_s = v[:,40:50]\n",
    "v_bq = v[:,40:50]\n",
    "v_sq = v[:,40:50]\n",
    "vtp = v[:,40:50]\n",
    "vsl = v[:,40:50]\n",
    "bside_lab = buy_le.transform(bs)\n",
    "sside_lab = sell_le.transform(ss)\n",
    "bqside_lab = bq_le.transform(bq)\n",
    "sqside_lab = sq_le.transform(sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.83999999999969\n",
      "54.17000000000007\n",
      "217.26000000000022\n",
      "202.0899999999997\n"
     ]
    }
   ],
   "source": [
    "print(statistics.median(tp))\n",
    "print(statistics.median(sl))\n",
    "print(max(tp))\n",
    "print(max(sl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buy Side--------------------------\n",
      "0.5971223021582733\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.83      0.75        99\n",
      "           1       0.06      0.03      0.03        40\n",
      "\n",
      "    accuracy                           0.60       139\n",
      "   macro avg       0.37      0.43      0.39       139\n",
      "weighted avg       0.50      0.60      0.54       139\n",
      "\n",
      "Sell Side--------------------------\n",
      "0.5899280575539568\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.30      0.30        40\n",
      "           1       0.71      0.71      0.71        99\n",
      "\n",
      "    accuracy                           0.59       139\n",
      "   macro avg       0.50      0.50      0.50       139\n",
      "weighted avg       0.59      0.59      0.59       139\n",
      "\n",
      "Buy Quality Side--------------------------\n",
      "0.4892086330935252\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.56      0.55        78\n",
      "           1       0.41      0.39      0.40        61\n",
      "\n",
      "    accuracy                           0.49       139\n",
      "   macro avg       0.48      0.48      0.48       139\n",
      "weighted avg       0.49      0.49      0.49       139\n",
      "\n",
      "Sell Quality Side--------------------------\n",
      "0.5611510791366906\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.10      0.12        39\n",
      "           1       0.68      0.74      0.71       100\n",
      "\n",
      "    accuracy                           0.56       139\n",
      "   macro avg       0.41      0.42      0.41       139\n",
      "weighted avg       0.53      0.56      0.54       139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, \\\n",
    "    recall_score, confusion_matrix, classification_report, \\\n",
    "    accuracy_score, f1_score\n",
    "from tensorflow.keras.models import load_model\n",
    "bmodel = load_model('best_bi-GRU_SPX_buy_model_with_stats_RealVal.h5')\n",
    "smodel = load_model('best_bi-GRU_SPX_sell_model_with_stats_RealVal.h5')\n",
    "bqmodel = load_model('best_bi-GRU_SPX_buy-quality_model_with_stats_RealVal.h5')\n",
    "sqmodel = load_model('best_bi-GRU_SPX_sell-quality_model_with_stats_RealVal.h5')\n",
    "tpmodel = load_model('best_bi-GRU_SPX_Price-Up-Move_model_with_stats_RealVal.h5')\n",
    "slmodel = load_model('best_bi-GRU_SPX_Price-Down-Move_model_with_stats_RealVal.h5')\n",
    "v_v = v_v.reshape(-1,v_v.shape[1],1)\n",
    "v_s = v_s.reshape(-1,v_s.shape[1],1)\n",
    "v_bq = v_bq.reshape(-1,v_bq.shape[1],1)\n",
    "v_sq = v_sq.reshape(-1,v_sq.shape[1],1)\n",
    "vtp = vtp.reshape(-1,vtp.shape[1],1)\n",
    "vsl = vsl.reshape(-1,vsl.shape[1],1)\n",
    "\n",
    "preds_out = bmodel.predict(v_v)\n",
    "preds = np.argmax(preds_out, axis=1)\n",
    "probs = np.max(preds_out, axis=1)\n",
    "\n",
    "preds_out = smodel.predict(v_s)\n",
    "spreds = np.argmax(preds_out, axis=1)\n",
    "sprobs = np.max(preds_out, axis=1)\n",
    "\n",
    "preds_out = bqmodel.predict(v_bq)\n",
    "bqpreds = np.argmax(preds_out, axis=1)\n",
    "bqprobs = np.max(preds_out, axis=1)\n",
    "\n",
    "preds_out = sqmodel.predict(v_sq)\n",
    "sqpreds = np.argmax(preds_out, axis=1)\n",
    "sqprobs = np.max(preds_out, axis=1)\n",
    "\n",
    "tppreds_out = tpmodel.predict(vtp)\n",
    "\n",
    "\n",
    "slpreds_out = slmodel.predict(vsl)\n",
    "\n",
    "\n",
    "\n",
    "print('Buy Side--------------------------')\n",
    "print(accuracy_score(bside_lab, preds))\n",
    "print(classification_report(bside_lab, preds))\n",
    "\n",
    "print('Sell Side--------------------------')\n",
    "print(accuracy_score(sside_lab, spreds))\n",
    "print(classification_report(sside_lab, spreds))\n",
    "\n",
    "print('Buy Quality Side--------------------------')\n",
    "print(accuracy_score(bqside_lab, bqpreds))\n",
    "print(classification_report(bqside_lab, bqpreds))\n",
    "\n",
    "print('Sell Quality Side--------------------------')\n",
    "print(accuracy_score(sqside_lab, sqpreds))\n",
    "print(classification_report(sqside_lab, sqpreds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['buy_side'] = buy_le.inverse_transform(preds)\n",
    "df['buy_side_orig'] = buy_le.inverse_transform(bside_lab)\n",
    "df['Buy_Probs'] = probs\n",
    "df['sell_side'] = sell_le.inverse_transform(spreds)\n",
    "df['sell_side_orig'] = sell_le.inverse_transform(sside_lab)\n",
    "df['Sell_Probs'] = sprobs\n",
    "df['buy_quality'] = bq_le.inverse_transform(bqpreds)\n",
    "df['bq_Probs'] = bqprobs\n",
    "df['sell_quality'] = sq_le.inverse_transform(sqpreds)\n",
    "df['sq_Probs'] = sqprobs\n",
    "df['Predicted_upmove'] = tppreds_out\n",
    "df['Predicted_downmove'] = slpreds_out\n",
    "df['Actual_upmove'] = tp\n",
    "df['Actual_downmove'] = sl\n",
    "\n",
    "df.to_csv('last_10_test_output_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:54:15] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-merror:0.47953\n",
      "Will train until validation_0-merror hasn't improved in 30 rounds.\n",
      "[1]\tvalidation_0-merror:0.49123\n",
      "[2]\tvalidation_0-merror:0.45614\n",
      "[3]\tvalidation_0-merror:0.46784\n",
      "[4]\tvalidation_0-merror:0.48538\n",
      "[5]\tvalidation_0-merror:0.47368\n",
      "[6]\tvalidation_0-merror:0.47368\n",
      "[7]\tvalidation_0-merror:0.44444\n",
      "[8]\tvalidation_0-merror:0.48538\n",
      "[9]\tvalidation_0-merror:0.46784\n",
      "[10]\tvalidation_0-merror:0.46199\n",
      "[11]\tvalidation_0-merror:0.48538\n",
      "[12]\tvalidation_0-merror:0.46199\n",
      "[13]\tvalidation_0-merror:0.43860\n",
      "[14]\tvalidation_0-merror:0.46784\n",
      "[15]\tvalidation_0-merror:0.47368\n",
      "[16]\tvalidation_0-merror:0.45614\n",
      "[17]\tvalidation_0-merror:0.45614\n",
      "[18]\tvalidation_0-merror:0.46199\n",
      "[19]\tvalidation_0-merror:0.47368\n",
      "[20]\tvalidation_0-merror:0.49708\n",
      "[21]\tvalidation_0-merror:0.49123\n",
      "[22]\tvalidation_0-merror:0.50877\n",
      "[23]\tvalidation_0-merror:0.49708\n",
      "[24]\tvalidation_0-merror:0.49708\n",
      "[25]\tvalidation_0-merror:0.46784\n",
      "[26]\tvalidation_0-merror:0.46784\n",
      "[27]\tvalidation_0-merror:0.45029\n",
      "[28]\tvalidation_0-merror:0.44444\n",
      "[29]\tvalidation_0-merror:0.44444\n",
      "[30]\tvalidation_0-merror:0.45029\n",
      "[31]\tvalidation_0-merror:0.45029\n",
      "[32]\tvalidation_0-merror:0.45614\n",
      "[33]\tvalidation_0-merror:0.46199\n",
      "[34]\tvalidation_0-merror:0.47953\n",
      "[35]\tvalidation_0-merror:0.47953\n",
      "[36]\tvalidation_0-merror:0.47368\n",
      "[37]\tvalidation_0-merror:0.49123\n",
      "[38]\tvalidation_0-merror:0.48538\n",
      "[39]\tvalidation_0-merror:0.47953\n",
      "[40]\tvalidation_0-merror:0.47953\n",
      "[41]\tvalidation_0-merror:0.46784\n",
      "[42]\tvalidation_0-merror:0.45029\n",
      "[43]\tvalidation_0-merror:0.46199\n",
      "Stopping. Best iteration:\n",
      "[13]\tvalidation_0-merror:0.43860\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=100,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=2000, n_jobs=0, num_class=2, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', use_label_encoder=False,\n",
       "              validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "xgbc = XGBClassifier(max_depth=100, objective='multi:softprob', use_label_encoder=False,num_class=2,n_estimators=2000)\n",
    "\n",
    "\n",
    "xgbc.fit(X_trainb_gru.reshape(-1,X_trainb_gru.shape[1]), y_trainb,\n",
    "         eval_set=[(X_testb_gru.reshape(-1,X_testb_gru.shape[1]),y_testb)]\n",
    "         ,early_stopping_rounds=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
